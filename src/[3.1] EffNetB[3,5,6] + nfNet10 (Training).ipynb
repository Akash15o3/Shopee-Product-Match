{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CefjfwX72PE-"
   },
   "source": [
    "# References\n",
    "\n",
    "* Thanks to PyTorch Arcface Implementation by @tanulsingh077 from [here](https://www.kaggle.com/tanulsingh077/pytorch-metric-learning-pipeline-only-images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip2rC61p3KLb"
   },
   "source": [
    "**Task:** \n",
    "\n",
    "Training nfNet10, EfficientNet(b4-b6) models `model_name` in **pretrained_model_config**\n",
    "\n",
    "- **nfNet10**         - Trained for 12 epochs - ~8 hours\n",
    "- **EfficientNet b4** - Trained for 12 epochs - ~10 hours\n",
    "- **EfficientNet b5** - Trained for 12 epochs - ~15 hours\n",
    "- **EfficientNet b6** - Trained for 10 epochs - ~34 hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 9174,
     "status": "ok",
     "timestamp": 1618551834361,
     "user": {
      "displayName": "Sudha Arulmani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmEq56Sys_hPq7SL1T-PCfBrLQ2STecB40OsxN=s64",
      "userId": "11629712198186904901"
     },
     "user_tz": 420
    },
    "id": "ZuRcTmpobXtp",
    "outputId": "bf234577-b510-4fdd-dc83-9b154d96d535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.5)\n",
      "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.8.1+cu101)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.9.1+cu101)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (1.19.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
      "Collecting albumentations\n",
      "  Using cached https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl\n",
      "Installing collected packages: albumentations\n",
      "  Found existing installation: albumentations 0.5.2\n",
      "    Uninstalling albumentations-0.5.2:\n",
      "      Successfully uninstalled albumentations-0.5.2\n",
      "Successfully installed albumentations-0.5.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "albumentations"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.9.1+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.19.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision) (1.8.1+cu101)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm\n",
    "!pip install --upgrade --force-reinstall --no-deps albumentations\n",
    "!pip install torchvision \n",
    "!pip install tqdm\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import torchvision\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26119,
     "status": "ok",
     "timestamp": 1618615352725,
     "user": {
      "displayName": "Sudha Arulmani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmEq56Sys_hPq7SL1T-PCfBrLQ2STecB40OsxN=s64",
      "userId": "11629712198186904901"
     },
     "user_tz": 420
    },
    "id": "twY3-4Vk3TXi",
    "outputId": "5a3d52ca-6ea6-46e5-ab1b-61dd3559a804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1618615355387,
     "user": {
      "displayName": "Sudha Arulmani",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhmEq56Sys_hPq7SL1T-PCfBrLQ2STecB40OsxN=s64",
      "userId": "11629712198186904901"
     },
     "user_tz": 420
    },
    "id": "aL6gKqja_d5P",
    "outputId": "d86c567f-a1a7-4486-b1b4-31d0dafaa048"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/256_TermProj\n"
     ]
    }
   ],
   "source": [
    "cd '/content/gdrive/MyDrive/256_TermProj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fClr_3Kg1XTm"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bl_olwj62PFG"
   },
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZajgDUks66ee"
   },
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer, scheduler, epoch, device):\n",
    "    model.train()\n",
    "    fin_loss = 0.0\n",
    "    tk = tqdm(data_loader, desc = \"Training epoch: \" + str(epoch+1))\n",
    "\n",
    "    for t,data in enumerate(tk):\n",
    "        optimizer.zero_grad()\n",
    "        for k,v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "\n",
    "        _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        fin_loss += loss.item() \n",
    "\n",
    "        tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1)), 'LR' : optimizer.param_groups[0]['lr']})\n",
    "\n",
    "    scheduler.step()\n",
    "    return fin_loss / len(data_loader)\n",
    "\n",
    "\n",
    "def eval_fn(model, data_loader, epoch, device):\n",
    "    model.eval()\n",
    "    fin_loss = 0.0\n",
    "    tk = tqdm(data_loader, desc = \"Validation epoch: \" + str(epoch+1))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t,data in enumerate(tk):\n",
    "            for k,v in data.items():\n",
    "                data[k] = v.to(device)\n",
    "\n",
    "            _, loss = model(**data)\n",
    "            fin_loss += loss.item() \n",
    "\n",
    "            tk.set_postfix({'loss' : '%.6f' %float(fin_loss/(t+1))})\n",
    "        return fin_loss / len(data_loader)\n",
    "\n",
    "#dataset\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class Product_Images(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        self.df = df \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        row = self.df.iloc[idx]\n",
    "        label = row.label_group\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, row.image)\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        return {\n",
    "            'image' : image,\n",
    "            'label' : torch.tensor(label).long()\n",
    "        }\n",
    "\n",
    "\n",
    "#custom_scheduler.py\n",
    "import torch \n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "\n",
    "class ProductImageScheduler(_LRScheduler):\n",
    "    def __init__(self, optimizer, lr_start=5e-6, lr_max=1e-5,\n",
    "                 lr_min=1e-6, lr_ramp_ep=5, lr_sus_ep=0, lr_decay=0.4,\n",
    "                 last_epoch=-1):\n",
    "        self.lr_start = lr_start\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_ramp_ep = lr_ramp_ep\n",
    "        self.lr_sus_ep = lr_sus_ep\n",
    "        self.lr_decay = lr_decay\n",
    "        super(ProductImageScheduler, self).__init__(optimizer, last_epoch)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        if not self._get_lr_called_within_step:\n",
    "            warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
    "                          \"please use `get_last_lr()`.\", UserWarning)\n",
    "        if self.last_epoch == 0:\n",
    "            self.last_epoch += 1\n",
    "            return [self.lr_start for _ in self.optimizer.param_groups]\n",
    "        lr = self._compute_lr_from_epoch()\n",
    "        self.last_epoch += 1\n",
    "        return [lr for _ in self.optimizer.param_groups]\n",
    "    \n",
    "    def _get_closed_form_lr(self):\n",
    "        return self.base_lrs\n",
    "    \n",
    "    def _compute_lr_from_epoch(self):\n",
    "        if self.last_epoch < self.lr_ramp_ep:\n",
    "            lr = ((self.lr_max - self.lr_start) / \n",
    "                  self.lr_ramp_ep * self.last_epoch + \n",
    "                  self.lr_start)\n",
    "        elif self.last_epoch < self.lr_ramp_ep + self.lr_sus_ep:\n",
    "            lr = self.lr_max\n",
    "        else:\n",
    "            lr = ((self.lr_max - self.lr_min) * self.lr_decay**\n",
    "                  (self.last_epoch - self.lr_ramp_ep - self.lr_sus_ep) + \n",
    "                  self.lr_min)\n",
    "        return lr\n",
    "\n",
    "import albumentations\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "\n",
    "def get_train_transforms(img_size=512):\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Resize(img_size, img_size, always_apply=True),\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.VerticalFlip(p=0.5),\n",
    "        albumentations.Rotate(limit=120, p=0.8),\n",
    "        albumentations.RandomBrightness(limit=(0.09, 0.6), p=0.5),\n",
    "        albumentations.Normalize(\n",
    "            mean = [0.485, 0.456, 0.406],\n",
    "            std = [0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])\n",
    "\n",
    "def get_valid_transforms(img_size=512):\n",
    "\n",
    "    return albumentations.Compose([\n",
    "        albumentations.Resize(img_size, img_size, always_apply=True),\n",
    "        albumentations.Normalize(\n",
    "            mean = [0.485, 0.456, 0.406],\n",
    "            std = [0.229, 0.224, 0.225]\n",
    "        ),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYsCNqxt2PFH"
   },
   "source": [
    "# Config and Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGapuMge2PFI"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'input/shopee-product-matching/train_images'\n",
    "TRAIN_CSV = 'input/utils-shopee/folds.csv'\n",
    "MODEL_PATH = 'output/'\n",
    "\n",
    "\n",
    "class pretrained_model_config:\n",
    "    seed = 54\n",
    "    img_size = 512\n",
    "    classes = 11014\n",
    "    scale = 30\n",
    "    margin = 0.5\n",
    "    fc_dim = 512\n",
    "    epochs = 10\n",
    "    batch_size = 4\n",
    "    num_workers = 4\n",
    "    model_name = 'tf_efficientnet_b6'\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    scheduler_params = {\n",
    "        \"lr_start\": 1e-5,\n",
    "        \"lr_max\": 1e-5 * batch_size,     # 1e-5 * 32 (if batch_size(=32) is different then)\n",
    "        \"lr_min\": 1e-6,\n",
    "        \"lr_ramp_ep\": 5,\n",
    "        \"lr_sus_ep\": 0,\n",
    "        \"lr_decay\": 0.8,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QR6AZ_4B2PFJ"
   },
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWqV5gOS2PFJ"
   },
   "outputs": [],
   "source": [
    "class ImageRecog_MarginLoss_ArcFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ImageRecog_MarginLoss_ArcFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "    \n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps / self.out_features\n",
    "\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output *= self.scale\n",
    "        return output, nn.CrossEntropyLoss()(output,label)\n",
    "\n",
    "\n",
    "class ProductImageModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes = pretrained_model_config.classes,\n",
    "        model_name = pretrained_model_config.model_name,\n",
    "        fc_dim = pretrained_model_config.fc_dim,\n",
    "        margin = pretrained_model_config.margin,\n",
    "        scale = pretrained_model_config.scale,\n",
    "        use_fc = True,\n",
    "        pretrained = True):\n",
    "\n",
    "        super(ProductImageModel,self).__init__()\n",
    "        print('Building Model Backbone for {} model'.format(model_name))\n",
    "\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        in_features = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        self.backbone.global_pool = nn.Identity()\n",
    "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
    "        self.use_fc = use_fc\n",
    "\n",
    "        if use_fc:\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.classifier = nn.Linear(in_features, fc_dim)\n",
    "            self.bn = nn.BatchNorm1d(fc_dim)\n",
    "            self._init_params()\n",
    "            in_features = fc_dim\n",
    "\n",
    "        self.final = ImageRecog_MarginLoss_ArcFace(\n",
    "            in_features,\n",
    "            n_classes,\n",
    "            scale = scale,\n",
    "            margin = margin,\n",
    "            easy_margin = False,\n",
    "            ls_eps = 0.0\n",
    "        )\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.classifier.weight)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, image, label):\n",
    "        features = self.extract_features(image)\n",
    "        if self.training:\n",
    "            logits = self.final(features, label)\n",
    "            return logits\n",
    "        else:\n",
    "            return features\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        if self.use_fc and self.training:\n",
    "            x = self.dropout(x)\n",
    "            x = self.classifier(x)\n",
    "            x = self.bn(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXDkeoEs2PFK"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5yLHD5B2PFK"
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    \n",
    "    df = pd.read_csv('input/shopee-product-matching/train.csv')\n",
    "\n",
    "    labelencoder= LabelEncoder()\n",
    "    df['label_group'] = labelencoder.fit_transform(df['label_group'])\n",
    "\n",
    "    trainset = Product_Images(df,\n",
    "                             DATA_DIR,\n",
    "                             transform = get_train_transforms(img_size = pretrained_model_config.img_size))\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size = pretrained_model_config.batch_size,\n",
    "        num_workers = pretrained_model_config.num_workers,\n",
    "        pin_memory = True,\n",
    "        shuffle = True,\n",
    "        drop_last = True\n",
    "    )\n",
    "\n",
    "    model = ProductImageModel()\n",
    "    model.to(pretrained_model_config.device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr = pretrained_model_config.scheduler_params['lr_start'])\n",
    "    scheduler = ProductImageScheduler(optimizer, **pretrained_model_config.scheduler_params)\n",
    "\n",
    "    for epoch in range(pretrained_model_config.epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        avg_loss_train = train_fn(model, trainloader, optimizer, scheduler, epoch, pretrained_model_config.device)\n",
    "        torch.save(model.state_dict(), MODEL_PATH + 'arcface_512x512_{}.pt'.format(pretrained_model_config.model_name))\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "            },\n",
    "            MODEL_PATH + 'arcface_512x512_{}_checkpoints.pt'.format(pretrained_model_config.model_name)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "ztwY3JvR2PFL",
    "outputId": "fd1a07d8-c79e-447f-a6f2-8950a595d7d5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model Backbone for tf_efficientnet_b6 model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Training epoch: 10:  71%|███████   | 6062/8562 [24:35<09:55,  4.20it/s, loss=16.646075, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6063/8562 [24:35<09:51,  4.22it/s, loss=16.646075, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6063/8562 [24:36<09:51,  4.22it/s, loss=16.646473, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6064/8562 [24:36<09:53,  4.21it/s, loss=16.646473, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6064/8562 [24:36<09:53,  4.21it/s, loss=16.647086, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6065/8562 [24:36<09:49,  4.23it/s, loss=16.647086, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6065/8562 [24:36<09:49,  4.23it/s, loss=16.647514, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6066/8562 [24:36<09:54,  4.20it/s, loss=16.647514, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6066/8562 [24:36<09:54,  4.20it/s, loss=16.646045, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6067/8562 [24:36<09:52,  4.21it/s, loss=16.646045, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6067/8562 [24:36<09:52,  4.21it/s, loss=16.646711, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6068/8562 [24:36<09:51,  4.22it/s, loss=16.646711, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6068/8562 [24:37<09:51,  4.22it/s, loss=16.646756, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6069/8562 [24:37<09:48,  4.24it/s, loss=16.646756, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6069/8562 [24:37<09:48,  4.24it/s, loss=16.646912, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6070/8562 [24:37<09:45,  4.26it/s, loss=16.646912, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6070/8562 [24:37<09:45,  4.26it/s, loss=16.646498, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6071/8562 [24:37<09:44,  4.26it/s, loss=16.646498, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6071/8562 [24:37<09:44,  4.26it/s, loss=16.646761, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6072/8562 [24:37<09:44,  4.26it/s, loss=16.646761, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6072/8562 [24:38<09:44,  4.26it/s, loss=16.646048, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6073/8562 [24:38<09:50,  4.22it/s, loss=16.646048, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6073/8562 [24:38<09:50,  4.22it/s, loss=16.646069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6074/8562 [24:38<09:46,  4.24it/s, loss=16.646069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6074/8562 [24:38<09:46,  4.24it/s, loss=16.645659, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6075/8562 [24:38<09:50,  4.21it/s, loss=16.645659, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6075/8562 [24:38<09:50,  4.21it/s, loss=16.646166, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6076/8562 [24:38<09:48,  4.23it/s, loss=16.646166, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6076/8562 [24:39<09:48,  4.23it/s, loss=16.646304, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6077/8562 [24:39<09:49,  4.22it/s, loss=16.646304, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6077/8562 [24:39<09:49,  4.22it/s, loss=16.645967, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6078/8562 [24:39<09:47,  4.23it/s, loss=16.645967, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6078/8562 [24:39<09:47,  4.23it/s, loss=16.646637, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6079/8562 [24:39<09:48,  4.22it/s, loss=16.646637, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6079/8562 [24:39<09:48,  4.22it/s, loss=16.646371, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6080/8562 [24:39<10:00,  4.14it/s, loss=16.646371, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6080/8562 [24:40<10:00,  4.14it/s, loss=16.646373, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6081/8562 [24:40<09:57,  4.15it/s, loss=16.646373, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6081/8562 [24:40<09:57,  4.15it/s, loss=16.645652, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6082/8562 [24:40<10:06,  4.09it/s, loss=16.645652, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6082/8562 [24:40<10:06,  4.09it/s, loss=16.645957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6083/8562 [24:40<09:58,  4.14it/s, loss=16.645957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6083/8562 [24:40<09:58,  4.14it/s, loss=16.645987, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6084/8562 [24:40<09:54,  4.17it/s, loss=16.645987, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6084/8562 [24:41<09:54,  4.17it/s, loss=16.646381, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6085/8562 [24:41<09:48,  4.21it/s, loss=16.646381, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6085/8562 [24:41<09:48,  4.21it/s, loss=16.645683, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6086/8562 [24:41<09:46,  4.22it/s, loss=16.645683, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6086/8562 [24:41<09:46,  4.22it/s, loss=16.646372, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6087/8562 [24:41<09:46,  4.22it/s, loss=16.646372, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6087/8562 [24:41<09:46,  4.22it/s, loss=16.646189, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6088/8562 [24:41<09:41,  4.25it/s, loss=16.646189, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6088/8562 [24:41<09:41,  4.25it/s, loss=16.646615, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6089/8562 [24:41<09:42,  4.25it/s, loss=16.646615, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6089/8562 [24:42<09:42,  4.25it/s, loss=16.647084, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6090/8562 [24:42<09:36,  4.29it/s, loss=16.647084, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6090/8562 [24:42<09:36,  4.29it/s, loss=16.647313, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6091/8562 [24:42<09:38,  4.27it/s, loss=16.647313, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6091/8562 [24:42<09:38,  4.27it/s, loss=16.646595, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6092/8562 [24:42<09:42,  4.24it/s, loss=16.646595, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6092/8562 [24:42<09:42,  4.24it/s, loss=16.646398, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6093/8562 [24:42<09:46,  4.21it/s, loss=16.646398, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6093/8562 [24:43<09:46,  4.21it/s, loss=16.646551, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6094/8562 [24:43<09:43,  4.23it/s, loss=16.646551, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6094/8562 [24:43<09:43,  4.23it/s, loss=16.646878, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6095/8562 [24:43<09:42,  4.23it/s, loss=16.646878, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6095/8562 [24:43<09:42,  4.23it/s, loss=16.646486, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6096/8562 [24:43<09:41,  4.24it/s, loss=16.646486, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6096/8562 [24:43<09:41,  4.24it/s, loss=16.645852, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6097/8562 [24:43<09:42,  4.23it/s, loss=16.645852, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6097/8562 [24:44<09:42,  4.23it/s, loss=16.645025, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6098/8562 [24:44<09:45,  4.20it/s, loss=16.645025, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6098/8562 [24:44<09:45,  4.20it/s, loss=16.645257, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6099/8562 [24:44<09:42,  4.23it/s, loss=16.645257, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6099/8562 [24:44<09:42,  4.23it/s, loss=16.645237, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6100/8562 [24:44<09:41,  4.23it/s, loss=16.645237, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████   | 6100/8562 [24:44<09:41,  4.23it/s, loss=16.645329, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6101/8562 [24:44<10:25,  3.94it/s, loss=16.645329, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6101/8562 [24:45<10:25,  3.94it/s, loss=16.645556, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6102/8562 [24:45<10:13,  4.01it/s, loss=16.645556, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6102/8562 [24:45<10:13,  4.01it/s, loss=16.646169, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6103/8562 [24:45<10:06,  4.05it/s, loss=16.646169, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6103/8562 [24:45<10:06,  4.05it/s, loss=16.646914, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6104/8562 [24:45<10:06,  4.05it/s, loss=16.646914, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6104/8562 [24:45<10:06,  4.05it/s, loss=16.646678, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6105/8562 [24:45<10:07,  4.05it/s, loss=16.646678, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6105/8562 [24:46<10:07,  4.05it/s, loss=16.646585, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6106/8562 [24:46<09:57,  4.11it/s, loss=16.646585, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6106/8562 [24:46<09:57,  4.11it/s, loss=16.646860, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6107/8562 [24:46<09:49,  4.17it/s, loss=16.646860, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6107/8562 [24:46<09:49,  4.17it/s, loss=16.646522, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6108/8562 [24:46<09:45,  4.19it/s, loss=16.646522, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6108/8562 [24:46<09:45,  4.19it/s, loss=16.646221, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6109/8562 [24:46<10:03,  4.06it/s, loss=16.646221, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6109/8562 [24:47<10:03,  4.06it/s, loss=16.645546, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6110/8562 [24:47<09:56,  4.11it/s, loss=16.645546, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6110/8562 [24:47<09:56,  4.11it/s, loss=16.646006, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6111/8562 [24:47<09:49,  4.16it/s, loss=16.646006, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6111/8562 [24:47<09:49,  4.16it/s, loss=16.645666, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6112/8562 [24:47<09:47,  4.17it/s, loss=16.645666, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6112/8562 [24:47<09:47,  4.17it/s, loss=16.645991, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6113/8562 [24:47<09:56,  4.11it/s, loss=16.645991, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6113/8562 [24:47<09:56,  4.11it/s, loss=16.645434, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6114/8562 [24:47<09:49,  4.15it/s, loss=16.645434, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6114/8562 [24:48<09:49,  4.15it/s, loss=16.645439, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6115/8562 [24:48<09:48,  4.16it/s, loss=16.645439, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6115/8562 [24:48<09:48,  4.16it/s, loss=16.645896, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6116/8562 [24:48<09:56,  4.10it/s, loss=16.645896, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6116/8562 [24:48<09:56,  4.10it/s, loss=16.645726, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6117/8562 [24:48<10:05,  4.04it/s, loss=16.645726, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6117/8562 [24:48<10:05,  4.04it/s, loss=16.645345, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6118/8562 [24:48<10:15,  3.97it/s, loss=16.645345, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6118/8562 [24:49<10:15,  3.97it/s, loss=16.645734, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6119/8562 [24:49<10:06,  4.03it/s, loss=16.645734, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6119/8562 [24:49<10:06,  4.03it/s, loss=16.645699, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6120/8562 [24:49<09:59,  4.08it/s, loss=16.645699, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6120/8562 [24:49<09:59,  4.08it/s, loss=16.645488, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6121/8562 [24:49<09:56,  4.09it/s, loss=16.645488, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  71%|███████▏  | 6121/8562 [24:49<09:56,  4.09it/s, loss=16.645936, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6122/8562 [24:49<09:48,  4.14it/s, loss=16.645936, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6122/8562 [24:50<09:48,  4.14it/s, loss=16.646585, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6123/8562 [24:50<09:48,  4.14it/s, loss=16.646585, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6123/8562 [24:50<09:48,  4.14it/s, loss=16.646492, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6124/8562 [24:50<09:42,  4.19it/s, loss=16.646492, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6124/8562 [24:50<09:42,  4.19it/s, loss=16.646859, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6125/8562 [24:50<09:38,  4.21it/s, loss=16.646859, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6125/8562 [24:50<09:38,  4.21it/s, loss=16.647527, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6126/8562 [24:50<09:38,  4.21it/s, loss=16.647527, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6126/8562 [24:51<09:38,  4.21it/s, loss=16.647357, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6127/8562 [24:51<09:39,  4.20it/s, loss=16.647357, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6127/8562 [24:51<09:39,  4.20it/s, loss=16.647258, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6128/8562 [24:51<09:35,  4.23it/s, loss=16.647258, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6128/8562 [24:51<09:35,  4.23it/s, loss=16.647061, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6129/8562 [24:51<09:31,  4.25it/s, loss=16.647061, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6129/8562 [24:51<09:31,  4.25it/s, loss=16.646980, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6130/8562 [24:51<09:38,  4.21it/s, loss=16.646980, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6130/8562 [24:52<09:38,  4.21it/s, loss=16.647629, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6131/8562 [24:52<09:34,  4.23it/s, loss=16.647629, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6131/8562 [24:52<09:34,  4.23it/s, loss=16.647660, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6132/8562 [24:52<09:41,  4.18it/s, loss=16.647660, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6132/8562 [24:52<09:41,  4.18it/s, loss=16.647337, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6133/8562 [24:52<09:41,  4.18it/s, loss=16.647337, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6133/8562 [24:52<09:41,  4.18it/s, loss=16.648152, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6134/8562 [24:52<09:47,  4.13it/s, loss=16.648152, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6134/8562 [24:53<09:47,  4.13it/s, loss=16.647284, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6135/8562 [24:53<09:38,  4.19it/s, loss=16.647284, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6135/8562 [24:53<09:38,  4.19it/s, loss=16.647313, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6136/8562 [24:53<09:37,  4.20it/s, loss=16.647313, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6136/8562 [24:53<09:37,  4.20it/s, loss=16.647140, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6137/8562 [24:53<09:35,  4.21it/s, loss=16.647140, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6137/8562 [24:53<09:35,  4.21it/s, loss=16.647907, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6138/8562 [24:53<09:29,  4.25it/s, loss=16.647907, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6138/8562 [24:53<09:29,  4.25it/s, loss=16.648467, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6139/8562 [24:53<09:26,  4.28it/s, loss=16.648467, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6139/8562 [24:54<09:26,  4.28it/s, loss=16.647564, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6140/8562 [24:54<09:24,  4.29it/s, loss=16.647564, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6140/8562 [24:54<09:24,  4.29it/s, loss=16.648325, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6141/8562 [24:54<09:23,  4.30it/s, loss=16.648325, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6141/8562 [24:54<09:23,  4.30it/s, loss=16.647704, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6142/8562 [24:54<09:24,  4.29it/s, loss=16.647704, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6142/8562 [24:54<09:24,  4.29it/s, loss=16.648102, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6143/8562 [24:54<09:29,  4.25it/s, loss=16.648102, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6143/8562 [24:55<09:29,  4.25it/s, loss=16.648526, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6144/8562 [24:55<09:27,  4.26it/s, loss=16.648526, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6144/8562 [24:55<09:27,  4.26it/s, loss=16.649058, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6145/8562 [24:55<09:24,  4.28it/s, loss=16.649058, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6145/8562 [24:55<09:24,  4.28it/s, loss=16.649567, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6146/8562 [24:55<09:23,  4.29it/s, loss=16.649567, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6146/8562 [24:55<09:23,  4.29it/s, loss=16.649836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6147/8562 [24:55<09:23,  4.29it/s, loss=16.649836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6147/8562 [24:56<09:23,  4.29it/s, loss=16.650265, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6148/8562 [24:56<09:26,  4.26it/s, loss=16.650265, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6148/8562 [24:56<09:26,  4.26it/s, loss=16.650544, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6149/8562 [24:56<09:37,  4.18it/s, loss=16.650544, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6149/8562 [24:56<09:37,  4.18it/s, loss=16.649388, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6150/8562 [24:56<09:32,  4.21it/s, loss=16.649388, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6150/8562 [24:56<09:32,  4.21it/s, loss=16.649856, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6151/8562 [24:56<09:39,  4.16it/s, loss=16.649856, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6151/8562 [24:57<09:39,  4.16it/s, loss=16.649448, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6152/8562 [24:57<09:33,  4.20it/s, loss=16.649448, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6152/8562 [24:57<09:33,  4.20it/s, loss=16.649191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6153/8562 [24:57<09:29,  4.23it/s, loss=16.649191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6153/8562 [24:57<09:29,  4.23it/s, loss=16.649065, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6154/8562 [24:57<10:05,  3.98it/s, loss=16.649065, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6154/8562 [24:57<10:05,  3.98it/s, loss=16.648665, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6155/8562 [24:57<11:03,  3.63it/s, loss=16.648665, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6155/8562 [24:58<11:03,  3.63it/s, loss=16.648928, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6156/8562 [24:58<11:35,  3.46it/s, loss=16.648928, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6156/8562 [24:58<11:35,  3.46it/s, loss=16.648764, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6157/8562 [24:58<11:49,  3.39it/s, loss=16.648764, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6157/8562 [24:58<11:49,  3.39it/s, loss=16.648984, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6158/8562 [24:58<12:08,  3.30it/s, loss=16.648984, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6158/8562 [24:59<12:08,  3.30it/s, loss=16.648659, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6159/8562 [24:59<12:34,  3.18it/s, loss=16.648659, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6159/8562 [24:59<12:34,  3.18it/s, loss=16.648472, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6160/8562 [24:59<12:55,  3.10it/s, loss=16.648472, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6160/8562 [24:59<12:55,  3.10it/s, loss=16.648493, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6161/8562 [24:59<13:10,  3.04it/s, loss=16.648493, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6161/8562 [25:00<13:10,  3.04it/s, loss=16.649005, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6162/8562 [25:00<13:30,  2.96it/s, loss=16.649005, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6162/8562 [25:00<13:30,  2.96it/s, loss=16.649051, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6163/8562 [25:00<12:57,  3.08it/s, loss=16.649051, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6163/8562 [25:00<12:57,  3.08it/s, loss=16.648515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6164/8562 [25:00<13:23,  2.98it/s, loss=16.648515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6164/8562 [25:01<13:23,  2.98it/s, loss=16.648874, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6165/8562 [25:01<13:04,  3.05it/s, loss=16.648874, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6165/8562 [25:01<13:04,  3.05it/s, loss=16.649244, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6166/8562 [25:01<12:13,  3.27it/s, loss=16.649244, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6166/8562 [25:01<12:13,  3.27it/s, loss=16.648372, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6167/8562 [25:01<12:18,  3.24it/s, loss=16.648372, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6167/8562 [25:02<12:18,  3.24it/s, loss=16.648276, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6168/8562 [25:02<12:54,  3.09it/s, loss=16.648276, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6168/8562 [25:02<12:54,  3.09it/s, loss=16.648197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6169/8562 [25:02<12:46,  3.12it/s, loss=16.648197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6169/8562 [25:02<12:46,  3.12it/s, loss=16.648690, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6170/8562 [25:02<13:20,  2.99it/s, loss=16.648690, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6170/8562 [25:03<13:20,  2.99it/s, loss=16.648584, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6171/8562 [25:03<13:43,  2.90it/s, loss=16.648584, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6171/8562 [25:03<13:43,  2.90it/s, loss=16.648917, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6172/8562 [25:03<13:42,  2.90it/s, loss=16.648917, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6172/8562 [25:03<13:42,  2.90it/s, loss=16.647913, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6173/8562 [25:03<12:57,  3.07it/s, loss=16.647913, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6173/8562 [25:04<12:57,  3.07it/s, loss=16.647381, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6174/8562 [25:04<12:20,  3.22it/s, loss=16.647381, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6174/8562 [25:04<12:20,  3.22it/s, loss=16.647234, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6175/8562 [25:04<12:16,  3.24it/s, loss=16.647234, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6175/8562 [25:04<12:16,  3.24it/s, loss=16.647774, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6176/8562 [25:04<12:11,  3.26it/s, loss=16.647774, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6176/8562 [25:04<12:11,  3.26it/s, loss=16.648106, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6177/8562 [25:04<11:33,  3.44it/s, loss=16.648106, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6177/8562 [25:05<11:33,  3.44it/s, loss=16.648195, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6178/8562 [25:05<10:53,  3.65it/s, loss=16.648195, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6178/8562 [25:05<10:53,  3.65it/s, loss=16.648590, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6179/8562 [25:05<10:25,  3.81it/s, loss=16.648590, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6179/8562 [25:05<10:25,  3.81it/s, loss=16.648984, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6180/8562 [25:05<10:05,  3.94it/s, loss=16.648984, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6180/8562 [25:05<10:05,  3.94it/s, loss=16.649164, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6181/8562 [25:05<09:52,  4.02it/s, loss=16.649164, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6181/8562 [25:06<09:52,  4.02it/s, loss=16.650023, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6182/8562 [25:06<09:49,  4.04it/s, loss=16.650023, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6182/8562 [25:06<09:49,  4.04it/s, loss=16.649712, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6183/8562 [25:06<09:38,  4.11it/s, loss=16.649712, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6183/8562 [25:06<09:38,  4.11it/s, loss=16.649328, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6184/8562 [25:06<09:32,  4.15it/s, loss=16.649328, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6184/8562 [25:06<09:32,  4.15it/s, loss=16.649019, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6185/8562 [25:06<09:40,  4.09it/s, loss=16.649019, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6185/8562 [25:07<09:40,  4.09it/s, loss=16.649243, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6186/8562 [25:07<09:35,  4.13it/s, loss=16.649243, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6186/8562 [25:07<09:35,  4.13it/s, loss=16.649259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6187/8562 [25:07<09:33,  4.14it/s, loss=16.649259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6187/8562 [25:07<09:33,  4.14it/s, loss=16.649277, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6188/8562 [25:07<09:29,  4.17it/s, loss=16.649277, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6188/8562 [25:07<09:29,  4.17it/s, loss=16.648702, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6189/8562 [25:07<09:31,  4.15it/s, loss=16.648702, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6189/8562 [25:08<09:31,  4.15it/s, loss=16.648961, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6190/8562 [25:08<09:29,  4.17it/s, loss=16.648961, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6190/8562 [25:08<09:29,  4.17it/s, loss=16.648200, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6191/8562 [25:08<09:28,  4.17it/s, loss=16.648200, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6191/8562 [25:08<09:28,  4.17it/s, loss=16.648776, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6192/8562 [25:08<09:24,  4.20it/s, loss=16.648776, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6192/8562 [25:08<09:24,  4.20it/s, loss=16.648661, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6193/8562 [25:08<09:29,  4.16it/s, loss=16.648661, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6193/8562 [25:08<09:29,  4.16it/s, loss=16.646944, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6194/8562 [25:08<09:26,  4.18it/s, loss=16.646944, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6194/8562 [25:09<09:26,  4.18it/s, loss=16.647185, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6195/8562 [25:09<09:42,  4.06it/s, loss=16.647185, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6195/8562 [25:09<09:42,  4.06it/s, loss=16.647452, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6196/8562 [25:09<09:37,  4.10it/s, loss=16.647452, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6196/8562 [25:09<09:37,  4.10it/s, loss=16.647652, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6197/8562 [25:09<09:39,  4.08it/s, loss=16.647652, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6197/8562 [25:09<09:39,  4.08it/s, loss=16.647956, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6198/8562 [25:09<09:42,  4.06it/s, loss=16.647956, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6198/8562 [25:10<09:42,  4.06it/s, loss=16.647715, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6199/8562 [25:10<09:32,  4.12it/s, loss=16.647715, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6199/8562 [25:10<09:32,  4.12it/s, loss=16.647578, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6200/8562 [25:10<09:27,  4.16it/s, loss=16.647578, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6200/8562 [25:10<09:27,  4.16it/s, loss=16.648342, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6201/8562 [25:10<09:25,  4.17it/s, loss=16.648342, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6201/8562 [25:10<09:25,  4.17it/s, loss=16.648256, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6202/8562 [25:10<09:25,  4.17it/s, loss=16.648256, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6202/8562 [25:11<09:25,  4.17it/s, loss=16.647933, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6203/8562 [25:11<09:22,  4.20it/s, loss=16.647933, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6203/8562 [25:11<09:22,  4.20it/s, loss=16.647361, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6204/8562 [25:11<09:20,  4.20it/s, loss=16.647361, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6204/8562 [25:11<09:20,  4.20it/s, loss=16.647086, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6205/8562 [25:11<09:18,  4.22it/s, loss=16.647086, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6205/8562 [25:11<09:18,  4.22it/s, loss=16.647494, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6206/8562 [25:11<09:13,  4.26it/s, loss=16.647494, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6206/8562 [25:12<09:13,  4.26it/s, loss=16.647236, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6207/8562 [25:12<09:13,  4.25it/s, loss=16.647236, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  72%|███████▏  | 6207/8562 [25:12<09:13,  4.25it/s, loss=16.647321, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6208/8562 [25:12<09:17,  4.22it/s, loss=16.647321, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6208/8562 [25:12<09:17,  4.22it/s, loss=16.647000, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6209/8562 [25:12<09:20,  4.20it/s, loss=16.647000, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6209/8562 [25:12<09:20,  4.20it/s, loss=16.646717, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6210/8562 [25:12<09:16,  4.23it/s, loss=16.646717, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6210/8562 [25:13<09:16,  4.23it/s, loss=16.646199, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6211/8562 [25:13<09:14,  4.24it/s, loss=16.646199, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6211/8562 [25:13<09:14,  4.24it/s, loss=16.646637, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6212/8562 [25:13<09:15,  4.23it/s, loss=16.646637, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6212/8562 [25:13<09:15,  4.23it/s, loss=16.646883, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6213/8562 [25:13<09:12,  4.26it/s, loss=16.646883, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6213/8562 [25:13<09:12,  4.26it/s, loss=16.647500, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6214/8562 [25:13<09:10,  4.27it/s, loss=16.647500, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6214/8562 [25:13<09:10,  4.27it/s, loss=16.646819, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6215/8562 [25:13<09:09,  4.27it/s, loss=16.646819, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6215/8562 [25:14<09:09,  4.27it/s, loss=16.647109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6216/8562 [25:14<09:10,  4.26it/s, loss=16.647109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6216/8562 [25:14<09:10,  4.26it/s, loss=16.646190, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6217/8562 [25:14<09:10,  4.26it/s, loss=16.646190, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6217/8562 [25:14<09:10,  4.26it/s, loss=16.645917, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6218/8562 [25:14<09:13,  4.23it/s, loss=16.645917, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6218/8562 [25:14<09:13,  4.23it/s, loss=16.646061, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6219/8562 [25:14<09:10,  4.25it/s, loss=16.646061, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6219/8562 [25:15<09:10,  4.25it/s, loss=16.646222, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6220/8562 [25:15<09:08,  4.27it/s, loss=16.646222, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6220/8562 [25:15<09:08,  4.27it/s, loss=16.646572, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6221/8562 [25:15<09:11,  4.25it/s, loss=16.646572, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6221/8562 [25:15<09:11,  4.25it/s, loss=16.646772, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6222/8562 [25:15<09:07,  4.27it/s, loss=16.646772, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6222/8562 [25:15<09:07,  4.27it/s, loss=16.647214, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6223/8562 [25:15<09:20,  4.17it/s, loss=16.647214, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6223/8562 [25:16<09:20,  4.17it/s, loss=16.647508, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6224/8562 [25:16<09:20,  4.17it/s, loss=16.647508, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6224/8562 [25:16<09:20,  4.17it/s, loss=16.647888, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6225/8562 [25:16<09:19,  4.17it/s, loss=16.647888, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6225/8562 [25:16<09:19,  4.17it/s, loss=16.648632, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6226/8562 [25:16<09:18,  4.18it/s, loss=16.648632, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6226/8562 [25:16<09:18,  4.18it/s, loss=16.648523, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6227/8562 [25:16<09:21,  4.16it/s, loss=16.648523, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6227/8562 [25:17<09:21,  4.16it/s, loss=16.648681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6228/8562 [25:17<09:15,  4.20it/s, loss=16.648681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6228/8562 [25:17<09:15,  4.20it/s, loss=16.649249, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6229/8562 [25:17<09:11,  4.23it/s, loss=16.649249, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6229/8562 [25:17<09:11,  4.23it/s, loss=16.649210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6230/8562 [25:17<09:09,  4.24it/s, loss=16.649210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6230/8562 [25:17<09:09,  4.24it/s, loss=16.648974, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6231/8562 [25:17<09:09,  4.25it/s, loss=16.648974, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6231/8562 [25:18<09:09,  4.25it/s, loss=16.649068, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6232/8562 [25:18<09:08,  4.25it/s, loss=16.649068, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6232/8562 [25:18<09:08,  4.25it/s, loss=16.649570, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6233/8562 [25:18<09:10,  4.23it/s, loss=16.649570, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6233/8562 [25:18<09:10,  4.23it/s, loss=16.649375, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6234/8562 [25:18<09:09,  4.23it/s, loss=16.649375, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6234/8562 [25:18<09:09,  4.23it/s, loss=16.648398, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6235/8562 [25:18<09:06,  4.26it/s, loss=16.648398, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6235/8562 [25:18<09:06,  4.26it/s, loss=16.648394, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6236/8562 [25:18<09:07,  4.25it/s, loss=16.648394, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6236/8562 [25:19<09:07,  4.25it/s, loss=16.647848, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6237/8562 [25:19<09:11,  4.22it/s, loss=16.647848, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6237/8562 [25:19<09:11,  4.22it/s, loss=16.647695, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6238/8562 [25:19<09:18,  4.16it/s, loss=16.647695, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6238/8562 [25:19<09:18,  4.16it/s, loss=16.648053, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6239/8562 [25:19<09:17,  4.17it/s, loss=16.648053, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6239/8562 [25:19<09:17,  4.17it/s, loss=16.647773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6240/8562 [25:19<09:24,  4.11it/s, loss=16.647773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6240/8562 [25:20<09:24,  4.11it/s, loss=16.647599, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6241/8562 [25:20<09:25,  4.10it/s, loss=16.647599, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6241/8562 [25:20<09:25,  4.10it/s, loss=16.647852, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6242/8562 [25:20<09:15,  4.18it/s, loss=16.647852, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6242/8562 [25:20<09:15,  4.18it/s, loss=16.648063, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6243/8562 [25:20<09:08,  4.23it/s, loss=16.648063, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6243/8562 [25:20<09:08,  4.23it/s, loss=16.648424, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6244/8562 [25:20<09:12,  4.19it/s, loss=16.648424, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6244/8562 [25:21<09:12,  4.19it/s, loss=16.648379, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6245/8562 [25:21<09:08,  4.23it/s, loss=16.648379, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6245/8562 [25:21<09:08,  4.23it/s, loss=16.648182, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6246/8562 [25:21<09:14,  4.18it/s, loss=16.648182, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6246/8562 [25:21<09:14,  4.18it/s, loss=16.648490, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6247/8562 [25:21<09:11,  4.20it/s, loss=16.648490, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6247/8562 [25:21<09:11,  4.20it/s, loss=16.647429, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6248/8562 [25:21<09:09,  4.21it/s, loss=16.647429, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6248/8562 [25:22<09:09,  4.21it/s, loss=16.647409, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6249/8562 [25:22<09:05,  4.24it/s, loss=16.647409, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6249/8562 [25:22<09:05,  4.24it/s, loss=16.647809, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6250/8562 [25:22<09:06,  4.23it/s, loss=16.647809, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6250/8562 [25:22<09:06,  4.23it/s, loss=16.647713, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6251/8562 [25:22<09:05,  4.24it/s, loss=16.647713, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6251/8562 [25:22<09:05,  4.24it/s, loss=16.648017, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6252/8562 [25:22<09:03,  4.25it/s, loss=16.648017, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6252/8562 [25:23<09:03,  4.25it/s, loss=16.648650, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6253/8562 [25:23<09:05,  4.23it/s, loss=16.648650, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6253/8562 [25:23<09:05,  4.23it/s, loss=16.647897, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6254/8562 [25:23<09:06,  4.23it/s, loss=16.647897, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6254/8562 [25:23<09:06,  4.23it/s, loss=16.648193, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6255/8562 [25:23<09:03,  4.25it/s, loss=16.648193, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6255/8562 [25:23<09:03,  4.25it/s, loss=16.647743, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6256/8562 [25:23<09:00,  4.26it/s, loss=16.647743, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6256/8562 [25:23<09:00,  4.26it/s, loss=16.647623, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6257/8562 [25:23<09:06,  4.22it/s, loss=16.647623, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6257/8562 [25:24<09:06,  4.22it/s, loss=16.646777, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6258/8562 [25:24<09:04,  4.23it/s, loss=16.646777, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6258/8562 [25:24<09:04,  4.23it/s, loss=16.646369, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6259/8562 [25:24<09:02,  4.25it/s, loss=16.646369, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6259/8562 [25:24<09:02,  4.25it/s, loss=16.646676, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6260/8562 [25:24<09:01,  4.25it/s, loss=16.646676, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6260/8562 [25:24<09:01,  4.25it/s, loss=16.645454, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6261/8562 [25:24<09:04,  4.23it/s, loss=16.645454, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6261/8562 [25:25<09:04,  4.23it/s, loss=16.644972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6262/8562 [25:25<09:04,  4.23it/s, loss=16.644972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6262/8562 [25:25<09:04,  4.23it/s, loss=16.645340, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6263/8562 [25:25<09:03,  4.23it/s, loss=16.645340, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6263/8562 [25:25<09:03,  4.23it/s, loss=16.645121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6264/8562 [25:25<09:07,  4.20it/s, loss=16.645121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6264/8562 [25:25<09:07,  4.20it/s, loss=16.645210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6265/8562 [25:25<09:04,  4.22it/s, loss=16.645210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6265/8562 [25:26<09:04,  4.22it/s, loss=16.645259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6266/8562 [25:26<09:08,  4.18it/s, loss=16.645259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6266/8562 [25:26<09:08,  4.18it/s, loss=16.645112, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6267/8562 [25:26<09:05,  4.21it/s, loss=16.645112, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6267/8562 [25:26<09:05,  4.21it/s, loss=16.645825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6268/8562 [25:26<09:02,  4.23it/s, loss=16.645825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6268/8562 [25:26<09:02,  4.23it/s, loss=16.646109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6269/8562 [25:26<08:59,  4.25it/s, loss=16.646109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6269/8562 [25:27<08:59,  4.25it/s, loss=16.646829, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6270/8562 [25:27<08:58,  4.26it/s, loss=16.646829, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6270/8562 [25:27<08:58,  4.26it/s, loss=16.647579, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6271/8562 [25:27<09:06,  4.19it/s, loss=16.647579, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6271/8562 [25:27<09:06,  4.19it/s, loss=16.647388, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6272/8562 [25:27<09:04,  4.21it/s, loss=16.647388, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6272/8562 [25:27<09:04,  4.21it/s, loss=16.647545, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6273/8562 [25:27<09:01,  4.23it/s, loss=16.647545, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6273/8562 [25:27<09:01,  4.23it/s, loss=16.648304, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6274/8562 [25:27<09:03,  4.21it/s, loss=16.648304, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6274/8562 [25:28<09:03,  4.21it/s, loss=16.648052, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6275/8562 [25:28<09:02,  4.22it/s, loss=16.648052, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6275/8562 [25:28<09:02,  4.22it/s, loss=16.648783, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6276/8562 [25:28<09:01,  4.22it/s, loss=16.648783, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6276/8562 [25:28<09:01,  4.22it/s, loss=16.648745, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6277/8562 [25:28<08:58,  4.24it/s, loss=16.648745, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6277/8562 [25:28<08:58,  4.24it/s, loss=16.648844, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6278/8562 [25:28<09:04,  4.19it/s, loss=16.648844, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6278/8562 [25:29<09:04,  4.19it/s, loss=16.649319, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6279/8562 [25:29<09:00,  4.22it/s, loss=16.649319, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6279/8562 [25:29<09:00,  4.22it/s, loss=16.649424, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6280/8562 [25:29<08:58,  4.24it/s, loss=16.649424, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6280/8562 [25:29<08:58,  4.24it/s, loss=16.649074, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6281/8562 [25:29<09:00,  4.22it/s, loss=16.649074, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6281/8562 [25:29<09:00,  4.22it/s, loss=16.648937, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6282/8562 [25:29<08:57,  4.24it/s, loss=16.648937, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6282/8562 [25:30<08:57,  4.24it/s, loss=16.648547, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6283/8562 [25:30<08:58,  4.23it/s, loss=16.648547, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6283/8562 [25:30<08:58,  4.23it/s, loss=16.648876, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6284/8562 [25:30<08:56,  4.25it/s, loss=16.648876, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6284/8562 [25:30<08:56,  4.25it/s, loss=16.648300, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6285/8562 [25:30<08:59,  4.22it/s, loss=16.648300, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6285/8562 [25:30<08:59,  4.22it/s, loss=16.647807, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6286/8562 [25:30<08:58,  4.23it/s, loss=16.647807, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6286/8562 [25:31<08:58,  4.23it/s, loss=16.647811, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6287/8562 [25:31<08:55,  4.25it/s, loss=16.647811, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6287/8562 [25:31<08:55,  4.25it/s, loss=16.647796, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6288/8562 [25:31<08:54,  4.25it/s, loss=16.647796, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6288/8562 [25:31<08:54,  4.25it/s, loss=16.648281, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6289/8562 [25:31<08:51,  4.28it/s, loss=16.648281, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6289/8562 [25:31<08:51,  4.28it/s, loss=16.647172, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6290/8562 [25:31<08:51,  4.28it/s, loss=16.647172, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6290/8562 [25:31<08:51,  4.28it/s, loss=16.646572, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6291/8562 [25:31<08:49,  4.29it/s, loss=16.646572, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6291/8562 [25:32<08:49,  4.29it/s, loss=16.646635, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6292/8562 [25:32<08:56,  4.23it/s, loss=16.646635, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6292/8562 [25:32<08:56,  4.23it/s, loss=16.646611, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6293/8562 [25:32<08:52,  4.26it/s, loss=16.646611, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  73%|███████▎  | 6293/8562 [25:32<08:52,  4.26it/s, loss=16.646501, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6294/8562 [25:32<08:49,  4.29it/s, loss=16.646501, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6294/8562 [25:32<08:49,  4.29it/s, loss=16.646740, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6295/8562 [25:32<08:50,  4.27it/s, loss=16.646740, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6295/8562 [25:33<08:50,  4.27it/s, loss=16.646932, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6296/8562 [25:33<08:51,  4.27it/s, loss=16.646932, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6296/8562 [25:33<08:51,  4.27it/s, loss=16.646963, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6297/8562 [25:33<08:49,  4.28it/s, loss=16.646963, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6297/8562 [25:33<08:49,  4.28it/s, loss=16.646895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6298/8562 [25:33<08:49,  4.27it/s, loss=16.646895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6298/8562 [25:33<08:49,  4.27it/s, loss=16.646711, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6299/8562 [25:33<08:49,  4.28it/s, loss=16.646711, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6299/8562 [25:34<08:49,  4.28it/s, loss=16.646972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6300/8562 [25:34<08:46,  4.29it/s, loss=16.646972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6300/8562 [25:34<08:46,  4.29it/s, loss=16.647330, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6301/8562 [25:34<08:47,  4.28it/s, loss=16.647330, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6301/8562 [25:34<08:47,  4.28it/s, loss=16.647472, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6302/8562 [25:34<08:44,  4.31it/s, loss=16.647472, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6302/8562 [25:34<08:44,  4.31it/s, loss=16.647170, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6303/8562 [25:34<08:42,  4.32it/s, loss=16.647170, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6303/8562 [25:35<08:42,  4.32it/s, loss=16.647428, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6304/8562 [25:35<08:44,  4.31it/s, loss=16.647428, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6304/8562 [25:35<08:44,  4.31it/s, loss=16.647017, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6305/8562 [25:35<08:45,  4.29it/s, loss=16.647017, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6305/8562 [25:35<08:45,  4.29it/s, loss=16.646529, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6306/8562 [25:35<08:43,  4.31it/s, loss=16.646529, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6306/8562 [25:35<08:43,  4.31it/s, loss=16.646173, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6307/8562 [25:35<08:44,  4.30it/s, loss=16.646173, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6307/8562 [25:35<08:44,  4.30it/s, loss=16.646528, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6308/8562 [25:35<08:50,  4.25it/s, loss=16.646528, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6308/8562 [25:36<08:50,  4.25it/s, loss=16.646528, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6309/8562 [25:36<08:54,  4.22it/s, loss=16.646528, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6309/8562 [25:36<08:54,  4.22it/s, loss=16.646681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6310/8562 [25:36<08:49,  4.25it/s, loss=16.646681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6310/8562 [25:36<08:49,  4.25it/s, loss=16.647008, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6311/8562 [25:36<08:51,  4.24it/s, loss=16.647008, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6311/8562 [25:36<08:51,  4.24it/s, loss=16.647241, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6312/8562 [25:36<08:50,  4.24it/s, loss=16.647241, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6312/8562 [25:37<08:50,  4.24it/s, loss=16.647940, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6313/8562 [25:37<08:51,  4.23it/s, loss=16.647940, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6313/8562 [25:37<08:51,  4.23it/s, loss=16.647462, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6314/8562 [25:37<08:47,  4.26it/s, loss=16.647462, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▎  | 6314/8562 [25:37<08:47,  4.26it/s, loss=16.647276, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6315/8562 [25:37<08:46,  4.26it/s, loss=16.647276, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6315/8562 [25:37<08:46,  4.26it/s, loss=16.646573, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6316/8562 [25:37<08:47,  4.26it/s, loss=16.646573, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6316/8562 [25:38<08:47,  4.26it/s, loss=16.646626, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6317/8562 [25:38<08:48,  4.25it/s, loss=16.646626, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6317/8562 [25:38<08:48,  4.25it/s, loss=16.646961, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6318/8562 [25:38<09:02,  4.13it/s, loss=16.646961, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6318/8562 [25:38<09:02,  4.13it/s, loss=16.646941, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6319/8562 [25:38<08:56,  4.18it/s, loss=16.646941, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6319/8562 [25:38<08:56,  4.18it/s, loss=16.645625, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6320/8562 [25:38<09:19,  4.01it/s, loss=16.645625, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6320/8562 [25:39<09:19,  4.01it/s, loss=16.646016, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6321/8562 [25:39<10:45,  3.47it/s, loss=16.646016, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6321/8562 [25:39<10:45,  3.47it/s, loss=16.645052, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6322/8562 [25:39<10:07,  3.69it/s, loss=16.645052, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6322/8562 [25:39<10:07,  3.69it/s, loss=16.644731, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6323/8562 [25:39<09:41,  3.85it/s, loss=16.644731, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6323/8562 [25:39<09:41,  3.85it/s, loss=16.645345, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6324/8562 [25:39<09:24,  3.97it/s, loss=16.645345, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6324/8562 [25:40<09:24,  3.97it/s, loss=16.644923, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6325/8562 [25:40<09:11,  4.06it/s, loss=16.644923, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6325/8562 [25:40<09:11,  4.06it/s, loss=16.645257, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6326/8562 [25:40<09:06,  4.09it/s, loss=16.645257, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6326/8562 [25:40<09:06,  4.09it/s, loss=16.645707, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6327/8562 [25:40<08:58,  4.15it/s, loss=16.645707, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6327/8562 [25:40<08:58,  4.15it/s, loss=16.646349, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6328/8562 [25:40<08:52,  4.20it/s, loss=16.646349, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6328/8562 [25:41<08:52,  4.20it/s, loss=16.646592, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6329/8562 [25:41<08:46,  4.24it/s, loss=16.646592, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6329/8562 [25:41<08:46,  4.24it/s, loss=16.647088, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6330/8562 [25:41<08:41,  4.28it/s, loss=16.647088, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6330/8562 [25:41<08:41,  4.28it/s, loss=16.646807, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6331/8562 [25:41<08:39,  4.30it/s, loss=16.646807, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6331/8562 [25:41<08:39,  4.30it/s, loss=16.645978, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6332/8562 [25:41<08:38,  4.30it/s, loss=16.645978, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6332/8562 [25:42<08:38,  4.30it/s, loss=16.645972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6333/8562 [25:42<08:37,  4.31it/s, loss=16.645972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6333/8562 [25:42<08:37,  4.31it/s, loss=16.646195, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6334/8562 [25:42<08:36,  4.31it/s, loss=16.646195, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6334/8562 [25:42<08:36,  4.31it/s, loss=16.645325, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6335/8562 [25:42<08:37,  4.30it/s, loss=16.645325, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6335/8562 [25:42<08:37,  4.30it/s, loss=16.644947, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6336/8562 [25:42<08:36,  4.31it/s, loss=16.644947, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6336/8562 [25:42<08:36,  4.31it/s, loss=16.645237, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6337/8562 [25:42<08:48,  4.21it/s, loss=16.645237, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6337/8562 [25:43<08:48,  4.21it/s, loss=16.645357, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6338/8562 [25:43<08:51,  4.18it/s, loss=16.645357, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6338/8562 [25:43<08:51,  4.18it/s, loss=16.644606, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6339/8562 [25:43<08:46,  4.22it/s, loss=16.644606, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6339/8562 [25:43<08:46,  4.22it/s, loss=16.644348, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6340/8562 [25:43<08:43,  4.24it/s, loss=16.644348, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6340/8562 [25:43<08:43,  4.24it/s, loss=16.644532, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6341/8562 [25:43<08:43,  4.24it/s, loss=16.644532, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6341/8562 [25:44<08:43,  4.24it/s, loss=16.644615, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6342/8562 [25:44<08:48,  4.20it/s, loss=16.644615, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6342/8562 [25:44<08:48,  4.20it/s, loss=16.645051, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6343/8562 [25:44<08:46,  4.22it/s, loss=16.645051, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6343/8562 [25:44<08:46,  4.22it/s, loss=16.645559, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6344/8562 [25:44<08:41,  4.25it/s, loss=16.645559, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6344/8562 [25:44<08:41,  4.25it/s, loss=16.645448, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6345/8562 [25:44<08:39,  4.27it/s, loss=16.645448, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6345/8562 [25:45<08:39,  4.27it/s, loss=16.645715, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6346/8562 [25:45<08:42,  4.24it/s, loss=16.645715, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6346/8562 [25:45<08:42,  4.24it/s, loss=16.645349, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6347/8562 [25:45<08:40,  4.26it/s, loss=16.645349, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6347/8562 [25:45<08:40,  4.26it/s, loss=16.645021, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6348/8562 [25:45<08:41,  4.25it/s, loss=16.645021, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6348/8562 [25:45<08:41,  4.25it/s, loss=16.645117, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6349/8562 [25:45<08:42,  4.24it/s, loss=16.645117, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6349/8562 [25:46<08:42,  4.24it/s, loss=16.645827, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6350/8562 [25:46<09:21,  3.94it/s, loss=16.645827, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6350/8562 [25:46<09:21,  3.94it/s, loss=16.646140, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6351/8562 [25:46<09:07,  4.04it/s, loss=16.646140, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6351/8562 [25:46<09:07,  4.04it/s, loss=16.646305, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6352/8562 [25:46<08:57,  4.12it/s, loss=16.646305, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6352/8562 [25:46<08:57,  4.12it/s, loss=16.646279, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6353/8562 [25:46<08:47,  4.19it/s, loss=16.646279, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6353/8562 [25:47<08:47,  4.19it/s, loss=16.646836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6354/8562 [25:47<08:48,  4.18it/s, loss=16.646836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6354/8562 [25:47<08:48,  4.18it/s, loss=16.646384, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6355/8562 [25:47<08:48,  4.18it/s, loss=16.646384, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6355/8562 [25:47<08:48,  4.18it/s, loss=16.646821, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6356/8562 [25:47<08:48,  4.18it/s, loss=16.646821, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6356/8562 [25:47<08:48,  4.18it/s, loss=16.647140, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6357/8562 [25:47<08:51,  4.15it/s, loss=16.647140, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6357/8562 [25:47<08:51,  4.15it/s, loss=16.647285, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6358/8562 [25:47<08:46,  4.19it/s, loss=16.647285, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6358/8562 [25:48<08:46,  4.19it/s, loss=16.647139, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6359/8562 [25:48<08:43,  4.21it/s, loss=16.647139, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6359/8562 [25:48<08:43,  4.21it/s, loss=16.647531, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6360/8562 [25:48<08:38,  4.25it/s, loss=16.647531, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6360/8562 [25:48<08:38,  4.25it/s, loss=16.646720, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6361/8562 [25:48<08:37,  4.25it/s, loss=16.646720, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6361/8562 [25:48<08:37,  4.25it/s, loss=16.646557, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6362/8562 [25:48<08:35,  4.26it/s, loss=16.646557, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6362/8562 [25:49<08:35,  4.26it/s, loss=16.646824, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6363/8562 [25:49<08:35,  4.27it/s, loss=16.646824, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6363/8562 [25:49<08:35,  4.27it/s, loss=16.647039, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6364/8562 [25:49<08:32,  4.29it/s, loss=16.647039, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6364/8562 [25:49<08:32,  4.29it/s, loss=16.646299, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6365/8562 [25:49<08:31,  4.30it/s, loss=16.646299, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6365/8562 [25:49<08:31,  4.30it/s, loss=16.646291, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6366/8562 [25:49<08:31,  4.29it/s, loss=16.646291, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6366/8562 [25:50<08:31,  4.29it/s, loss=16.646162, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6367/8562 [25:50<08:32,  4.28it/s, loss=16.646162, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6367/8562 [25:50<08:32,  4.28it/s, loss=16.646347, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6368/8562 [25:50<08:35,  4.25it/s, loss=16.646347, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6368/8562 [25:50<08:35,  4.25it/s, loss=16.646866, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6369/8562 [25:50<08:37,  4.24it/s, loss=16.646866, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6369/8562 [25:50<08:37,  4.24it/s, loss=16.647238, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6370/8562 [25:50<08:34,  4.26it/s, loss=16.647238, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6370/8562 [25:51<08:34,  4.26it/s, loss=16.646729, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6371/8562 [25:51<08:34,  4.26it/s, loss=16.646729, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6371/8562 [25:51<08:34,  4.26it/s, loss=16.646643, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6372/8562 [25:51<08:32,  4.27it/s, loss=16.646643, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6372/8562 [25:51<08:32,  4.27it/s, loss=16.647113, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6373/8562 [25:51<08:34,  4.25it/s, loss=16.647113, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6373/8562 [25:51<08:34,  4.25it/s, loss=16.647247, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6374/8562 [25:51<08:33,  4.26it/s, loss=16.647247, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6374/8562 [25:51<08:33,  4.26it/s, loss=16.647540, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6375/8562 [25:51<08:32,  4.26it/s, loss=16.647540, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6375/8562 [25:52<08:32,  4.26it/s, loss=16.646805, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6376/8562 [25:52<08:32,  4.27it/s, loss=16.646805, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6376/8562 [25:52<08:32,  4.27it/s, loss=16.646336, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6377/8562 [25:52<08:33,  4.26it/s, loss=16.646336, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6377/8562 [25:52<08:33,  4.26it/s, loss=16.646831, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6378/8562 [25:52<08:32,  4.26it/s, loss=16.646831, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  74%|███████▍  | 6378/8562 [25:52<08:32,  4.26it/s, loss=16.647405, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6379/8562 [25:52<08:32,  4.26it/s, loss=16.647405, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6379/8562 [25:53<08:32,  4.26it/s, loss=16.647182, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6380/8562 [25:53<08:35,  4.23it/s, loss=16.647182, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6380/8562 [25:53<08:35,  4.23it/s, loss=16.647118, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6381/8562 [25:53<08:34,  4.24it/s, loss=16.647118, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6381/8562 [25:53<08:34,  4.24it/s, loss=16.647707, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6382/8562 [25:53<08:36,  4.22it/s, loss=16.647707, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6382/8562 [25:53<08:36,  4.22it/s, loss=16.647860, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6383/8562 [25:53<08:34,  4.23it/s, loss=16.647860, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6383/8562 [25:54<08:34,  4.23it/s, loss=16.647819, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6384/8562 [25:54<08:33,  4.24it/s, loss=16.647819, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6384/8562 [25:54<08:33,  4.24it/s, loss=16.647688, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6385/8562 [25:54<08:31,  4.25it/s, loss=16.647688, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6385/8562 [25:54<08:31,  4.25it/s, loss=16.647876, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6386/8562 [25:54<08:31,  4.25it/s, loss=16.647876, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6386/8562 [25:54<08:31,  4.25it/s, loss=16.647453, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6387/8562 [25:54<08:35,  4.22it/s, loss=16.647453, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6387/8562 [25:55<08:35,  4.22it/s, loss=16.647616, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6388/8562 [25:55<08:31,  4.25it/s, loss=16.647616, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6388/8562 [25:55<08:31,  4.25it/s, loss=16.647471, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6389/8562 [25:55<08:28,  4.27it/s, loss=16.647471, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6389/8562 [25:55<08:28,  4.27it/s, loss=16.646094, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6390/8562 [25:55<08:26,  4.29it/s, loss=16.646094, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6390/8562 [25:55<08:26,  4.29it/s, loss=16.645854, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6391/8562 [25:55<08:30,  4.25it/s, loss=16.645854, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6391/8562 [25:55<08:30,  4.25it/s, loss=16.645457, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6392/8562 [25:55<08:30,  4.25it/s, loss=16.645457, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6392/8562 [25:56<08:30,  4.25it/s, loss=16.645705, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6393/8562 [25:56<08:28,  4.27it/s, loss=16.645705, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6393/8562 [25:56<08:28,  4.27it/s, loss=16.646127, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6394/8562 [25:56<08:31,  4.24it/s, loss=16.646127, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6394/8562 [25:56<08:31,  4.24it/s, loss=16.645654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6395/8562 [25:56<08:36,  4.19it/s, loss=16.645654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6395/8562 [25:56<08:36,  4.19it/s, loss=16.645331, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6396/8562 [25:56<08:37,  4.19it/s, loss=16.645331, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6396/8562 [25:57<08:37,  4.19it/s, loss=16.645992, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6397/8562 [25:57<08:32,  4.22it/s, loss=16.645992, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6397/8562 [25:57<08:32,  4.22it/s, loss=16.646506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6398/8562 [25:57<08:34,  4.20it/s, loss=16.646506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6398/8562 [25:57<08:34,  4.20it/s, loss=16.646471, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6399/8562 [25:57<08:30,  4.24it/s, loss=16.646471, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6399/8562 [25:57<08:30,  4.24it/s, loss=16.646429, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6400/8562 [25:57<08:27,  4.26it/s, loss=16.646429, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6400/8562 [25:58<08:27,  4.26it/s, loss=16.646383, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6401/8562 [25:58<08:25,  4.27it/s, loss=16.646383, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6401/8562 [25:58<08:25,  4.27it/s, loss=16.645936, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6402/8562 [25:58<08:22,  4.30it/s, loss=16.645936, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6402/8562 [25:58<08:22,  4.30it/s, loss=16.645460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6403/8562 [25:58<08:42,  4.13it/s, loss=16.645460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6403/8562 [25:58<08:42,  4.13it/s, loss=16.645314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6404/8562 [25:58<09:38,  3.73it/s, loss=16.645314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6404/8562 [25:59<09:38,  3.73it/s, loss=16.644519, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6405/8562 [25:59<10:09,  3.54it/s, loss=16.644519, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6405/8562 [25:59<10:09,  3.54it/s, loss=16.645209, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6406/8562 [25:59<09:56,  3.62it/s, loss=16.645209, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6406/8562 [25:59<09:56,  3.62it/s, loss=16.644902, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6407/8562 [25:59<10:24,  3.45it/s, loss=16.644902, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6407/8562 [26:00<10:24,  3.45it/s, loss=16.645006, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6408/8562 [26:00<10:47,  3.33it/s, loss=16.645006, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6408/8562 [26:00<10:47,  3.33it/s, loss=16.645238, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6409/8562 [26:00<10:49,  3.31it/s, loss=16.645238, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6409/8562 [26:00<10:49,  3.31it/s, loss=16.645217, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6410/8562 [26:00<10:36,  3.38it/s, loss=16.645217, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6410/8562 [26:00<10:36,  3.38it/s, loss=16.644812, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6411/8562 [26:01<10:35,  3.39it/s, loss=16.644812, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6411/8562 [26:01<10:35,  3.39it/s, loss=16.644665, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6412/8562 [26:01<10:28,  3.42it/s, loss=16.644665, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6412/8562 [26:01<10:28,  3.42it/s, loss=16.644790, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6413/8562 [26:01<10:03,  3.56it/s, loss=16.644790, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6413/8562 [26:01<10:03,  3.56it/s, loss=16.644672, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6414/8562 [26:01<10:21,  3.45it/s, loss=16.644672, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6414/8562 [26:02<10:21,  3.45it/s, loss=16.644641, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6415/8562 [26:02<10:42,  3.34it/s, loss=16.644641, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6415/8562 [26:02<10:42,  3.34it/s, loss=16.644864, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6416/8562 [26:02<10:21,  3.45it/s, loss=16.644864, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6416/8562 [26:02<10:21,  3.45it/s, loss=16.644659, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6417/8562 [26:02<10:50,  3.30it/s, loss=16.644659, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6417/8562 [26:03<10:50,  3.30it/s, loss=16.644201, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6418/8562 [26:03<10:50,  3.30it/s, loss=16.644201, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6418/8562 [26:03<10:50,  3.30it/s, loss=16.644500, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6419/8562 [26:03<11:42,  3.05it/s, loss=16.644500, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6419/8562 [26:03<11:42,  3.05it/s, loss=16.644468, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6420/8562 [26:03<11:24,  3.13it/s, loss=16.644468, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6420/8562 [26:04<11:24,  3.13it/s, loss=16.644534, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6421/8562 [26:04<11:28,  3.11it/s, loss=16.644534, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▍  | 6421/8562 [26:04<11:28,  3.11it/s, loss=16.644371, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6422/8562 [26:04<11:12,  3.18it/s, loss=16.644371, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6422/8562 [26:04<11:12,  3.18it/s, loss=16.644293, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6423/8562 [26:04<11:36,  3.07it/s, loss=16.644293, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6423/8562 [26:04<11:36,  3.07it/s, loss=16.644630, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6424/8562 [26:04<10:50,  3.29it/s, loss=16.644630, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6424/8562 [26:05<10:50,  3.29it/s, loss=16.644681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6425/8562 [26:05<11:01,  3.23it/s, loss=16.644681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6425/8562 [26:05<11:01,  3.23it/s, loss=16.644341, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6426/8562 [26:05<10:41,  3.33it/s, loss=16.644341, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6426/8562 [26:05<10:41,  3.33it/s, loss=16.644889, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6427/8562 [26:05<10:18,  3.45it/s, loss=16.644889, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6427/8562 [26:06<10:18,  3.45it/s, loss=16.644411, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6428/8562 [26:06<11:05,  3.21it/s, loss=16.644411, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6428/8562 [26:06<11:05,  3.21it/s, loss=16.644367, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6429/8562 [26:06<10:38,  3.34it/s, loss=16.644367, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6429/8562 [26:06<10:38,  3.34it/s, loss=16.643775, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6430/8562 [26:06<09:57,  3.57it/s, loss=16.643775, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6430/8562 [26:06<09:57,  3.57it/s, loss=16.643165, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6431/8562 [26:06<09:25,  3.77it/s, loss=16.643165, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6431/8562 [26:07<09:25,  3.77it/s, loss=16.643571, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6432/8562 [26:07<09:06,  3.90it/s, loss=16.643571, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6432/8562 [26:07<09:06,  3.90it/s, loss=16.643898, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6433/8562 [26:07<08:53,  3.99it/s, loss=16.643898, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6433/8562 [26:07<08:53,  3.99it/s, loss=16.643818, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6434/8562 [26:07<08:44,  4.06it/s, loss=16.643818, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6434/8562 [26:07<08:44,  4.06it/s, loss=16.643492, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6435/8562 [26:07<08:36,  4.12it/s, loss=16.643492, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6435/8562 [26:08<08:36,  4.12it/s, loss=16.643418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6436/8562 [26:08<08:33,  4.14it/s, loss=16.643418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6436/8562 [26:08<08:33,  4.14it/s, loss=16.643656, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6437/8562 [26:08<08:26,  4.19it/s, loss=16.643656, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6437/8562 [26:08<08:26,  4.19it/s, loss=16.643109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6438/8562 [26:08<08:22,  4.22it/s, loss=16.643109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6438/8562 [26:08<08:22,  4.22it/s, loss=16.642955, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6439/8562 [26:08<08:21,  4.24it/s, loss=16.642955, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6439/8562 [26:09<08:21,  4.24it/s, loss=16.643099, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6440/8562 [26:09<08:19,  4.25it/s, loss=16.643099, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6440/8562 [26:09<08:19,  4.25it/s, loss=16.643499, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6441/8562 [26:09<08:15,  4.28it/s, loss=16.643499, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6441/8562 [26:09<08:15,  4.28it/s, loss=16.643187, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6442/8562 [26:09<08:18,  4.25it/s, loss=16.643187, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6442/8562 [26:09<08:18,  4.25it/s, loss=16.642825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6443/8562 [26:09<08:23,  4.21it/s, loss=16.642825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6443/8562 [26:10<08:23,  4.21it/s, loss=16.642877, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6444/8562 [26:10<08:18,  4.25it/s, loss=16.642877, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6444/8562 [26:10<08:18,  4.25it/s, loss=16.642540, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6445/8562 [26:10<08:14,  4.28it/s, loss=16.642540, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6445/8562 [26:10<08:14,  4.28it/s, loss=16.642225, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6446/8562 [26:10<08:16,  4.26it/s, loss=16.642225, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6446/8562 [26:10<08:16,  4.26it/s, loss=16.641689, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6447/8562 [26:10<08:16,  4.26it/s, loss=16.641689, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6447/8562 [26:10<08:16,  4.26it/s, loss=16.642066, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6448/8562 [26:10<08:14,  4.27it/s, loss=16.642066, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6448/8562 [26:11<08:14,  4.27it/s, loss=16.642107, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6449/8562 [26:11<08:32,  4.12it/s, loss=16.642107, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6449/8562 [26:11<08:32,  4.12it/s, loss=16.642226, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6450/8562 [26:11<08:26,  4.17it/s, loss=16.642226, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6450/8562 [26:11<08:26,  4.17it/s, loss=16.641275, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6451/8562 [26:11<08:25,  4.18it/s, loss=16.641275, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6451/8562 [26:11<08:25,  4.18it/s, loss=16.641635, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6452/8562 [26:11<08:20,  4.21it/s, loss=16.641635, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6452/8562 [26:12<08:20,  4.21it/s, loss=16.641627, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6453/8562 [26:12<08:32,  4.11it/s, loss=16.641627, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6453/8562 [26:12<08:32,  4.11it/s, loss=16.642001, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6454/8562 [26:12<08:29,  4.14it/s, loss=16.642001, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6454/8562 [26:12<08:29,  4.14it/s, loss=16.641543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6455/8562 [26:12<08:23,  4.19it/s, loss=16.641543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6455/8562 [26:12<08:23,  4.19it/s, loss=16.641318, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6456/8562 [26:12<08:19,  4.21it/s, loss=16.641318, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6456/8562 [26:13<08:19,  4.21it/s, loss=16.640251, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6457/8562 [26:13<08:15,  4.25it/s, loss=16.640251, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6457/8562 [26:13<08:15,  4.25it/s, loss=16.640140, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6458/8562 [26:13<08:15,  4.24it/s, loss=16.640140, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6458/8562 [26:13<08:15,  4.24it/s, loss=16.640899, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6459/8562 [26:13<08:12,  4.27it/s, loss=16.640899, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6459/8562 [26:13<08:12,  4.27it/s, loss=16.640104, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6460/8562 [26:13<08:11,  4.27it/s, loss=16.640104, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6460/8562 [26:14<08:11,  4.27it/s, loss=16.639492, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6461/8562 [26:14<08:16,  4.23it/s, loss=16.639492, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6461/8562 [26:14<08:16,  4.23it/s, loss=16.639848, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6462/8562 [26:14<08:12,  4.26it/s, loss=16.639848, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6462/8562 [26:14<08:12,  4.26it/s, loss=16.639912, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6463/8562 [26:14<08:11,  4.27it/s, loss=16.639912, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6463/8562 [26:14<08:11,  4.27it/s, loss=16.639861, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6464/8562 [26:14<08:18,  4.21it/s, loss=16.639861, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  75%|███████▌  | 6464/8562 [26:14<08:18,  4.21it/s, loss=16.640282, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6465/8562 [26:14<08:16,  4.22it/s, loss=16.640282, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6465/8562 [26:15<08:16,  4.22it/s, loss=16.640875, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6466/8562 [26:15<08:13,  4.25it/s, loss=16.640875, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6466/8562 [26:15<08:13,  4.25it/s, loss=16.639706, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6467/8562 [26:15<08:09,  4.28it/s, loss=16.639706, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6467/8562 [26:15<08:09,  4.28it/s, loss=16.639577, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6468/8562 [26:15<08:06,  4.30it/s, loss=16.639577, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6468/8562 [26:15<08:06,  4.30it/s, loss=16.639632, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6469/8562 [26:15<08:09,  4.27it/s, loss=16.639632, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6469/8562 [26:16<08:09,  4.27it/s, loss=16.639726, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6470/8562 [26:16<08:11,  4.26it/s, loss=16.639726, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6470/8562 [26:16<08:11,  4.26it/s, loss=16.639611, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6471/8562 [26:16<08:08,  4.28it/s, loss=16.639611, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6471/8562 [26:16<08:08,  4.28it/s, loss=16.638854, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6472/8562 [26:16<08:12,  4.24it/s, loss=16.638854, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6472/8562 [26:16<08:12,  4.24it/s, loss=16.638665, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6473/8562 [26:16<08:11,  4.25it/s, loss=16.638665, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6473/8562 [26:17<08:11,  4.25it/s, loss=16.637970, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6474/8562 [26:17<08:17,  4.20it/s, loss=16.637970, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6474/8562 [26:17<08:17,  4.20it/s, loss=16.637831, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6475/8562 [26:17<08:20,  4.17it/s, loss=16.637831, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6475/8562 [26:17<08:20,  4.17it/s, loss=16.637957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6476/8562 [26:17<08:13,  4.22it/s, loss=16.637957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6476/8562 [26:17<08:13,  4.22it/s, loss=16.638359, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6477/8562 [26:17<08:12,  4.23it/s, loss=16.638359, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6477/8562 [26:18<08:12,  4.23it/s, loss=16.638964, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6478/8562 [26:18<08:22,  4.15it/s, loss=16.638964, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6478/8562 [26:18<08:22,  4.15it/s, loss=16.638663, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6479/8562 [26:18<08:15,  4.21it/s, loss=16.638663, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6479/8562 [26:18<08:15,  4.21it/s, loss=16.639174, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6480/8562 [26:18<08:11,  4.24it/s, loss=16.639174, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6480/8562 [26:18<08:11,  4.24it/s, loss=16.638882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6481/8562 [26:18<08:11,  4.23it/s, loss=16.638882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6481/8562 [26:19<08:11,  4.23it/s, loss=16.638354, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6482/8562 [26:19<08:10,  4.24it/s, loss=16.638354, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6482/8562 [26:19<08:10,  4.24it/s, loss=16.638854, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6483/8562 [26:19<08:09,  4.25it/s, loss=16.638854, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6483/8562 [26:19<08:09,  4.25it/s, loss=16.638583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6484/8562 [26:19<08:06,  4.27it/s, loss=16.638583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6484/8562 [26:19<08:06,  4.27it/s, loss=16.638504, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6485/8562 [26:19<08:05,  4.28it/s, loss=16.638504, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6485/8562 [26:19<08:05,  4.28it/s, loss=16.638991, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6486/8562 [26:19<08:04,  4.28it/s, loss=16.638991, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6486/8562 [26:20<08:04,  4.28it/s, loss=16.639176, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6487/8562 [26:20<08:02,  4.30it/s, loss=16.639176, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6487/8562 [26:20<08:02,  4.30it/s, loss=16.639738, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6488/8562 [26:20<07:59,  4.32it/s, loss=16.639738, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6488/8562 [26:20<07:59,  4.32it/s, loss=16.639549, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6489/8562 [26:20<08:06,  4.26it/s, loss=16.639549, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6489/8562 [26:20<08:06,  4.26it/s, loss=16.639703, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6490/8562 [26:20<08:04,  4.28it/s, loss=16.639703, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6490/8562 [26:21<08:04,  4.28it/s, loss=16.640232, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6491/8562 [26:21<08:07,  4.24it/s, loss=16.640232, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6491/8562 [26:21<08:07,  4.24it/s, loss=16.640660, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6492/8562 [26:21<08:06,  4.25it/s, loss=16.640660, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6492/8562 [26:21<08:06,  4.25it/s, loss=16.640455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6493/8562 [26:21<08:03,  4.27it/s, loss=16.640455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6493/8562 [26:21<08:03,  4.27it/s, loss=16.640699, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6494/8562 [26:21<08:04,  4.27it/s, loss=16.640699, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6494/8562 [26:22<08:04,  4.27it/s, loss=16.639561, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6495/8562 [26:22<08:05,  4.26it/s, loss=16.639561, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6495/8562 [26:22<08:05,  4.26it/s, loss=16.639506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6496/8562 [26:22<08:06,  4.25it/s, loss=16.639506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6496/8562 [26:22<08:06,  4.25it/s, loss=16.639587, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6497/8562 [26:22<08:05,  4.26it/s, loss=16.639587, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6497/8562 [26:22<08:05,  4.26it/s, loss=16.639337, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6498/8562 [26:22<08:03,  4.26it/s, loss=16.639337, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6498/8562 [26:22<08:03,  4.26it/s, loss=16.639628, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6499/8562 [26:22<08:01,  4.29it/s, loss=16.639628, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6499/8562 [26:23<08:01,  4.29it/s, loss=16.640208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6500/8562 [26:23<08:00,  4.29it/s, loss=16.640208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6500/8562 [26:23<08:00,  4.29it/s, loss=16.639590, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6501/8562 [26:23<07:59,  4.30it/s, loss=16.639590, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6501/8562 [26:23<07:59,  4.30it/s, loss=16.639525, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6502/8562 [26:23<07:59,  4.29it/s, loss=16.639525, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6502/8562 [26:23<07:59,  4.29it/s, loss=16.639182, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6503/8562 [26:23<08:00,  4.28it/s, loss=16.639182, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6503/8562 [26:24<08:00,  4.28it/s, loss=16.639732, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6504/8562 [26:24<08:01,  4.28it/s, loss=16.639732, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6504/8562 [26:24<08:01,  4.28it/s, loss=16.639995, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6505/8562 [26:24<08:03,  4.26it/s, loss=16.639995, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6505/8562 [26:24<08:03,  4.26it/s, loss=16.640303, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6506/8562 [26:24<08:06,  4.23it/s, loss=16.640303, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6506/8562 [26:24<08:06,  4.23it/s, loss=16.640529, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6507/8562 [26:24<08:04,  4.24it/s, loss=16.640529, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6507/8562 [26:25<08:04,  4.24it/s, loss=16.641068, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6508/8562 [26:25<08:08,  4.20it/s, loss=16.641068, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6508/8562 [26:25<08:08,  4.20it/s, loss=16.641682, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6509/8562 [26:25<08:09,  4.19it/s, loss=16.641682, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6509/8562 [26:25<08:09,  4.19it/s, loss=16.641581, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6510/8562 [26:25<08:06,  4.22it/s, loss=16.641581, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6510/8562 [26:25<08:06,  4.22it/s, loss=16.641138, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6511/8562 [26:25<08:04,  4.23it/s, loss=16.641138, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6511/8562 [26:26<08:04,  4.23it/s, loss=16.641684, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6512/8562 [26:26<08:03,  4.24it/s, loss=16.641684, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6512/8562 [26:26<08:03,  4.24it/s, loss=16.641336, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6513/8562 [26:26<08:01,  4.25it/s, loss=16.641336, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6513/8562 [26:26<08:01,  4.25it/s, loss=16.640773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6514/8562 [26:26<08:03,  4.23it/s, loss=16.640773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6514/8562 [26:26<08:03,  4.23it/s, loss=16.640558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6515/8562 [26:26<08:00,  4.26it/s, loss=16.640558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6515/8562 [26:26<08:00,  4.26it/s, loss=16.641055, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6516/8562 [26:26<07:59,  4.27it/s, loss=16.641055, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6516/8562 [26:27<07:59,  4.27it/s, loss=16.641336, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6517/8562 [26:27<08:00,  4.25it/s, loss=16.641336, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6517/8562 [26:27<08:00,  4.25it/s, loss=16.641542, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6518/8562 [26:27<08:00,  4.26it/s, loss=16.641542, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6518/8562 [26:27<08:00,  4.26it/s, loss=16.641469, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6519/8562 [26:27<07:59,  4.26it/s, loss=16.641469, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6519/8562 [26:27<07:59,  4.26it/s, loss=16.641438, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6520/8562 [26:27<07:55,  4.29it/s, loss=16.641438, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6520/8562 [26:28<07:55,  4.29it/s, loss=16.641506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6521/8562 [26:28<07:52,  4.32it/s, loss=16.641506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6521/8562 [26:28<07:52,  4.32it/s, loss=16.641731, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6522/8562 [26:28<07:51,  4.33it/s, loss=16.641731, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6522/8562 [26:28<07:51,  4.33it/s, loss=16.641871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6523/8562 [26:28<07:52,  4.32it/s, loss=16.641871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6523/8562 [26:28<07:52,  4.32it/s, loss=16.642349, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6524/8562 [26:28<07:53,  4.31it/s, loss=16.642349, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6524/8562 [26:29<07:53,  4.31it/s, loss=16.642314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6525/8562 [26:29<07:51,  4.32it/s, loss=16.642314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6525/8562 [26:29<07:51,  4.32it/s, loss=16.642001, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6526/8562 [26:29<07:50,  4.33it/s, loss=16.642001, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6526/8562 [26:29<07:50,  4.33it/s, loss=16.642510, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6527/8562 [26:29<07:49,  4.33it/s, loss=16.642510, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6527/8562 [26:29<07:49,  4.33it/s, loss=16.643285, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6528/8562 [26:29<07:59,  4.24it/s, loss=16.643285, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▌  | 6528/8562 [26:30<07:59,  4.24it/s, loss=16.642273, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6529/8562 [26:30<07:56,  4.26it/s, loss=16.642273, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6529/8562 [26:30<07:56,  4.26it/s, loss=16.641769, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6530/8562 [26:30<07:54,  4.28it/s, loss=16.641769, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6530/8562 [26:30<07:54,  4.28it/s, loss=16.641981, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6531/8562 [26:30<07:54,  4.28it/s, loss=16.641981, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6531/8562 [26:30<07:54,  4.28it/s, loss=16.642161, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6532/8562 [26:30<07:54,  4.27it/s, loss=16.642161, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6532/8562 [26:30<07:54,  4.27it/s, loss=16.642443, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6533/8562 [26:30<07:56,  4.26it/s, loss=16.642443, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6533/8562 [26:31<07:56,  4.26it/s, loss=16.642981, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6534/8562 [26:31<07:55,  4.27it/s, loss=16.642981, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6534/8562 [26:31<07:55,  4.27it/s, loss=16.642442, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6535/8562 [26:31<07:54,  4.27it/s, loss=16.642442, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6535/8562 [26:31<07:54,  4.27it/s, loss=16.642762, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6536/8562 [26:31<07:56,  4.25it/s, loss=16.642762, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6536/8562 [26:31<07:56,  4.25it/s, loss=16.642894, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6537/8562 [26:31<07:53,  4.28it/s, loss=16.642894, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6537/8562 [26:32<07:53,  4.28it/s, loss=16.643261, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6538/8562 [26:32<08:00,  4.21it/s, loss=16.643261, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6538/8562 [26:32<08:00,  4.21it/s, loss=16.642761, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6539/8562 [26:32<08:01,  4.20it/s, loss=16.642761, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6539/8562 [26:32<08:01,  4.20it/s, loss=16.642156, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6540/8562 [26:32<07:58,  4.23it/s, loss=16.642156, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6540/8562 [26:32<07:58,  4.23it/s, loss=16.641741, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6541/8562 [26:32<07:57,  4.23it/s, loss=16.641741, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6541/8562 [26:33<07:57,  4.23it/s, loss=16.642356, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6542/8562 [26:33<07:56,  4.24it/s, loss=16.642356, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6542/8562 [26:33<07:56,  4.24it/s, loss=16.642571, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6543/8562 [26:33<07:56,  4.24it/s, loss=16.642571, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6543/8562 [26:33<07:56,  4.24it/s, loss=16.642238, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6544/8562 [26:33<07:55,  4.25it/s, loss=16.642238, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6544/8562 [26:33<07:55,  4.25it/s, loss=16.641952, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6545/8562 [26:33<08:00,  4.20it/s, loss=16.641952, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6545/8562 [26:34<08:00,  4.20it/s, loss=16.642367, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6546/8562 [26:34<07:55,  4.24it/s, loss=16.642367, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6546/8562 [26:34<07:55,  4.24it/s, loss=16.641404, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6547/8562 [26:34<07:52,  4.27it/s, loss=16.641404, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6547/8562 [26:34<07:52,  4.27it/s, loss=16.641427, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6548/8562 [26:34<08:02,  4.17it/s, loss=16.641427, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6548/8562 [26:34<08:02,  4.17it/s, loss=16.641581, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6549/8562 [26:34<07:59,  4.20it/s, loss=16.641581, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  76%|███████▋  | 6549/8562 [26:34<07:59,  4.20it/s, loss=16.641576, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6550/8562 [26:34<07:56,  4.23it/s, loss=16.641576, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6550/8562 [26:35<07:56,  4.23it/s, loss=16.641337, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6551/8562 [26:35<07:51,  4.26it/s, loss=16.641337, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6551/8562 [26:35<07:51,  4.26it/s, loss=16.641110, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6552/8562 [26:35<07:50,  4.27it/s, loss=16.641110, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6552/8562 [26:35<07:50,  4.27it/s, loss=16.640881, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6553/8562 [26:35<07:47,  4.30it/s, loss=16.640881, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6553/8562 [26:35<07:47,  4.30it/s, loss=16.641316, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6554/8562 [26:35<07:46,  4.30it/s, loss=16.641316, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6554/8562 [26:36<07:46,  4.30it/s, loss=16.641627, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6555/8562 [26:36<07:45,  4.31it/s, loss=16.641627, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6555/8562 [26:36<07:45,  4.31it/s, loss=16.641507, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6556/8562 [26:36<07:45,  4.31it/s, loss=16.641507, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6556/8562 [26:36<07:45,  4.31it/s, loss=16.640789, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6557/8562 [26:36<07:44,  4.32it/s, loss=16.640789, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6557/8562 [26:36<07:44,  4.32it/s, loss=16.641451, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6558/8562 [26:36<07:43,  4.32it/s, loss=16.641451, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6558/8562 [26:37<07:43,  4.32it/s, loss=16.640515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6559/8562 [26:37<07:51,  4.25it/s, loss=16.640515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6559/8562 [26:37<07:51,  4.25it/s, loss=16.640423, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6560/8562 [26:37<07:49,  4.27it/s, loss=16.640423, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6560/8562 [26:37<07:49,  4.27it/s, loss=16.640517, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6561/8562 [26:37<07:46,  4.29it/s, loss=16.640517, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6561/8562 [26:37<07:46,  4.29it/s, loss=16.640833, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6562/8562 [26:37<07:54,  4.22it/s, loss=16.640833, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6562/8562 [26:37<07:54,  4.22it/s, loss=16.640904, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6563/8562 [26:38<07:50,  4.25it/s, loss=16.640904, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6563/8562 [26:38<07:50,  4.25it/s, loss=16.641239, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6564/8562 [26:38<07:47,  4.27it/s, loss=16.641239, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6564/8562 [26:38<07:47,  4.27it/s, loss=16.641262, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6565/8562 [26:38<07:46,  4.28it/s, loss=16.641262, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6565/8562 [26:38<07:46,  4.28it/s, loss=16.641183, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6566/8562 [26:38<07:43,  4.31it/s, loss=16.641183, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6566/8562 [26:38<07:43,  4.31it/s, loss=16.641319, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6567/8562 [26:38<07:43,  4.30it/s, loss=16.641319, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6567/8562 [26:39<07:43,  4.30it/s, loss=16.641654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6568/8562 [26:39<07:42,  4.31it/s, loss=16.641654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6568/8562 [26:39<07:42,  4.31it/s, loss=16.641069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6569/8562 [26:39<07:41,  4.32it/s, loss=16.641069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6569/8562 [26:39<07:41,  4.32it/s, loss=16.641396, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6570/8562 [26:39<07:40,  4.33it/s, loss=16.641396, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6570/8562 [26:39<07:40,  4.33it/s, loss=16.641264, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6571/8562 [26:39<07:55,  4.19it/s, loss=16.641264, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6571/8562 [26:40<07:55,  4.19it/s, loss=16.641050, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6572/8562 [26:40<07:52,  4.21it/s, loss=16.641050, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6572/8562 [26:40<07:52,  4.21it/s, loss=16.641237, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6573/8562 [26:40<08:01,  4.13it/s, loss=16.641237, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6573/8562 [26:40<08:01,  4.13it/s, loss=16.640972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6574/8562 [26:40<07:55,  4.18it/s, loss=16.640972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6574/8562 [26:40<07:55,  4.18it/s, loss=16.641223, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6575/8562 [26:40<07:50,  4.23it/s, loss=16.641223, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6575/8562 [26:41<07:50,  4.23it/s, loss=16.641292, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6576/8562 [26:41<07:47,  4.25it/s, loss=16.641292, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6576/8562 [26:41<07:47,  4.25it/s, loss=16.641702, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6577/8562 [26:41<07:47,  4.24it/s, loss=16.641702, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6577/8562 [26:41<07:47,  4.24it/s, loss=16.642185, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6578/8562 [26:41<07:46,  4.25it/s, loss=16.642185, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6578/8562 [26:41<07:46,  4.25it/s, loss=16.642402, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6579/8562 [26:41<07:45,  4.26it/s, loss=16.642402, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6579/8562 [26:42<07:45,  4.26it/s, loss=16.642935, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6580/8562 [26:42<07:55,  4.17it/s, loss=16.642935, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6580/8562 [26:42<07:55,  4.17it/s, loss=16.642591, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6581/8562 [26:42<07:53,  4.18it/s, loss=16.642591, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6581/8562 [26:42<07:53,  4.18it/s, loss=16.643058, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6582/8562 [26:42<07:53,  4.18it/s, loss=16.643058, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6582/8562 [26:42<07:53,  4.18it/s, loss=16.642192, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6583/8562 [26:42<07:48,  4.22it/s, loss=16.642192, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6583/8562 [26:42<07:48,  4.22it/s, loss=16.642716, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6584/8562 [26:42<07:48,  4.22it/s, loss=16.642716, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6584/8562 [26:43<07:48,  4.22it/s, loss=16.642670, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6585/8562 [26:43<07:59,  4.12it/s, loss=16.642670, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6585/8562 [26:43<07:59,  4.12it/s, loss=16.642647, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6586/8562 [26:43<08:00,  4.12it/s, loss=16.642647, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6586/8562 [26:43<08:00,  4.12it/s, loss=16.642811, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6587/8562 [26:43<07:53,  4.17it/s, loss=16.642811, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6587/8562 [26:44<07:53,  4.17it/s, loss=16.643693, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6588/8562 [26:44<08:44,  3.76it/s, loss=16.643693, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6588/8562 [26:44<08:44,  3.76it/s, loss=16.643627, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6589/8562 [26:44<08:26,  3.90it/s, loss=16.643627, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6589/8562 [26:44<08:26,  3.90it/s, loss=16.644364, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6590/8562 [26:44<08:20,  3.94it/s, loss=16.644364, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6590/8562 [26:44<08:20,  3.94it/s, loss=16.644809, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6591/8562 [26:44<08:06,  4.05it/s, loss=16.644809, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6591/8562 [26:44<08:06,  4.05it/s, loss=16.644685, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6592/8562 [26:44<08:01,  4.09it/s, loss=16.644685, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6592/8562 [26:45<08:01,  4.09it/s, loss=16.644620, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6593/8562 [26:45<08:08,  4.03it/s, loss=16.644620, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6593/8562 [26:45<08:08,  4.03it/s, loss=16.645140, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6594/8562 [26:45<08:03,  4.07it/s, loss=16.645140, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6594/8562 [26:45<08:03,  4.07it/s, loss=16.644820, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6595/8562 [26:45<07:54,  4.14it/s, loss=16.644820, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6595/8562 [26:45<07:54,  4.14it/s, loss=16.644944, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6596/8562 [26:45<07:50,  4.18it/s, loss=16.644944, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6596/8562 [26:46<07:50,  4.18it/s, loss=16.645081, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6597/8562 [26:46<07:47,  4.20it/s, loss=16.645081, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6597/8562 [26:46<07:47,  4.20it/s, loss=16.644456, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6598/8562 [26:46<07:43,  4.23it/s, loss=16.644456, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6598/8562 [26:46<07:43,  4.23it/s, loss=16.644470, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6599/8562 [26:46<07:39,  4.27it/s, loss=16.644470, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6599/8562 [26:46<07:39,  4.27it/s, loss=16.644619, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6600/8562 [26:46<07:36,  4.29it/s, loss=16.644619, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6600/8562 [26:47<07:36,  4.29it/s, loss=16.644315, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6601/8562 [26:47<07:38,  4.28it/s, loss=16.644315, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6601/8562 [26:47<07:38,  4.28it/s, loss=16.644590, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6602/8562 [26:47<07:37,  4.29it/s, loss=16.644590, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6602/8562 [26:47<07:37,  4.29it/s, loss=16.643729, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6603/8562 [26:47<07:37,  4.28it/s, loss=16.643729, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6603/8562 [26:47<07:37,  4.28it/s, loss=16.643338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6604/8562 [26:47<07:37,  4.28it/s, loss=16.643338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6604/8562 [26:48<07:37,  4.28it/s, loss=16.643938, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6605/8562 [26:48<07:41,  4.24it/s, loss=16.643938, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6605/8562 [26:48<07:41,  4.24it/s, loss=16.643899, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6606/8562 [26:48<07:50,  4.16it/s, loss=16.643899, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6606/8562 [26:48<07:50,  4.16it/s, loss=16.644558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6607/8562 [26:48<07:57,  4.09it/s, loss=16.644558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6607/8562 [26:48<07:57,  4.09it/s, loss=16.643621, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6608/8562 [26:48<07:51,  4.15it/s, loss=16.643621, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6608/8562 [26:49<07:51,  4.15it/s, loss=16.643927, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6609/8562 [26:49<07:45,  4.19it/s, loss=16.643927, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6609/8562 [26:49<07:45,  4.19it/s, loss=16.643359, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6610/8562 [26:49<07:44,  4.21it/s, loss=16.643359, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6610/8562 [26:49<07:44,  4.21it/s, loss=16.643714, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6611/8562 [26:49<07:41,  4.22it/s, loss=16.643714, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6611/8562 [26:49<07:41,  4.22it/s, loss=16.644118, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6612/8562 [26:49<07:45,  4.19it/s, loss=16.644118, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6612/8562 [26:49<07:45,  4.19it/s, loss=16.643871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6613/8562 [26:49<07:41,  4.22it/s, loss=16.643871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6613/8562 [26:50<07:41,  4.22it/s, loss=16.643786, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6614/8562 [26:50<07:39,  4.24it/s, loss=16.643786, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6614/8562 [26:50<07:39,  4.24it/s, loss=16.643379, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6615/8562 [26:50<07:45,  4.18it/s, loss=16.643379, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6615/8562 [26:50<07:45,  4.18it/s, loss=16.642889, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6616/8562 [26:50<07:41,  4.21it/s, loss=16.642889, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6616/8562 [26:50<07:41,  4.21it/s, loss=16.642948, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6617/8562 [26:50<07:50,  4.13it/s, loss=16.642948, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6617/8562 [26:51<07:50,  4.13it/s, loss=16.643363, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6618/8562 [26:51<07:46,  4.17it/s, loss=16.643363, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6618/8562 [26:51<07:46,  4.17it/s, loss=16.643168, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6619/8562 [26:51<07:45,  4.18it/s, loss=16.643168, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6619/8562 [26:51<07:45,  4.18it/s, loss=16.642807, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6620/8562 [26:51<07:43,  4.19it/s, loss=16.642807, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6620/8562 [26:51<07:43,  4.19it/s, loss=16.642482, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6621/8562 [26:51<07:41,  4.21it/s, loss=16.642482, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6621/8562 [26:52<07:41,  4.21it/s, loss=16.642215, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6622/8562 [26:52<07:38,  4.23it/s, loss=16.642215, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6622/8562 [26:52<07:38,  4.23it/s, loss=16.642226, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6623/8562 [26:52<07:36,  4.25it/s, loss=16.642226, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6623/8562 [26:52<07:36,  4.25it/s, loss=16.642893, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6624/8562 [26:52<07:34,  4.27it/s, loss=16.642893, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6624/8562 [26:52<07:34,  4.27it/s, loss=16.643389, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6625/8562 [26:52<07:33,  4.27it/s, loss=16.643389, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6625/8562 [26:53<07:33,  4.27it/s, loss=16.643240, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6626/8562 [26:53<07:35,  4.25it/s, loss=16.643240, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6626/8562 [26:53<07:35,  4.25it/s, loss=16.642459, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6627/8562 [26:53<07:33,  4.27it/s, loss=16.642459, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6627/8562 [26:53<07:33,  4.27it/s, loss=16.642053, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6628/8562 [26:53<07:34,  4.25it/s, loss=16.642053, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6628/8562 [26:53<07:34,  4.25it/s, loss=16.641680, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6629/8562 [26:53<07:32,  4.27it/s, loss=16.641680, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6629/8562 [26:53<07:32,  4.27it/s, loss=16.641592, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6630/8562 [26:53<07:32,  4.27it/s, loss=16.641592, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6630/8562 [26:54<07:32,  4.27it/s, loss=16.641072, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6631/8562 [26:54<07:33,  4.26it/s, loss=16.641072, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6631/8562 [26:54<07:33,  4.26it/s, loss=16.641478, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6632/8562 [26:54<07:37,  4.21it/s, loss=16.641478, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6632/8562 [26:54<07:37,  4.21it/s, loss=16.641212, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6633/8562 [26:54<07:34,  4.25it/s, loss=16.641212, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6633/8562 [26:54<07:34,  4.25it/s, loss=16.640973, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6634/8562 [26:54<07:39,  4.20it/s, loss=16.640973, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6634/8562 [26:55<07:39,  4.20it/s, loss=16.641188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6635/8562 [26:55<07:35,  4.23it/s, loss=16.641188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  77%|███████▋  | 6635/8562 [26:55<07:35,  4.23it/s, loss=16.641604, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6636/8562 [26:55<07:31,  4.26it/s, loss=16.641604, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6636/8562 [26:55<07:31,  4.26it/s, loss=16.641461, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6637/8562 [26:55<07:29,  4.28it/s, loss=16.641461, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6637/8562 [26:55<07:29,  4.28it/s, loss=16.641398, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6638/8562 [26:55<07:26,  4.31it/s, loss=16.641398, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6638/8562 [26:56<07:26,  4.31it/s, loss=16.640940, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6639/8562 [26:56<07:26,  4.31it/s, loss=16.640940, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6639/8562 [26:56<07:26,  4.31it/s, loss=16.641219, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6640/8562 [26:56<07:28,  4.29it/s, loss=16.641219, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6640/8562 [26:56<07:28,  4.29it/s, loss=16.641639, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6641/8562 [26:56<07:29,  4.27it/s, loss=16.641639, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6641/8562 [26:56<07:29,  4.27it/s, loss=16.641189, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6642/8562 [26:56<07:37,  4.20it/s, loss=16.641189, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6642/8562 [26:57<07:37,  4.20it/s, loss=16.640274, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6643/8562 [26:57<07:33,  4.23it/s, loss=16.640274, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6643/8562 [26:57<07:33,  4.23it/s, loss=16.639921, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6644/8562 [26:57<07:37,  4.19it/s, loss=16.639921, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6644/8562 [26:57<07:37,  4.19it/s, loss=16.640497, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6645/8562 [26:57<07:35,  4.21it/s, loss=16.640497, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6645/8562 [26:57<07:35,  4.21it/s, loss=16.640407, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6646/8562 [26:57<07:32,  4.23it/s, loss=16.640407, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6646/8562 [26:57<07:32,  4.23it/s, loss=16.641121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6647/8562 [26:57<07:31,  4.24it/s, loss=16.641121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6647/8562 [26:58<07:31,  4.24it/s, loss=16.641313, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6648/8562 [26:58<07:38,  4.17it/s, loss=16.641313, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6648/8562 [26:58<07:38,  4.17it/s, loss=16.641099, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6649/8562 [26:58<07:36,  4.19it/s, loss=16.641099, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6649/8562 [26:58<07:36,  4.19it/s, loss=16.641273, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6650/8562 [26:58<07:32,  4.22it/s, loss=16.641273, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6650/8562 [26:58<07:32,  4.22it/s, loss=16.641382, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6651/8562 [26:58<07:35,  4.19it/s, loss=16.641382, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6651/8562 [26:59<07:35,  4.19it/s, loss=16.641608, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6652/8562 [26:59<07:43,  4.12it/s, loss=16.641608, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6652/8562 [26:59<07:43,  4.12it/s, loss=16.640986, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6653/8562 [26:59<07:38,  4.16it/s, loss=16.640986, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6653/8562 [26:59<07:38,  4.16it/s, loss=16.641428, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6654/8562 [26:59<07:37,  4.17it/s, loss=16.641428, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6654/8562 [26:59<07:37,  4.17it/s, loss=16.641857, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6655/8562 [26:59<07:34,  4.19it/s, loss=16.641857, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6655/8562 [27:00<07:34,  4.19it/s, loss=16.641734, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6656/8562 [27:00<07:33,  4.20it/s, loss=16.641734, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6656/8562 [27:00<07:33,  4.20it/s, loss=16.641119, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6657/8562 [27:00<07:30,  4.23it/s, loss=16.641119, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6657/8562 [27:00<07:30,  4.23it/s, loss=16.641147, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6658/8562 [27:00<07:30,  4.23it/s, loss=16.641147, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6658/8562 [27:00<07:30,  4.23it/s, loss=16.641628, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6659/8562 [27:00<07:43,  4.11it/s, loss=16.641628, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6659/8562 [27:01<07:43,  4.11it/s, loss=16.641840, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6660/8562 [27:01<07:38,  4.15it/s, loss=16.641840, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6660/8562 [27:01<07:38,  4.15it/s, loss=16.642252, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6661/8562 [27:01<07:48,  4.06it/s, loss=16.642252, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6661/8562 [27:01<07:48,  4.06it/s, loss=16.642258, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6662/8562 [27:01<08:56,  3.54it/s, loss=16.642258, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6662/8562 [27:01<08:56,  3.54it/s, loss=16.642634, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6663/8562 [27:02<08:46,  3.60it/s, loss=16.642634, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6663/8562 [27:02<08:46,  3.60it/s, loss=16.643092, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6664/8562 [27:02<09:58,  3.17it/s, loss=16.643092, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6664/8562 [27:02<09:58,  3.17it/s, loss=16.642432, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6665/8562 [27:02<10:28,  3.02it/s, loss=16.642432, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6665/8562 [27:03<10:28,  3.02it/s, loss=16.642392, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6666/8562 [27:03<10:04,  3.14it/s, loss=16.642392, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6666/8562 [27:03<10:04,  3.14it/s, loss=16.641837, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6667/8562 [27:03<10:06,  3.13it/s, loss=16.641837, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6667/8562 [27:03<10:06,  3.13it/s, loss=16.642365, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6668/8562 [27:03<09:29,  3.33it/s, loss=16.642365, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6668/8562 [27:03<09:29,  3.33it/s, loss=16.643012, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6669/8562 [27:03<09:37,  3.28it/s, loss=16.643012, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6669/8562 [27:04<09:37,  3.28it/s, loss=16.642930, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6670/8562 [27:04<09:17,  3.39it/s, loss=16.642930, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6670/8562 [27:04<09:17,  3.39it/s, loss=16.642599, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6671/8562 [27:04<09:07,  3.45it/s, loss=16.642599, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6671/8562 [27:04<09:07,  3.45it/s, loss=16.643315, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6672/8562 [27:04<09:41,  3.25it/s, loss=16.643315, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6672/8562 [27:05<09:41,  3.25it/s, loss=16.643805, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6673/8562 [27:05<09:54,  3.18it/s, loss=16.643805, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6673/8562 [27:05<09:54,  3.18it/s, loss=16.644110, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6674/8562 [27:05<10:01,  3.14it/s, loss=16.644110, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6674/8562 [27:05<10:01,  3.14it/s, loss=16.644872, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6675/8562 [27:05<10:11,  3.08it/s, loss=16.644872, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6675/8562 [27:06<10:11,  3.08it/s, loss=16.644613, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6676/8562 [27:06<09:54,  3.17it/s, loss=16.644613, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6676/8562 [27:06<09:54,  3.17it/s, loss=16.644864, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6677/8562 [27:06<10:23,  3.02it/s, loss=16.644864, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6677/8562 [27:06<10:23,  3.02it/s, loss=16.644999, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6678/8562 [27:06<10:24,  3.01it/s, loss=16.644999, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6678/8562 [27:07<10:24,  3.01it/s, loss=16.644514, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6679/8562 [27:07<10:05,  3.11it/s, loss=16.644514, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6679/8562 [27:07<10:05,  3.11it/s, loss=16.643593, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6680/8562 [27:07<09:55,  3.16it/s, loss=16.643593, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6680/8562 [27:07<09:55,  3.16it/s, loss=16.643636, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6681/8562 [27:07<10:04,  3.11it/s, loss=16.643636, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6681/8562 [27:08<10:04,  3.11it/s, loss=16.642314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6682/8562 [27:08<10:27,  2.99it/s, loss=16.642314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6682/8562 [27:08<10:27,  2.99it/s, loss=16.642049, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6683/8562 [27:08<10:14,  3.06it/s, loss=16.642049, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6683/8562 [27:08<10:14,  3.06it/s, loss=16.642009, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6684/8562 [27:08<09:23,  3.33it/s, loss=16.642009, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6684/8562 [27:08<09:23,  3.33it/s, loss=16.641930, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6685/8562 [27:08<08:47,  3.56it/s, loss=16.641930, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6685/8562 [27:09<08:47,  3.56it/s, loss=16.641982, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6686/8562 [27:09<08:20,  3.75it/s, loss=16.641982, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6686/8562 [27:09<08:20,  3.75it/s, loss=16.641997, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6687/8562 [27:09<08:02,  3.88it/s, loss=16.641997, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6687/8562 [27:09<08:02,  3.88it/s, loss=16.641480, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6688/8562 [27:09<07:48,  4.00it/s, loss=16.641480, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6688/8562 [27:09<07:48,  4.00it/s, loss=16.641712, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6689/8562 [27:09<07:42,  4.05it/s, loss=16.641712, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6689/8562 [27:10<07:42,  4.05it/s, loss=16.641256, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6690/8562 [27:10<07:34,  4.12it/s, loss=16.641256, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6690/8562 [27:10<07:34,  4.12it/s, loss=16.641063, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6691/8562 [27:10<07:29,  4.16it/s, loss=16.641063, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6691/8562 [27:10<07:29,  4.16it/s, loss=16.641120, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6692/8562 [27:10<07:26,  4.19it/s, loss=16.641120, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6692/8562 [27:10<07:26,  4.19it/s, loss=16.640996, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6693/8562 [27:10<07:27,  4.17it/s, loss=16.640996, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6693/8562 [27:11<07:27,  4.17it/s, loss=16.640412, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6694/8562 [27:11<07:22,  4.22it/s, loss=16.640412, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6694/8562 [27:11<07:22,  4.22it/s, loss=16.640389, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6695/8562 [27:11<07:22,  4.22it/s, loss=16.640389, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6695/8562 [27:11<07:22,  4.22it/s, loss=16.640714, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6696/8562 [27:11<07:20,  4.24it/s, loss=16.640714, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6696/8562 [27:11<07:20,  4.24it/s, loss=16.640966, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6697/8562 [27:11<07:28,  4.16it/s, loss=16.640966, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6697/8562 [27:11<07:28,  4.16it/s, loss=16.640741, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6698/8562 [27:11<07:25,  4.18it/s, loss=16.640741, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6698/8562 [27:12<07:25,  4.18it/s, loss=16.640614, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6699/8562 [27:12<07:21,  4.22it/s, loss=16.640614, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6699/8562 [27:12<07:21,  4.22it/s, loss=16.640663, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6700/8562 [27:12<07:18,  4.24it/s, loss=16.640663, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6700/8562 [27:12<07:18,  4.24it/s, loss=16.640912, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6701/8562 [27:12<07:17,  4.26it/s, loss=16.640912, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6701/8562 [27:12<07:17,  4.26it/s, loss=16.639871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6702/8562 [27:12<07:16,  4.26it/s, loss=16.639871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6702/8562 [27:13<07:16,  4.26it/s, loss=16.639994, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6703/8562 [27:13<07:14,  4.28it/s, loss=16.639994, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6703/8562 [27:13<07:14,  4.28it/s, loss=16.639720, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6704/8562 [27:13<07:15,  4.26it/s, loss=16.639720, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6704/8562 [27:13<07:15,  4.26it/s, loss=16.639918, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6705/8562 [27:13<07:15,  4.27it/s, loss=16.639918, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6705/8562 [27:13<07:15,  4.27it/s, loss=16.639781, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6706/8562 [27:13<07:15,  4.27it/s, loss=16.639781, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6706/8562 [27:14<07:15,  4.27it/s, loss=16.640280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6707/8562 [27:14<07:14,  4.27it/s, loss=16.640280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6707/8562 [27:14<07:14,  4.27it/s, loss=16.640109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6708/8562 [27:14<07:12,  4.29it/s, loss=16.640109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6708/8562 [27:14<07:12,  4.29it/s, loss=16.640588, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6709/8562 [27:14<07:12,  4.28it/s, loss=16.640588, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6709/8562 [27:14<07:12,  4.28it/s, loss=16.640648, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6710/8562 [27:14<07:14,  4.26it/s, loss=16.640648, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6710/8562 [27:15<07:14,  4.26it/s, loss=16.640972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6711/8562 [27:15<07:12,  4.28it/s, loss=16.640972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6711/8562 [27:15<07:12,  4.28it/s, loss=16.641286, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6712/8562 [27:15<07:11,  4.29it/s, loss=16.641286, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6712/8562 [27:15<07:11,  4.29it/s, loss=16.640925, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6713/8562 [27:15<07:10,  4.30it/s, loss=16.640925, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6713/8562 [27:15<07:10,  4.30it/s, loss=16.641908, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6714/8562 [27:15<07:09,  4.30it/s, loss=16.641908, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6714/8562 [27:15<07:09,  4.30it/s, loss=16.642004, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6715/8562 [27:15<07:10,  4.29it/s, loss=16.642004, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6715/8562 [27:16<07:10,  4.29it/s, loss=16.642518, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6716/8562 [27:16<07:14,  4.25it/s, loss=16.642518, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6716/8562 [27:16<07:14,  4.25it/s, loss=16.642946, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6717/8562 [27:16<07:14,  4.25it/s, loss=16.642946, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6717/8562 [27:16<07:14,  4.25it/s, loss=16.642387, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6718/8562 [27:16<07:11,  4.27it/s, loss=16.642387, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6718/8562 [27:16<07:11,  4.27it/s, loss=16.641552, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6719/8562 [27:16<07:11,  4.27it/s, loss=16.641552, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6719/8562 [27:17<07:11,  4.27it/s, loss=16.641822, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6720/8562 [27:17<07:13,  4.25it/s, loss=16.641822, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6720/8562 [27:17<07:13,  4.25it/s, loss=16.641823, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6721/8562 [27:17<07:11,  4.26it/s, loss=16.641823, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  78%|███████▊  | 6721/8562 [27:17<07:11,  4.26it/s, loss=16.642223, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6722/8562 [27:17<07:22,  4.16it/s, loss=16.642223, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6722/8562 [27:17<07:22,  4.16it/s, loss=16.641662, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6723/8562 [27:17<07:25,  4.13it/s, loss=16.641662, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6723/8562 [27:18<07:25,  4.13it/s, loss=16.641961, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6724/8562 [27:18<07:28,  4.10it/s, loss=16.641961, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6724/8562 [27:18<07:28,  4.10it/s, loss=16.641983, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6725/8562 [27:18<07:23,  4.14it/s, loss=16.641983, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6725/8562 [27:18<07:23,  4.14it/s, loss=16.642133, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6726/8562 [27:18<07:22,  4.15it/s, loss=16.642133, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6726/8562 [27:18<07:22,  4.15it/s, loss=16.642012, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6727/8562 [27:18<07:19,  4.18it/s, loss=16.642012, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6727/8562 [27:19<07:19,  4.18it/s, loss=16.641478, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6728/8562 [27:19<07:19,  4.17it/s, loss=16.641478, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6728/8562 [27:19<07:19,  4.17it/s, loss=16.641176, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6729/8562 [27:19<07:20,  4.16it/s, loss=16.641176, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6729/8562 [27:19<07:20,  4.16it/s, loss=16.641386, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6730/8562 [27:19<07:17,  4.19it/s, loss=16.641386, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6730/8562 [27:19<07:17,  4.19it/s, loss=16.641552, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6731/8562 [27:19<07:15,  4.21it/s, loss=16.641552, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6731/8562 [27:20<07:15,  4.21it/s, loss=16.640604, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6732/8562 [27:20<07:15,  4.21it/s, loss=16.640604, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6732/8562 [27:20<07:15,  4.21it/s, loss=16.640581, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6733/8562 [27:20<07:14,  4.21it/s, loss=16.640581, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6733/8562 [27:20<07:14,  4.21it/s, loss=16.640869, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6734/8562 [27:20<07:13,  4.22it/s, loss=16.640869, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6734/8562 [27:20<07:13,  4.22it/s, loss=16.640302, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6735/8562 [27:20<07:13,  4.21it/s, loss=16.640302, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6735/8562 [27:20<07:13,  4.21it/s, loss=16.640161, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6736/8562 [27:20<07:14,  4.20it/s, loss=16.640161, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6736/8562 [27:21<07:14,  4.20it/s, loss=16.640275, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6737/8562 [27:21<07:14,  4.21it/s, loss=16.640275, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6737/8562 [27:21<07:14,  4.21it/s, loss=16.639208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6738/8562 [27:21<07:13,  4.21it/s, loss=16.639208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6738/8562 [27:21<07:13,  4.21it/s, loss=16.638611, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6739/8562 [27:21<07:10,  4.23it/s, loss=16.638611, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6739/8562 [27:21<07:10,  4.23it/s, loss=16.639061, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6740/8562 [27:21<07:11,  4.22it/s, loss=16.639061, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6740/8562 [27:22<07:11,  4.22it/s, loss=16.638238, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6741/8562 [27:22<07:11,  4.22it/s, loss=16.638238, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6741/8562 [27:22<07:11,  4.22it/s, loss=16.638402, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6742/8562 [27:22<07:11,  4.22it/s, loss=16.638402, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▊  | 6742/8562 [27:22<07:11,  4.22it/s, loss=16.638903, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6743/8562 [27:22<07:21,  4.12it/s, loss=16.638903, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6743/8562 [27:22<07:21,  4.12it/s, loss=16.638940, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6744/8562 [27:22<07:18,  4.15it/s, loss=16.638940, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6744/8562 [27:23<07:18,  4.15it/s, loss=16.638967, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6745/8562 [27:23<07:18,  4.14it/s, loss=16.638967, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6745/8562 [27:23<07:18,  4.14it/s, loss=16.639489, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6746/8562 [27:23<07:15,  4.17it/s, loss=16.639489, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6746/8562 [27:23<07:15,  4.17it/s, loss=16.638617, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6747/8562 [27:23<07:13,  4.19it/s, loss=16.638617, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6747/8562 [27:23<07:13,  4.19it/s, loss=16.638067, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6748/8562 [27:23<07:22,  4.10it/s, loss=16.638067, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6748/8562 [27:24<07:22,  4.10it/s, loss=16.637537, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6749/8562 [27:24<07:18,  4.13it/s, loss=16.637537, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6749/8562 [27:24<07:18,  4.13it/s, loss=16.637586, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6750/8562 [27:24<07:13,  4.18it/s, loss=16.637586, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6750/8562 [27:24<07:13,  4.18it/s, loss=16.637392, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6751/8562 [27:24<07:10,  4.21it/s, loss=16.637392, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6751/8562 [27:24<07:10,  4.21it/s, loss=16.637907, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6752/8562 [27:24<07:07,  4.23it/s, loss=16.637907, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6752/8562 [27:25<07:07,  4.23it/s, loss=16.636884, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6753/8562 [27:25<07:06,  4.25it/s, loss=16.636884, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6753/8562 [27:25<07:06,  4.25it/s, loss=16.636968, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6754/8562 [27:25<07:06,  4.24it/s, loss=16.636968, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6754/8562 [27:25<07:06,  4.24it/s, loss=16.637391, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6755/8562 [27:25<07:06,  4.24it/s, loss=16.637391, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6755/8562 [27:25<07:06,  4.24it/s, loss=16.637603, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6756/8562 [27:25<07:05,  4.24it/s, loss=16.637603, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6756/8562 [27:25<07:05,  4.24it/s, loss=16.637156, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6757/8562 [27:25<07:03,  4.26it/s, loss=16.637156, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6757/8562 [27:26<07:03,  4.26it/s, loss=16.637508, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6758/8562 [27:26<07:02,  4.27it/s, loss=16.637508, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6758/8562 [27:26<07:02,  4.27it/s, loss=16.637463, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6759/8562 [27:26<07:05,  4.24it/s, loss=16.637463, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6759/8562 [27:26<07:05,  4.24it/s, loss=16.637201, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6760/8562 [27:26<07:04,  4.24it/s, loss=16.637201, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6760/8562 [27:26<07:04,  4.24it/s, loss=16.636685, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6761/8562 [27:26<07:10,  4.18it/s, loss=16.636685, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6761/8562 [27:27<07:10,  4.18it/s, loss=16.637109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6762/8562 [27:27<07:09,  4.19it/s, loss=16.637109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6762/8562 [27:27<07:09,  4.19it/s, loss=16.636878, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6763/8562 [27:27<07:07,  4.21it/s, loss=16.636878, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6763/8562 [27:27<07:07,  4.21it/s, loss=16.637165, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6764/8562 [27:27<07:06,  4.21it/s, loss=16.637165, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6764/8562 [27:27<07:06,  4.21it/s, loss=16.636229, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6765/8562 [27:27<07:06,  4.21it/s, loss=16.636229, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6765/8562 [27:28<07:06,  4.21it/s, loss=16.635578, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6766/8562 [27:28<07:06,  4.22it/s, loss=16.635578, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6766/8562 [27:28<07:06,  4.22it/s, loss=16.635858, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6767/8562 [27:28<07:06,  4.21it/s, loss=16.635858, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6767/8562 [27:28<07:06,  4.21it/s, loss=16.636066, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6768/8562 [27:28<07:08,  4.19it/s, loss=16.636066, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6768/8562 [27:28<07:08,  4.19it/s, loss=16.635098, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6769/8562 [27:28<07:04,  4.22it/s, loss=16.635098, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6769/8562 [27:29<07:04,  4.22it/s, loss=16.635446, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6770/8562 [27:29<07:03,  4.23it/s, loss=16.635446, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6770/8562 [27:29<07:03,  4.23it/s, loss=16.635320, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6771/8562 [27:29<07:04,  4.22it/s, loss=16.635320, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6771/8562 [27:29<07:04,  4.22it/s, loss=16.635534, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6772/8562 [27:29<07:08,  4.18it/s, loss=16.635534, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6772/8562 [27:29<07:08,  4.18it/s, loss=16.635635, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6773/8562 [27:29<07:06,  4.20it/s, loss=16.635635, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6773/8562 [27:30<07:06,  4.20it/s, loss=16.635732, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6774/8562 [27:30<07:04,  4.22it/s, loss=16.635732, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6774/8562 [27:30<07:04,  4.22it/s, loss=16.635902, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6775/8562 [27:30<07:00,  4.25it/s, loss=16.635902, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6775/8562 [27:30<07:00,  4.25it/s, loss=16.635507, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6776/8562 [27:30<06:59,  4.25it/s, loss=16.635507, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6776/8562 [27:30<06:59,  4.25it/s, loss=16.636208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6777/8562 [27:30<07:01,  4.24it/s, loss=16.636208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6777/8562 [27:30<07:01,  4.24it/s, loss=16.636316, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6778/8562 [27:30<07:01,  4.24it/s, loss=16.636316, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6778/8562 [27:31<07:01,  4.24it/s, loss=16.636643, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6779/8562 [27:31<07:04,  4.20it/s, loss=16.636643, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6779/8562 [27:31<07:04,  4.20it/s, loss=16.636475, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6780/8562 [27:31<07:02,  4.22it/s, loss=16.636475, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6780/8562 [27:31<07:02,  4.22it/s, loss=16.636341, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6781/8562 [27:31<07:01,  4.22it/s, loss=16.636341, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6781/8562 [27:31<07:01,  4.22it/s, loss=16.636603, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6782/8562 [27:31<07:01,  4.23it/s, loss=16.636603, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6782/8562 [27:32<07:01,  4.23it/s, loss=16.636642, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6783/8562 [27:32<07:08,  4.15it/s, loss=16.636642, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6783/8562 [27:32<07:08,  4.15it/s, loss=16.636049, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6784/8562 [27:32<07:04,  4.19it/s, loss=16.636049, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6784/8562 [27:32<07:04,  4.19it/s, loss=16.636394, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6785/8562 [27:32<07:02,  4.20it/s, loss=16.636394, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6785/8562 [27:32<07:02,  4.20it/s, loss=16.635916, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6786/8562 [27:32<07:00,  4.22it/s, loss=16.635916, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6786/8562 [27:33<07:00,  4.22it/s, loss=16.635148, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6787/8562 [27:33<06:58,  4.24it/s, loss=16.635148, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6787/8562 [27:33<06:58,  4.24it/s, loss=16.635149, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6788/8562 [27:33<06:56,  4.26it/s, loss=16.635149, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6788/8562 [27:33<06:56,  4.26it/s, loss=16.635666, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6789/8562 [27:33<06:56,  4.26it/s, loss=16.635666, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6789/8562 [27:33<06:56,  4.26it/s, loss=16.635712, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6790/8562 [27:33<07:10,  4.11it/s, loss=16.635712, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6790/8562 [27:34<07:10,  4.11it/s, loss=16.635838, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6791/8562 [27:34<07:08,  4.14it/s, loss=16.635838, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6791/8562 [27:34<07:08,  4.14it/s, loss=16.634479, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6792/8562 [27:34<07:04,  4.17it/s, loss=16.634479, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6792/8562 [27:34<07:04,  4.17it/s, loss=16.634434, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6793/8562 [27:34<07:00,  4.20it/s, loss=16.634434, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6793/8562 [27:34<07:00,  4.20it/s, loss=16.634620, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6794/8562 [27:34<06:56,  4.24it/s, loss=16.634620, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6794/8562 [27:34<06:56,  4.24it/s, loss=16.633814, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6795/8562 [27:34<06:56,  4.25it/s, loss=16.633814, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6795/8562 [27:35<06:56,  4.25it/s, loss=16.633577, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6796/8562 [27:35<06:52,  4.28it/s, loss=16.633577, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6796/8562 [27:35<06:52,  4.28it/s, loss=16.634163, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6797/8562 [27:35<06:53,  4.27it/s, loss=16.634163, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6797/8562 [27:35<06:53,  4.27it/s, loss=16.634084, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6798/8562 [27:35<06:56,  4.24it/s, loss=16.634084, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6798/8562 [27:35<06:56,  4.24it/s, loss=16.633923, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6799/8562 [27:35<06:56,  4.23it/s, loss=16.633923, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6799/8562 [27:36<06:56,  4.23it/s, loss=16.634397, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6800/8562 [27:36<06:55,  4.24it/s, loss=16.634397, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6800/8562 [27:36<06:55,  4.24it/s, loss=16.633916, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6801/8562 [27:36<07:05,  4.13it/s, loss=16.633916, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6801/8562 [27:36<07:05,  4.13it/s, loss=16.633391, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6802/8562 [27:36<07:00,  4.18it/s, loss=16.633391, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6802/8562 [27:36<07:00,  4.18it/s, loss=16.633910, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6803/8562 [27:36<06:57,  4.21it/s, loss=16.633910, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6803/8562 [27:37<06:57,  4.21it/s, loss=16.634351, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6804/8562 [27:37<06:55,  4.23it/s, loss=16.634351, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6804/8562 [27:37<06:55,  4.23it/s, loss=16.633371, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6805/8562 [27:37<06:54,  4.24it/s, loss=16.633371, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6805/8562 [27:37<06:54,  4.24it/s, loss=16.632724, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6806/8562 [27:37<06:52,  4.26it/s, loss=16.632724, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  79%|███████▉  | 6806/8562 [27:37<06:52,  4.26it/s, loss=16.632491, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6807/8562 [27:37<06:51,  4.27it/s, loss=16.632491, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6807/8562 [27:38<06:51,  4.27it/s, loss=16.631796, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6808/8562 [27:38<06:51,  4.26it/s, loss=16.631796, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6808/8562 [27:38<06:51,  4.26it/s, loss=16.631649, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6809/8562 [27:38<06:50,  4.27it/s, loss=16.631649, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6809/8562 [27:38<06:50,  4.27it/s, loss=16.631272, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6810/8562 [27:38<06:54,  4.23it/s, loss=16.631272, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6810/8562 [27:38<06:54,  4.23it/s, loss=16.631255, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6811/8562 [27:38<06:53,  4.23it/s, loss=16.631255, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6811/8562 [27:39<06:53,  4.23it/s, loss=16.631828, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6812/8562 [27:39<06:53,  4.23it/s, loss=16.631828, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6812/8562 [27:39<06:53,  4.23it/s, loss=16.632535, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6813/8562 [27:39<06:54,  4.22it/s, loss=16.632535, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6813/8562 [27:39<06:54,  4.22it/s, loss=16.633051, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6814/8562 [27:39<06:54,  4.22it/s, loss=16.633051, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6814/8562 [27:39<06:54,  4.22it/s, loss=16.632999, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6815/8562 [27:39<06:52,  4.23it/s, loss=16.632999, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6815/8562 [27:39<06:52,  4.23it/s, loss=16.632340, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6816/8562 [27:39<06:59,  4.16it/s, loss=16.632340, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6816/8562 [27:40<06:59,  4.16it/s, loss=16.632904, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6817/8562 [27:40<06:56,  4.19it/s, loss=16.632904, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6817/8562 [27:40<06:56,  4.19it/s, loss=16.632280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6818/8562 [27:40<06:54,  4.20it/s, loss=16.632280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6818/8562 [27:40<06:54,  4.20it/s, loss=16.631768, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6819/8562 [27:40<06:52,  4.22it/s, loss=16.631768, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6819/8562 [27:40<06:52,  4.22it/s, loss=16.632275, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6820/8562 [27:40<06:52,  4.23it/s, loss=16.632275, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6820/8562 [27:41<06:52,  4.23it/s, loss=16.632720, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6821/8562 [27:41<07:00,  4.14it/s, loss=16.632720, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6821/8562 [27:41<07:00,  4.14it/s, loss=16.632755, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6822/8562 [27:41<06:59,  4.15it/s, loss=16.632755, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6822/8562 [27:41<06:59,  4.15it/s, loss=16.631983, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6823/8562 [27:41<06:56,  4.17it/s, loss=16.631983, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6823/8562 [27:41<06:56,  4.17it/s, loss=16.632259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6824/8562 [27:41<06:51,  4.22it/s, loss=16.632259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6824/8562 [27:42<06:51,  4.22it/s, loss=16.632718, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6825/8562 [27:42<06:50,  4.24it/s, loss=16.632718, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6825/8562 [27:42<06:50,  4.24it/s, loss=16.632277, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6826/8562 [27:42<06:49,  4.24it/s, loss=16.632277, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6826/8562 [27:42<06:49,  4.24it/s, loss=16.632763, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6827/8562 [27:42<06:47,  4.25it/s, loss=16.632763, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6827/8562 [27:42<06:47,  4.25it/s, loss=16.633397, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6828/8562 [27:42<07:03,  4.09it/s, loss=16.633397, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6828/8562 [27:43<07:03,  4.09it/s, loss=16.632678, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6829/8562 [27:43<07:00,  4.12it/s, loss=16.632678, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6829/8562 [27:43<07:00,  4.12it/s, loss=16.632874, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6830/8562 [27:43<06:58,  4.14it/s, loss=16.632874, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6830/8562 [27:43<06:58,  4.14it/s, loss=16.632776, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6831/8562 [27:43<06:57,  4.15it/s, loss=16.632776, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6831/8562 [27:43<06:57,  4.15it/s, loss=16.632455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6832/8562 [27:43<06:53,  4.19it/s, loss=16.632455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6832/8562 [27:44<06:53,  4.19it/s, loss=16.632958, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6833/8562 [27:44<06:51,  4.20it/s, loss=16.632958, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6833/8562 [27:44<06:51,  4.20it/s, loss=16.633179, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6834/8562 [27:44<06:48,  4.23it/s, loss=16.633179, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6834/8562 [27:44<06:48,  4.23it/s, loss=16.633767, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6835/8562 [27:44<06:45,  4.26it/s, loss=16.633767, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6835/8562 [27:44<06:45,  4.26it/s, loss=16.633925, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6836/8562 [27:44<06:45,  4.26it/s, loss=16.633925, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6836/8562 [27:44<06:45,  4.26it/s, loss=16.633671, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6837/8562 [27:44<06:50,  4.20it/s, loss=16.633671, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6837/8562 [27:45<06:50,  4.20it/s, loss=16.633175, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6838/8562 [27:45<06:51,  4.19it/s, loss=16.633175, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6838/8562 [27:45<06:51,  4.19it/s, loss=16.633325, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6839/8562 [27:45<06:49,  4.20it/s, loss=16.633325, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6839/8562 [27:45<06:49,  4.20it/s, loss=16.633830, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6840/8562 [27:45<06:46,  4.23it/s, loss=16.633830, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6840/8562 [27:45<06:46,  4.23it/s, loss=16.633676, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6841/8562 [27:45<06:54,  4.16it/s, loss=16.633676, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6841/8562 [27:46<06:54,  4.16it/s, loss=16.634094, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6842/8562 [27:46<06:52,  4.17it/s, loss=16.634094, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6842/8562 [27:46<06:52,  4.17it/s, loss=16.634127, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6843/8562 [27:46<06:49,  4.20it/s, loss=16.634127, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6843/8562 [27:46<06:49,  4.20it/s, loss=16.634475, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6844/8562 [27:46<06:48,  4.21it/s, loss=16.634475, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6844/8562 [27:46<06:48,  4.21it/s, loss=16.634152, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6845/8562 [27:46<06:50,  4.18it/s, loss=16.634152, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6845/8562 [27:47<06:50,  4.18it/s, loss=16.633773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6846/8562 [27:47<06:50,  4.18it/s, loss=16.633773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6846/8562 [27:47<06:50,  4.18it/s, loss=16.633941, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6847/8562 [27:47<06:48,  4.20it/s, loss=16.633941, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6847/8562 [27:47<06:48,  4.20it/s, loss=16.633900, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6848/8562 [27:47<06:47,  4.20it/s, loss=16.633900, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6848/8562 [27:47<06:47,  4.20it/s, loss=16.633350, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6849/8562 [27:47<06:53,  4.14it/s, loss=16.633350, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|███████▉  | 6849/8562 [27:48<06:53,  4.14it/s, loss=16.632949, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6850/8562 [27:48<06:49,  4.18it/s, loss=16.632949, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6850/8562 [27:48<06:49,  4.18it/s, loss=16.632666, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6851/8562 [27:48<06:49,  4.18it/s, loss=16.632666, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6851/8562 [27:48<06:49,  4.18it/s, loss=16.631768, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6852/8562 [27:48<06:46,  4.21it/s, loss=16.631768, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6852/8562 [27:48<06:46,  4.21it/s, loss=16.631689, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6853/8562 [27:48<06:44,  4.23it/s, loss=16.631689, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6853/8562 [27:49<06:44,  4.23it/s, loss=16.631505, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6854/8562 [27:49<06:42,  4.24it/s, loss=16.631505, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6854/8562 [27:49<06:42,  4.24it/s, loss=16.631028, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6855/8562 [27:49<06:43,  4.23it/s, loss=16.631028, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6855/8562 [27:49<06:43,  4.23it/s, loss=16.631214, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6856/8562 [27:49<06:40,  4.26it/s, loss=16.631214, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6856/8562 [27:49<06:40,  4.26it/s, loss=16.631479, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6857/8562 [27:49<06:43,  4.22it/s, loss=16.631479, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6857/8562 [27:49<06:43,  4.22it/s, loss=16.631482, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6858/8562 [27:49<06:42,  4.23it/s, loss=16.631482, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6858/8562 [27:50<06:42,  4.23it/s, loss=16.631771, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6859/8562 [27:50<06:42,  4.23it/s, loss=16.631771, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6859/8562 [27:50<06:42,  4.23it/s, loss=16.631624, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6860/8562 [27:50<06:41,  4.24it/s, loss=16.631624, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6860/8562 [27:50<06:41,  4.24it/s, loss=16.631129, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6861/8562 [27:50<06:50,  4.15it/s, loss=16.631129, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6861/8562 [27:50<06:50,  4.15it/s, loss=16.631353, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6862/8562 [27:50<06:46,  4.18it/s, loss=16.631353, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6862/8562 [27:51<06:46,  4.18it/s, loss=16.631371, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6863/8562 [27:51<06:47,  4.17it/s, loss=16.631371, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6863/8562 [27:51<06:47,  4.17it/s, loss=16.631779, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6864/8562 [27:51<06:44,  4.20it/s, loss=16.631779, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6864/8562 [27:51<06:44,  4.20it/s, loss=16.631540, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6865/8562 [27:51<06:43,  4.21it/s, loss=16.631540, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6865/8562 [27:51<06:43,  4.21it/s, loss=16.631886, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6866/8562 [27:51<06:41,  4.22it/s, loss=16.631886, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6866/8562 [27:52<06:41,  4.22it/s, loss=16.632209, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6867/8562 [27:52<06:39,  4.24it/s, loss=16.632209, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6867/8562 [27:52<06:39,  4.24it/s, loss=16.632652, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6868/8562 [27:52<06:38,  4.26it/s, loss=16.632652, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6868/8562 [27:52<06:38,  4.26it/s, loss=16.632296, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6869/8562 [27:52<06:39,  4.23it/s, loss=16.632296, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6869/8562 [27:52<06:39,  4.23it/s, loss=16.632928, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6870/8562 [27:52<06:37,  4.26it/s, loss=16.632928, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6870/8562 [27:53<06:37,  4.26it/s, loss=16.632661, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6871/8562 [27:53<06:35,  4.27it/s, loss=16.632661, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6871/8562 [27:53<06:35,  4.27it/s, loss=16.632558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6872/8562 [27:53<06:34,  4.28it/s, loss=16.632558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6872/8562 [27:53<06:34,  4.28it/s, loss=16.632515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6873/8562 [27:53<06:34,  4.28it/s, loss=16.632515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6873/8562 [27:53<06:34,  4.28it/s, loss=16.632597, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6874/8562 [27:53<06:32,  4.31it/s, loss=16.632597, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6874/8562 [27:53<06:32,  4.31it/s, loss=16.633027, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6875/8562 [27:53<06:35,  4.27it/s, loss=16.633027, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6875/8562 [27:54<06:35,  4.27it/s, loss=16.632792, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6876/8562 [27:54<06:33,  4.28it/s, loss=16.632792, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6876/8562 [27:54<06:33,  4.28it/s, loss=16.632631, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6877/8562 [27:54<06:34,  4.27it/s, loss=16.632631, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6877/8562 [27:54<06:34,  4.27it/s, loss=16.632812, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6878/8562 [27:54<06:41,  4.20it/s, loss=16.632812, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6878/8562 [27:54<06:41,  4.20it/s, loss=16.633197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6879/8562 [27:54<06:38,  4.22it/s, loss=16.633197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6879/8562 [27:55<06:38,  4.22it/s, loss=16.633645, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6880/8562 [27:55<06:36,  4.24it/s, loss=16.633645, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6880/8562 [27:55<06:36,  4.24it/s, loss=16.634104, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6881/8562 [27:55<06:37,  4.23it/s, loss=16.634104, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6881/8562 [27:55<06:37,  4.23it/s, loss=16.633958, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6882/8562 [27:55<06:41,  4.19it/s, loss=16.633958, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6882/8562 [27:55<06:41,  4.19it/s, loss=16.634369, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6883/8562 [27:55<06:50,  4.09it/s, loss=16.634369, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6883/8562 [27:56<06:50,  4.09it/s, loss=16.634783, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6884/8562 [27:56<06:47,  4.12it/s, loss=16.634783, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6884/8562 [27:56<06:47,  4.12it/s, loss=16.634181, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6885/8562 [27:56<06:46,  4.13it/s, loss=16.634181, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6885/8562 [27:56<06:46,  4.13it/s, loss=16.634246, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6886/8562 [27:56<06:41,  4.17it/s, loss=16.634246, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6886/8562 [27:56<06:41,  4.17it/s, loss=16.633736, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6887/8562 [27:56<06:39,  4.19it/s, loss=16.633736, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6887/8562 [27:57<06:39,  4.19it/s, loss=16.634208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6888/8562 [27:57<06:36,  4.22it/s, loss=16.634208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6888/8562 [27:57<06:36,  4.22it/s, loss=16.634752, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6889/8562 [27:57<06:36,  4.22it/s, loss=16.634752, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6889/8562 [27:57<06:36,  4.22it/s, loss=16.635318, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6890/8562 [27:57<06:37,  4.20it/s, loss=16.635318, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6890/8562 [27:57<06:37,  4.20it/s, loss=16.635104, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6891/8562 [27:57<06:37,  4.20it/s, loss=16.635104, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6891/8562 [27:58<06:37,  4.20it/s, loss=16.635377, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6892/8562 [27:58<06:34,  4.23it/s, loss=16.635377, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  80%|████████  | 6892/8562 [27:58<06:34,  4.23it/s, loss=16.635588, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6893/8562 [27:58<06:34,  4.23it/s, loss=16.635588, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6893/8562 [27:58<06:34,  4.23it/s, loss=16.635037, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6894/8562 [27:58<06:32,  4.25it/s, loss=16.635037, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6894/8562 [27:58<06:32,  4.25it/s, loss=16.635416, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6895/8562 [27:58<06:32,  4.25it/s, loss=16.635416, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6895/8562 [27:58<06:32,  4.25it/s, loss=16.635959, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6896/8562 [27:58<06:41,  4.15it/s, loss=16.635959, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6896/8562 [27:59<06:41,  4.15it/s, loss=16.635961, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6897/8562 [27:59<07:26,  3.73it/s, loss=16.635961, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6897/8562 [27:59<07:26,  3.73it/s, loss=16.636240, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6898/8562 [27:59<08:04,  3.44it/s, loss=16.636240, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6898/8562 [27:59<08:04,  3.44it/s, loss=16.636773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6899/8562 [27:59<07:38,  3.63it/s, loss=16.636773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6899/8562 [28:00<07:38,  3.63it/s, loss=16.636809, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6900/8562 [28:00<07:17,  3.80it/s, loss=16.636809, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6900/8562 [28:00<07:17,  3.80it/s, loss=16.637036, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6901/8562 [28:00<07:07,  3.89it/s, loss=16.637036, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6901/8562 [28:00<07:07,  3.89it/s, loss=16.637247, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6902/8562 [28:00<06:55,  3.99it/s, loss=16.637247, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6902/8562 [28:00<06:55,  3.99it/s, loss=16.637613, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6903/8562 [28:00<06:49,  4.05it/s, loss=16.637613, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6903/8562 [28:01<06:49,  4.05it/s, loss=16.637100, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6904/8562 [28:01<06:44,  4.09it/s, loss=16.637100, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6904/8562 [28:01<06:44,  4.09it/s, loss=16.637109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6905/8562 [28:01<06:42,  4.11it/s, loss=16.637109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6905/8562 [28:01<06:42,  4.11it/s, loss=16.636600, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6906/8562 [28:01<06:39,  4.14it/s, loss=16.636600, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6906/8562 [28:01<06:39,  4.14it/s, loss=16.636882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6907/8562 [28:01<06:36,  4.17it/s, loss=16.636882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6907/8562 [28:02<06:36,  4.17it/s, loss=16.636692, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6908/8562 [28:02<06:38,  4.15it/s, loss=16.636692, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6908/8562 [28:02<06:38,  4.15it/s, loss=16.636854, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6909/8562 [28:02<06:35,  4.18it/s, loss=16.636854, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6909/8562 [28:02<06:35,  4.18it/s, loss=16.636558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6910/8562 [28:02<06:33,  4.20it/s, loss=16.636558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6910/8562 [28:02<06:33,  4.20it/s, loss=16.636490, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6911/8562 [28:02<06:32,  4.20it/s, loss=16.636490, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6911/8562 [28:02<06:32,  4.20it/s, loss=16.636395, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6912/8562 [28:02<06:30,  4.23it/s, loss=16.636395, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6912/8562 [28:03<06:30,  4.23it/s, loss=16.636047, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6913/8562 [28:03<06:27,  4.25it/s, loss=16.636047, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6913/8562 [28:03<06:27,  4.25it/s, loss=16.636058, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6914/8562 [28:03<06:25,  4.28it/s, loss=16.636058, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6914/8562 [28:03<06:25,  4.28it/s, loss=16.635677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6915/8562 [28:03<06:24,  4.28it/s, loss=16.635677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6915/8562 [28:03<06:24,  4.28it/s, loss=16.636025, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6916/8562 [28:03<06:37,  4.14it/s, loss=16.636025, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6916/8562 [28:04<06:37,  4.14it/s, loss=16.636247, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6917/8562 [28:04<06:38,  4.13it/s, loss=16.636247, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6917/8562 [28:04<06:38,  4.13it/s, loss=16.636422, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6918/8562 [28:04<06:31,  4.20it/s, loss=16.636422, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6918/8562 [28:04<06:31,  4.20it/s, loss=16.637069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6919/8562 [28:04<06:28,  4.23it/s, loss=16.637069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6919/8562 [28:04<06:28,  4.23it/s, loss=16.637344, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6920/8562 [28:04<06:43,  4.07it/s, loss=16.637344, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6920/8562 [28:05<06:43,  4.07it/s, loss=16.636982, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6921/8562 [28:05<07:01,  3.89it/s, loss=16.636982, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6921/8562 [28:05<07:01,  3.89it/s, loss=16.636951, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6922/8562 [28:05<07:45,  3.52it/s, loss=16.636951, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6922/8562 [28:05<07:45,  3.52it/s, loss=16.637157, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6923/8562 [28:05<07:47,  3.51it/s, loss=16.637157, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6923/8562 [28:06<07:47,  3.51it/s, loss=16.637535, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6924/8562 [28:06<07:41,  3.55it/s, loss=16.637535, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6924/8562 [28:06<07:41,  3.55it/s, loss=16.636768, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6925/8562 [28:06<08:06,  3.36it/s, loss=16.636768, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6925/8562 [28:06<08:06,  3.36it/s, loss=16.636516, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6926/8562 [28:06<08:35,  3.17it/s, loss=16.636516, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6926/8562 [28:07<08:35,  3.17it/s, loss=16.636146, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6927/8562 [28:07<08:37,  3.16it/s, loss=16.636146, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6927/8562 [28:07<08:37,  3.16it/s, loss=16.635700, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6928/8562 [28:07<08:12,  3.32it/s, loss=16.635700, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6928/8562 [28:07<08:12,  3.32it/s, loss=16.635941, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6929/8562 [28:07<07:58,  3.41it/s, loss=16.635941, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6929/8562 [28:08<07:58,  3.41it/s, loss=16.635150, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6930/8562 [28:08<08:31,  3.19it/s, loss=16.635150, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6930/8562 [28:08<08:31,  3.19it/s, loss=16.635141, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6931/8562 [28:08<08:44,  3.11it/s, loss=16.635141, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6931/8562 [28:08<08:44,  3.11it/s, loss=16.634923, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6932/8562 [28:08<08:13,  3.31it/s, loss=16.634923, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6932/8562 [28:08<08:13,  3.31it/s, loss=16.635371, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6933/8562 [28:08<08:05,  3.36it/s, loss=16.635371, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6933/8562 [28:09<08:05,  3.36it/s, loss=16.635687, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6934/8562 [28:09<08:19,  3.26it/s, loss=16.635687, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6934/8562 [28:09<08:19,  3.26it/s, loss=16.635759, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6935/8562 [28:09<08:14,  3.29it/s, loss=16.635759, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6935/8562 [28:09<08:14,  3.29it/s, loss=16.635380, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6936/8562 [28:09<08:00,  3.39it/s, loss=16.635380, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6936/8562 [28:10<08:00,  3.39it/s, loss=16.635897, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6937/8562 [28:10<08:32,  3.17it/s, loss=16.635897, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6937/8562 [28:10<08:32,  3.17it/s, loss=16.635778, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6938/8562 [28:10<08:26,  3.21it/s, loss=16.635778, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6938/8562 [28:10<08:26,  3.21it/s, loss=16.635543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6939/8562 [28:10<08:36,  3.14it/s, loss=16.635543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6939/8562 [28:11<08:36,  3.14it/s, loss=16.636193, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6940/8562 [28:11<09:04,  2.98it/s, loss=16.636193, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6940/8562 [28:11<09:04,  2.98it/s, loss=16.634909, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6941/8562 [28:11<09:02,  2.99it/s, loss=16.634909, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6941/8562 [28:11<09:02,  2.99it/s, loss=16.634288, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6942/8562 [28:11<09:37,  2.80it/s, loss=16.634288, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6942/8562 [28:12<09:37,  2.80it/s, loss=16.634882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6943/8562 [28:12<09:32,  2.83it/s, loss=16.634882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6943/8562 [28:12<09:32,  2.83it/s, loss=16.634656, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6944/8562 [28:12<08:34,  3.14it/s, loss=16.634656, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6944/8562 [28:12<08:34,  3.14it/s, loss=16.634489, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6945/8562 [28:12<07:58,  3.38it/s, loss=16.634489, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6945/8562 [28:12<07:58,  3.38it/s, loss=16.634266, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6946/8562 [28:12<07:33,  3.56it/s, loss=16.634266, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6946/8562 [28:13<07:33,  3.56it/s, loss=16.633949, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6947/8562 [28:13<07:11,  3.74it/s, loss=16.633949, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6947/8562 [28:13<07:11,  3.74it/s, loss=16.634405, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6948/8562 [28:13<06:56,  3.88it/s, loss=16.634405, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6948/8562 [28:13<06:56,  3.88it/s, loss=16.634293, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6949/8562 [28:13<06:45,  3.98it/s, loss=16.634293, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6949/8562 [28:13<06:45,  3.98it/s, loss=16.634933, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6950/8562 [28:13<06:41,  4.02it/s, loss=16.634933, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6950/8562 [28:14<06:41,  4.02it/s, loss=16.634222, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6951/8562 [28:14<06:37,  4.06it/s, loss=16.634222, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6951/8562 [28:14<06:37,  4.06it/s, loss=16.633752, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6952/8562 [28:14<06:34,  4.08it/s, loss=16.633752, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6952/8562 [28:14<06:34,  4.08it/s, loss=16.633917, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6953/8562 [28:14<06:31,  4.11it/s, loss=16.633917, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6953/8562 [28:14<06:31,  4.11it/s, loss=16.634442, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6954/8562 [28:14<06:29,  4.13it/s, loss=16.634442, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6954/8562 [28:15<06:29,  4.13it/s, loss=16.635207, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6955/8562 [28:15<06:30,  4.12it/s, loss=16.635207, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6955/8562 [28:15<06:30,  4.12it/s, loss=16.635095, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6956/8562 [28:15<06:27,  4.15it/s, loss=16.635095, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████  | 6956/8562 [28:15<06:27,  4.15it/s, loss=16.635150, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6957/8562 [28:15<06:22,  4.20it/s, loss=16.635150, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6957/8562 [28:15<06:22,  4.20it/s, loss=16.635264, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6958/8562 [28:15<06:22,  4.19it/s, loss=16.635264, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6958/8562 [28:16<06:22,  4.19it/s, loss=16.635836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6959/8562 [28:16<06:21,  4.21it/s, loss=16.635836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6959/8562 [28:16<06:21,  4.21it/s, loss=16.635979, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6960/8562 [28:16<06:28,  4.12it/s, loss=16.635979, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6960/8562 [28:16<06:28,  4.12it/s, loss=16.636035, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6961/8562 [28:16<06:29,  4.11it/s, loss=16.636035, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6961/8562 [28:16<06:29,  4.11it/s, loss=16.634614, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6962/8562 [28:16<06:26,  4.14it/s, loss=16.634614, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6962/8562 [28:17<06:26,  4.14it/s, loss=16.635053, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6963/8562 [28:17<06:29,  4.10it/s, loss=16.635053, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6963/8562 [28:17<06:29,  4.10it/s, loss=16.634534, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6964/8562 [28:17<06:26,  4.13it/s, loss=16.634534, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6964/8562 [28:17<06:26,  4.13it/s, loss=16.635167, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6965/8562 [28:17<06:25,  4.14it/s, loss=16.635167, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6965/8562 [28:17<06:25,  4.14it/s, loss=16.635407, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6966/8562 [28:17<06:25,  4.14it/s, loss=16.635407, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6966/8562 [28:18<06:25,  4.14it/s, loss=16.634631, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6967/8562 [28:18<06:22,  4.17it/s, loss=16.634631, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6967/8562 [28:18<06:22,  4.17it/s, loss=16.634927, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6968/8562 [28:18<06:19,  4.20it/s, loss=16.634927, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6968/8562 [28:18<06:19,  4.20it/s, loss=16.634720, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6969/8562 [28:18<06:19,  4.20it/s, loss=16.634720, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6969/8562 [28:18<06:19,  4.20it/s, loss=16.634558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6970/8562 [28:18<06:17,  4.22it/s, loss=16.634558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6970/8562 [28:18<06:17,  4.22it/s, loss=16.634910, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6971/8562 [28:18<06:17,  4.22it/s, loss=16.634910, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6971/8562 [28:19<06:17,  4.22it/s, loss=16.635405, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6972/8562 [28:19<06:21,  4.17it/s, loss=16.635405, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6972/8562 [28:19<06:21,  4.17it/s, loss=16.635100, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6973/8562 [28:19<06:20,  4.18it/s, loss=16.635100, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6973/8562 [28:19<06:20,  4.18it/s, loss=16.634951, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6974/8562 [28:19<06:18,  4.20it/s, loss=16.634951, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6974/8562 [28:19<06:18,  4.20it/s, loss=16.634747, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6975/8562 [28:19<06:16,  4.21it/s, loss=16.634747, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6975/8562 [28:20<06:16,  4.21it/s, loss=16.634024, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6976/8562 [28:20<06:14,  4.23it/s, loss=16.634024, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6976/8562 [28:20<06:14,  4.23it/s, loss=16.633674, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6977/8562 [28:20<06:14,  4.23it/s, loss=16.633674, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6977/8562 [28:20<06:14,  4.23it/s, loss=16.633905, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6978/8562 [28:20<06:11,  4.27it/s, loss=16.633905, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  81%|████████▏ | 6978/8562 [28:20<06:11,  4.27it/s, loss=16.634318, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6979/8562 [28:20<06:09,  4.28it/s, loss=16.634318, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6979/8562 [28:21<06:09,  4.28it/s, loss=16.634641, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6980/8562 [28:21<06:09,  4.28it/s, loss=16.634641, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6980/8562 [28:21<06:09,  4.28it/s, loss=16.635030, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6981/8562 [28:21<06:09,  4.28it/s, loss=16.635030, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6981/8562 [28:21<06:09,  4.28it/s, loss=16.635162, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6982/8562 [28:21<06:11,  4.26it/s, loss=16.635162, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6982/8562 [28:21<06:11,  4.26it/s, loss=16.634819, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6983/8562 [28:21<06:09,  4.28it/s, loss=16.634819, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6983/8562 [28:22<06:09,  4.28it/s, loss=16.635419, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6984/8562 [28:22<06:08,  4.28it/s, loss=16.635419, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6984/8562 [28:22<06:08,  4.28it/s, loss=16.635400, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6985/8562 [28:22<06:09,  4.27it/s, loss=16.635400, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6985/8562 [28:22<06:09,  4.27it/s, loss=16.635324, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6986/8562 [28:22<06:06,  4.29it/s, loss=16.635324, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6986/8562 [28:22<06:06,  4.29it/s, loss=16.635655, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6987/8562 [28:22<06:05,  4.31it/s, loss=16.635655, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6987/8562 [28:22<06:05,  4.31it/s, loss=16.636081, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6988/8562 [28:22<06:05,  4.30it/s, loss=16.636081, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6988/8562 [28:23<06:05,  4.30it/s, loss=16.636041, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6989/8562 [28:23<06:05,  4.31it/s, loss=16.636041, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6989/8562 [28:23<06:05,  4.31it/s, loss=16.636450, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6990/8562 [28:23<06:06,  4.29it/s, loss=16.636450, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6990/8562 [28:23<06:06,  4.29it/s, loss=16.636368, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6991/8562 [28:23<06:07,  4.28it/s, loss=16.636368, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6991/8562 [28:23<06:07,  4.28it/s, loss=16.636600, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6992/8562 [28:23<06:08,  4.26it/s, loss=16.636600, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6992/8562 [28:24<06:08,  4.26it/s, loss=16.636423, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6993/8562 [28:24<06:11,  4.22it/s, loss=16.636423, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6993/8562 [28:24<06:11,  4.22it/s, loss=16.636437, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6994/8562 [28:24<06:16,  4.17it/s, loss=16.636437, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6994/8562 [28:24<06:16,  4.17it/s, loss=16.636985, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6995/8562 [28:24<06:14,  4.19it/s, loss=16.636985, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6995/8562 [28:24<06:14,  4.19it/s, loss=16.637391, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6996/8562 [28:24<06:12,  4.20it/s, loss=16.637391, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6996/8562 [28:25<06:12,  4.20it/s, loss=16.637618, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6997/8562 [28:25<06:10,  4.22it/s, loss=16.637618, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6997/8562 [28:25<06:10,  4.22it/s, loss=16.638045, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6998/8562 [28:25<06:08,  4.24it/s, loss=16.638045, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6998/8562 [28:25<06:08,  4.24it/s, loss=16.638668, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6999/8562 [28:25<06:10,  4.21it/s, loss=16.638668, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 6999/8562 [28:25<06:10,  4.21it/s, loss=16.639202, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7000/8562 [28:25<06:08,  4.24it/s, loss=16.639202, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7000/8562 [28:26<06:08,  4.24it/s, loss=16.639231, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7001/8562 [28:26<06:07,  4.25it/s, loss=16.639231, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7001/8562 [28:26<06:07,  4.25it/s, loss=16.639381, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7002/8562 [28:26<06:05,  4.26it/s, loss=16.639381, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7002/8562 [28:26<06:05,  4.26it/s, loss=16.639658, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7003/8562 [28:26<06:06,  4.25it/s, loss=16.639658, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7003/8562 [28:26<06:06,  4.25it/s, loss=16.638758, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7004/8562 [28:26<06:06,  4.25it/s, loss=16.638758, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7004/8562 [28:26<06:06,  4.25it/s, loss=16.638875, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7005/8562 [28:26<06:12,  4.19it/s, loss=16.638875, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7005/8562 [28:27<06:12,  4.19it/s, loss=16.638847, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7006/8562 [28:27<06:10,  4.20it/s, loss=16.638847, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7006/8562 [28:27<06:10,  4.20it/s, loss=16.638164, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7007/8562 [28:27<06:16,  4.13it/s, loss=16.638164, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7007/8562 [28:27<06:16,  4.13it/s, loss=16.638161, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7008/8562 [28:27<06:12,  4.18it/s, loss=16.638161, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7008/8562 [28:27<06:12,  4.18it/s, loss=16.637803, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7009/8562 [28:27<06:23,  4.05it/s, loss=16.637803, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7009/8562 [28:28<06:23,  4.05it/s, loss=16.636679, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7010/8562 [28:28<06:17,  4.11it/s, loss=16.636679, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7010/8562 [28:28<06:17,  4.11it/s, loss=16.635806, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7011/8562 [28:28<06:17,  4.11it/s, loss=16.635806, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7011/8562 [28:28<06:17,  4.11it/s, loss=16.635746, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7012/8562 [28:28<06:12,  4.16it/s, loss=16.635746, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7012/8562 [28:28<06:12,  4.16it/s, loss=16.635662, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7013/8562 [28:28<06:11,  4.17it/s, loss=16.635662, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7013/8562 [28:29<06:11,  4.17it/s, loss=16.636074, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7014/8562 [28:29<06:09,  4.19it/s, loss=16.636074, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7014/8562 [28:29<06:09,  4.19it/s, loss=16.635965, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7015/8562 [28:29<06:08,  4.20it/s, loss=16.635965, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7015/8562 [28:29<06:08,  4.20it/s, loss=16.636418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7016/8562 [28:29<06:07,  4.20it/s, loss=16.636418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7016/8562 [28:29<06:07,  4.20it/s, loss=16.636949, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7017/8562 [28:29<06:06,  4.22it/s, loss=16.636949, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7017/8562 [28:30<06:06,  4.22it/s, loss=16.636989, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7018/8562 [28:30<06:11,  4.15it/s, loss=16.636989, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7018/8562 [28:30<06:11,  4.15it/s, loss=16.636583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7019/8562 [28:30<06:08,  4.19it/s, loss=16.636583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7019/8562 [28:30<06:08,  4.19it/s, loss=16.636389, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7020/8562 [28:30<06:08,  4.18it/s, loss=16.636389, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7020/8562 [28:30<06:08,  4.18it/s, loss=16.636673, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7021/8562 [28:30<06:05,  4.22it/s, loss=16.636673, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7021/8562 [28:31<06:05,  4.22it/s, loss=16.636077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7022/8562 [28:31<06:06,  4.21it/s, loss=16.636077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7022/8562 [28:31<06:06,  4.21it/s, loss=16.635952, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7023/8562 [28:31<06:04,  4.22it/s, loss=16.635952, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7023/8562 [28:31<06:04,  4.22it/s, loss=16.636245, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7024/8562 [28:31<06:02,  4.24it/s, loss=16.636245, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7024/8562 [28:31<06:02,  4.24it/s, loss=16.635897, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7025/8562 [28:31<06:00,  4.26it/s, loss=16.635897, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7025/8562 [28:31<06:00,  4.26it/s, loss=16.636485, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7026/8562 [28:32<06:03,  4.23it/s, loss=16.636485, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7026/8562 [28:32<06:03,  4.23it/s, loss=16.636611, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7027/8562 [28:32<06:01,  4.24it/s, loss=16.636611, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7027/8562 [28:32<06:01,  4.24it/s, loss=16.636970, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7028/8562 [28:32<06:00,  4.25it/s, loss=16.636970, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7028/8562 [28:32<06:00,  4.25it/s, loss=16.636953, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7029/8562 [28:32<06:04,  4.20it/s, loss=16.636953, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7029/8562 [28:32<06:04,  4.20it/s, loss=16.636997, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7030/8562 [28:32<06:06,  4.18it/s, loss=16.636997, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7030/8562 [28:33<06:06,  4.18it/s, loss=16.637746, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7031/8562 [28:33<06:05,  4.18it/s, loss=16.637746, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7031/8562 [28:33<06:05,  4.18it/s, loss=16.637520, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7032/8562 [28:33<06:04,  4.20it/s, loss=16.637520, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7032/8562 [28:33<06:04,  4.20it/s, loss=16.637848, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7033/8562 [28:33<06:03,  4.21it/s, loss=16.637848, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7033/8562 [28:33<06:03,  4.21it/s, loss=16.638243, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7034/8562 [28:33<06:02,  4.22it/s, loss=16.638243, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7034/8562 [28:34<06:02,  4.22it/s, loss=16.638425, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7035/8562 [28:34<06:04,  4.19it/s, loss=16.638425, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7035/8562 [28:34<06:04,  4.19it/s, loss=16.638795, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7036/8562 [28:34<06:05,  4.18it/s, loss=16.638795, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7036/8562 [28:34<06:05,  4.18it/s, loss=16.639123, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7037/8562 [28:34<06:02,  4.21it/s, loss=16.639123, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7037/8562 [28:34<06:02,  4.21it/s, loss=16.639306, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7038/8562 [28:34<06:00,  4.23it/s, loss=16.639306, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7038/8562 [28:35<06:00,  4.23it/s, loss=16.639280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7039/8562 [28:35<05:59,  4.24it/s, loss=16.639280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7039/8562 [28:35<05:59,  4.24it/s, loss=16.639439, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7040/8562 [28:35<06:01,  4.22it/s, loss=16.639439, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7040/8562 [28:35<06:01,  4.22it/s, loss=16.639621, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7041/8562 [28:35<06:00,  4.22it/s, loss=16.639621, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7041/8562 [28:35<06:00,  4.22it/s, loss=16.639485, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7042/8562 [28:35<05:59,  4.23it/s, loss=16.639485, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7042/8562 [28:36<05:59,  4.23it/s, loss=16.639455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7043/8562 [28:36<05:58,  4.24it/s, loss=16.639455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7043/8562 [28:36<05:58,  4.24it/s, loss=16.638394, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7044/8562 [28:36<05:56,  4.26it/s, loss=16.638394, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7044/8562 [28:36<05:56,  4.26it/s, loss=16.637934, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7045/8562 [28:36<05:56,  4.26it/s, loss=16.637934, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7045/8562 [28:36<05:56,  4.26it/s, loss=16.638506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7046/8562 [28:36<05:56,  4.25it/s, loss=16.638506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7046/8562 [28:36<05:56,  4.25it/s, loss=16.638332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7047/8562 [28:36<05:55,  4.26it/s, loss=16.638332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7047/8562 [28:37<05:55,  4.26it/s, loss=16.638564, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7048/8562 [28:37<05:53,  4.28it/s, loss=16.638564, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7048/8562 [28:37<05:53,  4.28it/s, loss=16.638196, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7049/8562 [28:37<05:54,  4.26it/s, loss=16.638196, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7049/8562 [28:37<05:54,  4.26it/s, loss=16.638612, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7050/8562 [28:37<05:56,  4.24it/s, loss=16.638612, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7050/8562 [28:37<05:56,  4.24it/s, loss=16.638809, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7051/8562 [28:37<05:55,  4.25it/s, loss=16.638809, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7051/8562 [28:38<05:55,  4.25it/s, loss=16.638195, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7052/8562 [28:38<05:55,  4.25it/s, loss=16.638195, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7052/8562 [28:38<05:55,  4.25it/s, loss=16.637943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7053/8562 [28:38<05:56,  4.24it/s, loss=16.637943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7053/8562 [28:38<05:56,  4.24it/s, loss=16.637735, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7054/8562 [28:38<05:55,  4.24it/s, loss=16.637735, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7054/8562 [28:38<05:55,  4.24it/s, loss=16.637707, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7055/8562 [28:38<05:53,  4.26it/s, loss=16.637707, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7055/8562 [28:39<05:53,  4.26it/s, loss=16.637095, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7056/8562 [28:39<06:01,  4.17it/s, loss=16.637095, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7056/8562 [28:39<06:01,  4.17it/s, loss=16.637197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7057/8562 [28:39<06:00,  4.17it/s, loss=16.637197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7057/8562 [28:39<06:00,  4.17it/s, loss=16.635952, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7058/8562 [28:39<05:58,  4.20it/s, loss=16.635952, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7058/8562 [28:39<05:58,  4.20it/s, loss=16.636257, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7059/8562 [28:39<05:56,  4.21it/s, loss=16.636257, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7059/8562 [28:40<05:56,  4.21it/s, loss=16.636736, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7060/8562 [28:40<05:55,  4.22it/s, loss=16.636736, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7060/8562 [28:40<05:55,  4.22it/s, loss=16.635780, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7061/8562 [28:40<05:56,  4.21it/s, loss=16.635780, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7061/8562 [28:40<05:56,  4.21it/s, loss=16.634820, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7062/8562 [28:40<05:55,  4.22it/s, loss=16.634820, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7062/8562 [28:40<05:55,  4.22it/s, loss=16.635149, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7063/8562 [28:40<05:54,  4.23it/s, loss=16.635149, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  82%|████████▏ | 7063/8562 [28:40<05:54,  4.23it/s, loss=16.635061, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7064/8562 [28:41<05:55,  4.22it/s, loss=16.635061, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7064/8562 [28:41<05:55,  4.22it/s, loss=16.635795, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7065/8562 [28:41<05:56,  4.19it/s, loss=16.635795, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7065/8562 [28:41<05:56,  4.19it/s, loss=16.635352, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7066/8562 [28:41<05:58,  4.17it/s, loss=16.635352, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7066/8562 [28:41<05:58,  4.17it/s, loss=16.635247, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7067/8562 [28:41<05:56,  4.19it/s, loss=16.635247, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7067/8562 [28:41<05:56,  4.19it/s, loss=16.635662, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7068/8562 [28:41<05:54,  4.21it/s, loss=16.635662, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7068/8562 [28:42<05:54,  4.21it/s, loss=16.635830, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7069/8562 [28:42<05:52,  4.23it/s, loss=16.635830, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7069/8562 [28:42<05:52,  4.23it/s, loss=16.635319, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7070/8562 [28:42<05:51,  4.25it/s, loss=16.635319, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7070/8562 [28:42<05:51,  4.25it/s, loss=16.635231, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7071/8562 [28:42<05:51,  4.24it/s, loss=16.635231, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7071/8562 [28:42<05:51,  4.24it/s, loss=16.635174, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7072/8562 [28:42<05:50,  4.25it/s, loss=16.635174, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7072/8562 [28:43<05:50,  4.25it/s, loss=16.635773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7073/8562 [28:43<05:49,  4.25it/s, loss=16.635773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7073/8562 [28:43<05:49,  4.25it/s, loss=16.635464, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7074/8562 [28:43<05:53,  4.21it/s, loss=16.635464, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7074/8562 [28:43<05:53,  4.21it/s, loss=16.635540, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7075/8562 [28:43<05:52,  4.22it/s, loss=16.635540, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7075/8562 [28:43<05:52,  4.22it/s, loss=16.635628, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7076/8562 [28:43<05:50,  4.25it/s, loss=16.635628, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7076/8562 [28:44<05:50,  4.25it/s, loss=16.635225, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7077/8562 [28:44<05:50,  4.23it/s, loss=16.635225, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7077/8562 [28:44<05:50,  4.23it/s, loss=16.635206, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7078/8562 [28:44<05:56,  4.17it/s, loss=16.635206, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7078/8562 [28:44<05:56,  4.17it/s, loss=16.635188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7079/8562 [28:44<05:54,  4.18it/s, loss=16.635188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7079/8562 [28:44<05:54,  4.18it/s, loss=16.635092, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7080/8562 [28:44<05:51,  4.21it/s, loss=16.635092, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7080/8562 [28:45<05:51,  4.21it/s, loss=16.635149, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7081/8562 [28:45<05:49,  4.24it/s, loss=16.635149, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7081/8562 [28:45<05:49,  4.24it/s, loss=16.634941, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7082/8562 [28:45<05:48,  4.25it/s, loss=16.634941, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7082/8562 [28:45<05:48,  4.25it/s, loss=16.634931, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7083/8562 [28:45<05:47,  4.26it/s, loss=16.634931, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7083/8562 [28:45<05:47,  4.26it/s, loss=16.635314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7084/8562 [28:45<05:52,  4.19it/s, loss=16.635314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7084/8562 [28:45<05:52,  4.19it/s, loss=16.635077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7085/8562 [28:45<05:51,  4.21it/s, loss=16.635077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7085/8562 [28:46<05:51,  4.21it/s, loss=16.635516, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7086/8562 [28:46<05:52,  4.18it/s, loss=16.635516, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7086/8562 [28:46<05:52,  4.18it/s, loss=16.635008, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7087/8562 [28:46<05:50,  4.21it/s, loss=16.635008, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7087/8562 [28:46<05:50,  4.21it/s, loss=16.635271, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7088/8562 [28:46<05:47,  4.24it/s, loss=16.635271, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7088/8562 [28:46<05:47,  4.24it/s, loss=16.634998, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7089/8562 [28:46<05:47,  4.24it/s, loss=16.634998, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7089/8562 [28:47<05:47,  4.24it/s, loss=16.634913, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7090/8562 [28:47<05:47,  4.23it/s, loss=16.634913, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7090/8562 [28:47<05:47,  4.23it/s, loss=16.635218, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7091/8562 [28:47<05:46,  4.25it/s, loss=16.635218, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7091/8562 [28:47<05:46,  4.25it/s, loss=16.635704, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7092/8562 [28:47<05:45,  4.25it/s, loss=16.635704, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7092/8562 [28:47<05:45,  4.25it/s, loss=16.634733, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7093/8562 [28:47<05:53,  4.16it/s, loss=16.634733, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7093/8562 [28:48<05:53,  4.16it/s, loss=16.634975, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7094/8562 [28:48<05:51,  4.18it/s, loss=16.634975, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7094/8562 [28:48<05:51,  4.18it/s, loss=16.634529, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7095/8562 [28:48<05:49,  4.20it/s, loss=16.634529, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7095/8562 [28:48<05:49,  4.20it/s, loss=16.633626, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7096/8562 [28:48<05:49,  4.20it/s, loss=16.633626, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7096/8562 [28:48<05:49,  4.20it/s, loss=16.633470, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7097/8562 [28:48<05:48,  4.21it/s, loss=16.633470, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7097/8562 [28:49<05:48,  4.21it/s, loss=16.633815, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7098/8562 [28:49<05:46,  4.22it/s, loss=16.633815, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7098/8562 [28:49<05:46,  4.22it/s, loss=16.634311, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7099/8562 [28:49<05:43,  4.26it/s, loss=16.634311, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7099/8562 [28:49<05:43,  4.26it/s, loss=16.634077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7100/8562 [28:49<05:45,  4.24it/s, loss=16.634077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7100/8562 [28:49<05:45,  4.24it/s, loss=16.634188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7101/8562 [28:49<05:44,  4.24it/s, loss=16.634188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7101/8562 [28:49<05:44,  4.24it/s, loss=16.634468, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7102/8562 [28:50<05:42,  4.26it/s, loss=16.634468, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7102/8562 [28:50<05:42,  4.26it/s, loss=16.634458, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7103/8562 [28:50<05:42,  4.27it/s, loss=16.634458, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7103/8562 [28:50<05:42,  4.27it/s, loss=16.634531, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7104/8562 [28:50<05:41,  4.27it/s, loss=16.634531, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7104/8562 [28:50<05:41,  4.27it/s, loss=16.634664, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7105/8562 [28:50<05:43,  4.24it/s, loss=16.634664, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7105/8562 [28:50<05:43,  4.24it/s, loss=16.634483, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7106/8562 [28:50<05:44,  4.23it/s, loss=16.634483, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7106/8562 [28:51<05:44,  4.23it/s, loss=16.635025, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7107/8562 [28:51<05:47,  4.19it/s, loss=16.635025, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7107/8562 [28:51<05:47,  4.19it/s, loss=16.635437, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7108/8562 [28:51<05:44,  4.22it/s, loss=16.635437, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7108/8562 [28:51<05:44,  4.22it/s, loss=16.635895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7109/8562 [28:51<05:50,  4.15it/s, loss=16.635895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7109/8562 [28:51<05:50,  4.15it/s, loss=16.635245, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7110/8562 [28:51<05:46,  4.19it/s, loss=16.635245, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7110/8562 [28:52<05:46,  4.19it/s, loss=16.634822, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7111/8562 [28:52<05:44,  4.21it/s, loss=16.634822, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7111/8562 [28:52<05:44,  4.21it/s, loss=16.634811, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7112/8562 [28:52<05:55,  4.08it/s, loss=16.634811, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7112/8562 [28:52<05:55,  4.08it/s, loss=16.635296, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7113/8562 [28:52<05:52,  4.11it/s, loss=16.635296, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7113/8562 [28:52<05:52,  4.11it/s, loss=16.634280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7114/8562 [28:52<05:48,  4.15it/s, loss=16.634280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7114/8562 [28:53<05:48,  4.15it/s, loss=16.633630, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7115/8562 [28:53<05:45,  4.19it/s, loss=16.633630, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7115/8562 [28:53<05:45,  4.19it/s, loss=16.634216, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7116/8562 [28:53<05:45,  4.18it/s, loss=16.634216, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7116/8562 [28:53<05:45,  4.18it/s, loss=16.634298, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7117/8562 [28:53<05:42,  4.22it/s, loss=16.634298, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7117/8562 [28:53<05:42,  4.22it/s, loss=16.634332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7118/8562 [28:53<05:40,  4.24it/s, loss=16.634332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7118/8562 [28:54<05:40,  4.24it/s, loss=16.634479, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7119/8562 [28:54<05:43,  4.20it/s, loss=16.634479, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7119/8562 [28:54<05:43,  4.20it/s, loss=16.634522, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7120/8562 [28:54<05:40,  4.24it/s, loss=16.634522, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7120/8562 [28:54<05:40,  4.24it/s, loss=16.634049, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7121/8562 [28:54<05:39,  4.24it/s, loss=16.634049, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7121/8562 [28:54<05:39,  4.24it/s, loss=16.633835, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7122/8562 [28:54<05:42,  4.20it/s, loss=16.633835, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7122/8562 [28:55<05:42,  4.20it/s, loss=16.633755, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7123/8562 [28:55<05:41,  4.22it/s, loss=16.633755, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7123/8562 [28:55<05:41,  4.22it/s, loss=16.633693, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7124/8562 [28:55<05:39,  4.23it/s, loss=16.633693, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7124/8562 [28:55<05:39,  4.23it/s, loss=16.633916, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7125/8562 [28:55<05:37,  4.25it/s, loss=16.633916, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7125/8562 [28:55<05:37,  4.25it/s, loss=16.634135, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7126/8562 [28:55<05:36,  4.27it/s, loss=16.634135, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7126/8562 [28:55<05:36,  4.27it/s, loss=16.634610, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7127/8562 [28:55<05:43,  4.17it/s, loss=16.634610, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7127/8562 [28:56<05:43,  4.17it/s, loss=16.634995, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7128/8562 [28:56<05:41,  4.21it/s, loss=16.634995, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7128/8562 [28:56<05:41,  4.21it/s, loss=16.634940, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7129/8562 [28:56<05:38,  4.23it/s, loss=16.634940, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7129/8562 [28:56<05:38,  4.23it/s, loss=16.635386, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7130/8562 [28:56<05:38,  4.23it/s, loss=16.635386, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7130/8562 [28:56<05:38,  4.23it/s, loss=16.635494, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7131/8562 [28:56<05:41,  4.19it/s, loss=16.635494, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7131/8562 [28:57<05:41,  4.19it/s, loss=16.635797, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7132/8562 [28:57<05:38,  4.22it/s, loss=16.635797, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7132/8562 [28:57<05:38,  4.22it/s, loss=16.635533, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7133/8562 [28:57<05:40,  4.20it/s, loss=16.635533, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7133/8562 [28:57<05:40,  4.20it/s, loss=16.635243, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7134/8562 [28:57<05:37,  4.24it/s, loss=16.635243, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7134/8562 [28:57<05:37,  4.24it/s, loss=16.635642, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7135/8562 [28:57<05:36,  4.24it/s, loss=16.635642, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7135/8562 [28:58<05:36,  4.24it/s, loss=16.636303, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7136/8562 [28:58<05:39,  4.20it/s, loss=16.636303, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7136/8562 [28:58<05:39,  4.20it/s, loss=16.636005, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7137/8562 [28:58<05:36,  4.23it/s, loss=16.636005, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7137/8562 [28:58<05:36,  4.23it/s, loss=16.635461, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7138/8562 [28:58<05:35,  4.25it/s, loss=16.635461, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7138/8562 [28:58<05:35,  4.25it/s, loss=16.636074, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7139/8562 [28:58<05:38,  4.20it/s, loss=16.636074, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7139/8562 [28:59<05:38,  4.20it/s, loss=16.636512, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7140/8562 [28:59<05:37,  4.22it/s, loss=16.636512, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7140/8562 [28:59<05:37,  4.22it/s, loss=16.636530, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7141/8562 [28:59<05:35,  4.23it/s, loss=16.636530, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7141/8562 [28:59<05:35,  4.23it/s, loss=16.637188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7142/8562 [28:59<05:34,  4.24it/s, loss=16.637188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7142/8562 [28:59<05:34,  4.24it/s, loss=16.637426, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7143/8562 [28:59<05:34,  4.24it/s, loss=16.637426, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7143/8562 [28:59<05:34,  4.24it/s, loss=16.637737, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7144/8562 [28:59<05:33,  4.25it/s, loss=16.637737, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7144/8562 [29:00<05:33,  4.25it/s, loss=16.638233, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7145/8562 [29:00<05:33,  4.25it/s, loss=16.638233, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7145/8562 [29:00<05:33,  4.25it/s, loss=16.638682, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7146/8562 [29:00<05:32,  4.26it/s, loss=16.638682, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7146/8562 [29:00<05:32,  4.26it/s, loss=16.638473, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7147/8562 [29:00<05:36,  4.21it/s, loss=16.638473, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7147/8562 [29:00<05:36,  4.21it/s, loss=16.638773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7148/8562 [29:00<05:34,  4.23it/s, loss=16.638773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7148/8562 [29:01<05:34,  4.23it/s, loss=16.638400, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7149/8562 [29:01<05:34,  4.23it/s, loss=16.638400, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  83%|████████▎ | 7149/8562 [29:01<05:34,  4.23it/s, loss=16.638168, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7150/8562 [29:01<05:35,  4.21it/s, loss=16.638168, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7150/8562 [29:01<05:35,  4.21it/s, loss=16.638583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7151/8562 [29:01<05:32,  4.25it/s, loss=16.638583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7151/8562 [29:01<05:32,  4.25it/s, loss=16.639078, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7152/8562 [29:01<05:32,  4.24it/s, loss=16.639078, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7152/8562 [29:02<05:32,  4.24it/s, loss=16.638385, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7153/8562 [29:02<05:30,  4.26it/s, loss=16.638385, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7153/8562 [29:02<05:30,  4.26it/s, loss=16.638752, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7154/8562 [29:02<05:33,  4.22it/s, loss=16.638752, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7154/8562 [29:02<05:33,  4.22it/s, loss=16.639013, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7155/8562 [29:02<05:38,  4.16it/s, loss=16.639013, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7155/8562 [29:02<05:38,  4.16it/s, loss=16.639512, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7156/8562 [29:02<05:36,  4.18it/s, loss=16.639512, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7156/8562 [29:03<05:36,  4.18it/s, loss=16.639594, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7157/8562 [29:03<05:38,  4.15it/s, loss=16.639594, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7157/8562 [29:03<05:38,  4.15it/s, loss=16.638975, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7158/8562 [29:03<05:34,  4.20it/s, loss=16.638975, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7158/8562 [29:03<05:34,  4.20it/s, loss=16.638342, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7159/8562 [29:03<05:36,  4.17it/s, loss=16.638342, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7159/8562 [29:03<05:36,  4.17it/s, loss=16.638660, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7160/8562 [29:03<05:34,  4.19it/s, loss=16.638660, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7160/8562 [29:04<05:34,  4.19it/s, loss=16.638864, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7161/8562 [29:04<05:33,  4.21it/s, loss=16.638864, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7161/8562 [29:04<05:33,  4.21it/s, loss=16.638921, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7162/8562 [29:04<05:31,  4.22it/s, loss=16.638921, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7162/8562 [29:04<05:31,  4.22it/s, loss=16.638542, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7163/8562 [29:04<05:30,  4.23it/s, loss=16.638542, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7163/8562 [29:04<05:30,  4.23it/s, loss=16.638871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7164/8562 [29:04<05:29,  4.24it/s, loss=16.638871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7164/8562 [29:04<05:29,  4.24it/s, loss=16.638531, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7165/8562 [29:04<05:27,  4.27it/s, loss=16.638531, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7165/8562 [29:05<05:27,  4.27it/s, loss=16.638188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7166/8562 [29:05<05:27,  4.26it/s, loss=16.638188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7166/8562 [29:05<05:27,  4.26it/s, loss=16.638578, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7167/8562 [29:05<05:28,  4.24it/s, loss=16.638578, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7167/8562 [29:05<05:28,  4.24it/s, loss=16.638953, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7168/8562 [29:05<05:28,  4.25it/s, loss=16.638953, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7168/8562 [29:05<05:28,  4.25it/s, loss=16.638765, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7169/8562 [29:05<05:27,  4.25it/s, loss=16.638765, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7169/8562 [29:06<05:27,  4.25it/s, loss=16.638553, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7170/8562 [29:06<05:28,  4.24it/s, loss=16.638553, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▎ | 7170/8562 [29:06<05:28,  4.24it/s, loss=16.638978, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7171/8562 [29:06<05:28,  4.23it/s, loss=16.638978, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7171/8562 [29:06<05:28,  4.23it/s, loss=16.638906, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7172/8562 [29:06<05:27,  4.25it/s, loss=16.638906, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7172/8562 [29:06<05:27,  4.25it/s, loss=16.639189, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7173/8562 [29:06<05:26,  4.26it/s, loss=16.639189, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7173/8562 [29:07<05:26,  4.26it/s, loss=16.639087, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7174/8562 [29:07<05:25,  4.26it/s, loss=16.639087, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7174/8562 [29:07<05:25,  4.26it/s, loss=16.639148, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7175/8562 [29:07<05:25,  4.26it/s, loss=16.639148, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7175/8562 [29:07<05:25,  4.26it/s, loss=16.638847, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7176/8562 [29:07<05:25,  4.26it/s, loss=16.638847, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7176/8562 [29:07<05:25,  4.26it/s, loss=16.639062, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7177/8562 [29:07<05:25,  4.26it/s, loss=16.639062, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7177/8562 [29:08<05:25,  4.26it/s, loss=16.639259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7178/8562 [29:08<05:25,  4.26it/s, loss=16.639259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7178/8562 [29:08<05:25,  4.26it/s, loss=16.638718, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7179/8562 [29:08<05:27,  4.22it/s, loss=16.638718, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7179/8562 [29:08<05:27,  4.22it/s, loss=16.638730, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7180/8562 [29:08<05:45,  4.00it/s, loss=16.638730, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7180/8562 [29:08<05:45,  4.00it/s, loss=16.638694, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7181/8562 [29:08<06:08,  3.74it/s, loss=16.638694, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7181/8562 [29:09<06:08,  3.74it/s, loss=16.637974, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7182/8562 [29:09<06:21,  3.62it/s, loss=16.637974, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7182/8562 [29:09<06:21,  3.62it/s, loss=16.637613, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7183/8562 [29:09<07:04,  3.25it/s, loss=16.637613, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7183/8562 [29:09<07:04,  3.25it/s, loss=16.638213, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7184/8562 [29:09<07:41,  2.99it/s, loss=16.638213, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7184/8562 [29:10<07:41,  2.99it/s, loss=16.638166, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7185/8562 [29:10<07:57,  2.88it/s, loss=16.638166, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7185/8562 [29:10<07:57,  2.88it/s, loss=16.638767, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7186/8562 [29:10<07:52,  2.91it/s, loss=16.638767, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7186/8562 [29:10<07:52,  2.91it/s, loss=16.638871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7187/8562 [29:10<07:52,  2.91it/s, loss=16.638871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7187/8562 [29:11<07:52,  2.91it/s, loss=16.638609, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7188/8562 [29:11<07:51,  2.92it/s, loss=16.638609, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7188/8562 [29:11<07:51,  2.92it/s, loss=16.638279, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7189/8562 [29:11<07:57,  2.87it/s, loss=16.638279, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7189/8562 [29:11<07:57,  2.87it/s, loss=16.638677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7190/8562 [29:11<07:47,  2.93it/s, loss=16.638677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7190/8562 [29:12<07:47,  2.93it/s, loss=16.637862, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7191/8562 [29:12<07:50,  2.92it/s, loss=16.637862, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7191/8562 [29:12<07:50,  2.92it/s, loss=16.637811, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7192/8562 [29:12<08:02,  2.84it/s, loss=16.637811, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7192/8562 [29:12<08:02,  2.84it/s, loss=16.637295, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7193/8562 [29:13<07:37,  2.99it/s, loss=16.637295, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7193/8562 [29:13<07:37,  2.99it/s, loss=16.637393, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7194/8562 [29:13<07:57,  2.87it/s, loss=16.637393, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7194/8562 [29:13<07:57,  2.87it/s, loss=16.637677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7195/8562 [29:13<07:38,  2.98it/s, loss=16.637677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7195/8562 [29:14<07:38,  2.98it/s, loss=16.637834, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7196/8562 [29:14<08:08,  2.80it/s, loss=16.637834, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7196/8562 [29:14<08:08,  2.80it/s, loss=16.637336, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7197/8562 [29:14<07:50,  2.90it/s, loss=16.637336, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7197/8562 [29:14<07:50,  2.90it/s, loss=16.637173, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7198/8562 [29:14<07:20,  3.10it/s, loss=16.637173, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7198/8562 [29:15<07:20,  3.10it/s, loss=16.637422, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7199/8562 [29:15<07:37,  2.98it/s, loss=16.637422, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7199/8562 [29:15<07:37,  2.98it/s, loss=16.637380, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7200/8562 [29:15<07:27,  3.05it/s, loss=16.637380, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7200/8562 [29:15<07:27,  3.05it/s, loss=16.636943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7201/8562 [29:15<07:19,  3.10it/s, loss=16.636943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7201/8562 [29:15<07:19,  3.10it/s, loss=16.637166, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7202/8562 [29:15<07:10,  3.16it/s, loss=16.637166, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7202/8562 [29:16<07:10,  3.16it/s, loss=16.637209, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7203/8562 [29:16<06:37,  3.42it/s, loss=16.637209, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7203/8562 [29:16<06:37,  3.42it/s, loss=16.637118, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7204/8562 [29:16<06:16,  3.61it/s, loss=16.637118, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7204/8562 [29:16<06:16,  3.61it/s, loss=16.637207, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7205/8562 [29:16<05:58,  3.78it/s, loss=16.637207, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7205/8562 [29:16<05:58,  3.78it/s, loss=16.637506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7206/8562 [29:16<05:50,  3.87it/s, loss=16.637506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7206/8562 [29:17<05:50,  3.87it/s, loss=16.637786, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7207/8562 [29:17<05:45,  3.92it/s, loss=16.637786, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7207/8562 [29:17<05:45,  3.92it/s, loss=16.637217, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7208/8562 [29:17<05:38,  4.00it/s, loss=16.637217, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7208/8562 [29:17<05:38,  4.00it/s, loss=16.636899, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7209/8562 [29:17<05:34,  4.04it/s, loss=16.636899, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7209/8562 [29:17<05:34,  4.04it/s, loss=16.636857, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7210/8562 [29:17<05:31,  4.08it/s, loss=16.636857, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7210/8562 [29:18<05:31,  4.08it/s, loss=16.637079, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7211/8562 [29:18<05:26,  4.13it/s, loss=16.637079, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7211/8562 [29:18<05:26,  4.13it/s, loss=16.636800, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7212/8562 [29:18<05:23,  4.18it/s, loss=16.636800, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7212/8562 [29:18<05:23,  4.18it/s, loss=16.637344, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7213/8562 [29:18<05:23,  4.18it/s, loss=16.637344, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7213/8562 [29:18<05:23,  4.18it/s, loss=16.636839, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7214/8562 [29:18<05:20,  4.21it/s, loss=16.636839, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7214/8562 [29:19<05:20,  4.21it/s, loss=16.637174, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7215/8562 [29:19<05:19,  4.22it/s, loss=16.637174, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7215/8562 [29:19<05:19,  4.22it/s, loss=16.637514, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7216/8562 [29:19<05:17,  4.24it/s, loss=16.637514, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7216/8562 [29:19<05:17,  4.24it/s, loss=16.637114, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7217/8562 [29:19<05:17,  4.23it/s, loss=16.637114, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7217/8562 [29:19<05:17,  4.23it/s, loss=16.636647, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7218/8562 [29:19<05:18,  4.22it/s, loss=16.636647, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7218/8562 [29:20<05:18,  4.22it/s, loss=16.636021, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7219/8562 [29:20<05:21,  4.17it/s, loss=16.636021, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7219/8562 [29:20<05:21,  4.17it/s, loss=16.635366, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7220/8562 [29:20<05:19,  4.20it/s, loss=16.635366, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7220/8562 [29:20<05:19,  4.20it/s, loss=16.635570, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7221/8562 [29:20<05:17,  4.22it/s, loss=16.635570, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7221/8562 [29:20<05:17,  4.22it/s, loss=16.635640, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7222/8562 [29:20<05:17,  4.22it/s, loss=16.635640, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7222/8562 [29:20<05:17,  4.22it/s, loss=16.635872, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7223/8562 [29:20<05:15,  4.25it/s, loss=16.635872, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7223/8562 [29:21<05:15,  4.25it/s, loss=16.636102, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7224/8562 [29:21<05:15,  4.24it/s, loss=16.636102, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7224/8562 [29:21<05:15,  4.24it/s, loss=16.635794, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7225/8562 [29:21<05:27,  4.08it/s, loss=16.635794, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7225/8562 [29:21<05:27,  4.08it/s, loss=16.636085, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7226/8562 [29:21<05:22,  4.15it/s, loss=16.636085, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7226/8562 [29:21<05:22,  4.15it/s, loss=16.636512, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7227/8562 [29:21<05:29,  4.05it/s, loss=16.636512, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7227/8562 [29:22<05:29,  4.05it/s, loss=16.636210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7228/8562 [29:22<05:23,  4.12it/s, loss=16.636210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7228/8562 [29:22<05:23,  4.12it/s, loss=16.635030, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7229/8562 [29:22<05:20,  4.16it/s, loss=16.635030, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7229/8562 [29:22<05:20,  4.16it/s, loss=16.635232, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7230/8562 [29:22<05:19,  4.16it/s, loss=16.635232, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7230/8562 [29:22<05:19,  4.16it/s, loss=16.634593, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7231/8562 [29:22<05:18,  4.18it/s, loss=16.634593, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7231/8562 [29:23<05:18,  4.18it/s, loss=16.634187, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7232/8562 [29:23<05:16,  4.20it/s, loss=16.634187, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7232/8562 [29:23<05:16,  4.20it/s, loss=16.634334, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7233/8562 [29:23<05:14,  4.22it/s, loss=16.634334, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7233/8562 [29:23<05:14,  4.22it/s, loss=16.634162, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7234/8562 [29:23<05:13,  4.24it/s, loss=16.634162, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  84%|████████▍ | 7234/8562 [29:23<05:13,  4.24it/s, loss=16.633507, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7235/8562 [29:23<05:13,  4.23it/s, loss=16.633507, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7235/8562 [29:24<05:13,  4.23it/s, loss=16.633133, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7236/8562 [29:24<05:13,  4.23it/s, loss=16.633133, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7236/8562 [29:24<05:13,  4.23it/s, loss=16.632839, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7237/8562 [29:24<05:12,  4.24it/s, loss=16.632839, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7237/8562 [29:24<05:12,  4.24it/s, loss=16.633159, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7238/8562 [29:24<05:10,  4.27it/s, loss=16.633159, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7238/8562 [29:24<05:10,  4.27it/s, loss=16.633423, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7239/8562 [29:24<05:10,  4.26it/s, loss=16.633423, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7239/8562 [29:25<05:10,  4.26it/s, loss=16.633642, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7240/8562 [29:25<05:11,  4.24it/s, loss=16.633642, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7240/8562 [29:25<05:11,  4.24it/s, loss=16.634077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7241/8562 [29:25<05:10,  4.25it/s, loss=16.634077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7241/8562 [29:25<05:10,  4.25it/s, loss=16.633429, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7242/8562 [29:25<05:12,  4.23it/s, loss=16.633429, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7242/8562 [29:25<05:12,  4.23it/s, loss=16.633343, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7243/8562 [29:25<05:11,  4.24it/s, loss=16.633343, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7243/8562 [29:25<05:11,  4.24it/s, loss=16.633542, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7244/8562 [29:25<05:09,  4.26it/s, loss=16.633542, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7244/8562 [29:26<05:09,  4.26it/s, loss=16.633861, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7245/8562 [29:26<05:09,  4.26it/s, loss=16.633861, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7245/8562 [29:26<05:09,  4.26it/s, loss=16.633414, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7246/8562 [29:26<05:11,  4.22it/s, loss=16.633414, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7246/8562 [29:26<05:11,  4.22it/s, loss=16.633483, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7247/8562 [29:26<05:10,  4.23it/s, loss=16.633483, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7247/8562 [29:26<05:10,  4.23it/s, loss=16.633698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7248/8562 [29:26<05:11,  4.22it/s, loss=16.633698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7248/8562 [29:27<05:11,  4.22it/s, loss=16.633840, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7249/8562 [29:27<05:08,  4.25it/s, loss=16.633840, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7249/8562 [29:27<05:08,  4.25it/s, loss=16.634338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7250/8562 [29:27<05:18,  4.11it/s, loss=16.634338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7250/8562 [29:27<05:18,  4.11it/s, loss=16.634093, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7251/8562 [29:27<05:15,  4.16it/s, loss=16.634093, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7251/8562 [29:27<05:15,  4.16it/s, loss=16.633547, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7252/8562 [29:27<05:12,  4.19it/s, loss=16.633547, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7252/8562 [29:28<05:12,  4.19it/s, loss=16.633475, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7253/8562 [29:28<05:11,  4.20it/s, loss=16.633475, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7253/8562 [29:28<05:11,  4.20it/s, loss=16.632953, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7254/8562 [29:28<05:10,  4.21it/s, loss=16.632953, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7254/8562 [29:28<05:10,  4.21it/s, loss=16.633451, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7255/8562 [29:28<05:10,  4.22it/s, loss=16.633451, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7255/8562 [29:28<05:10,  4.22it/s, loss=16.633764, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7256/8562 [29:28<05:08,  4.23it/s, loss=16.633764, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7256/8562 [29:29<05:08,  4.23it/s, loss=16.633551, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7257/8562 [29:29<05:08,  4.23it/s, loss=16.633551, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7257/8562 [29:29<05:08,  4.23it/s, loss=16.633385, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7258/8562 [29:29<05:07,  4.24it/s, loss=16.633385, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7258/8562 [29:29<05:07,  4.24it/s, loss=16.633180, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7259/8562 [29:29<05:06,  4.25it/s, loss=16.633180, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7259/8562 [29:29<05:06,  4.25it/s, loss=16.632758, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7260/8562 [29:29<05:04,  4.27it/s, loss=16.632758, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7260/8562 [29:29<05:04,  4.27it/s, loss=16.633385, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7261/8562 [29:29<05:05,  4.27it/s, loss=16.633385, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7261/8562 [29:30<05:05,  4.27it/s, loss=16.632481, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7262/8562 [29:30<05:04,  4.27it/s, loss=16.632481, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7262/8562 [29:30<05:04,  4.27it/s, loss=16.632654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7263/8562 [29:30<05:04,  4.26it/s, loss=16.632654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7263/8562 [29:30<05:04,  4.26it/s, loss=16.632648, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7264/8562 [29:30<05:04,  4.26it/s, loss=16.632648, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7264/8562 [29:30<05:04,  4.26it/s, loss=16.632210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7265/8562 [29:30<05:05,  4.25it/s, loss=16.632210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7265/8562 [29:31<05:05,  4.25it/s, loss=16.631775, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7266/8562 [29:31<05:08,  4.20it/s, loss=16.631775, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7266/8562 [29:31<05:08,  4.20it/s, loss=16.631984, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7267/8562 [29:31<05:10,  4.17it/s, loss=16.631984, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7267/8562 [29:31<05:10,  4.17it/s, loss=16.631986, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7268/8562 [29:31<05:11,  4.16it/s, loss=16.631986, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7268/8562 [29:31<05:11,  4.16it/s, loss=16.631649, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7269/8562 [29:31<05:15,  4.10it/s, loss=16.631649, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7269/8562 [29:32<05:15,  4.10it/s, loss=16.631637, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7270/8562 [29:32<05:10,  4.16it/s, loss=16.631637, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7270/8562 [29:32<05:10,  4.16it/s, loss=16.632006, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7271/8562 [29:32<05:08,  4.19it/s, loss=16.632006, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7271/8562 [29:32<05:08,  4.19it/s, loss=16.632236, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7272/8562 [29:32<05:07,  4.19it/s, loss=16.632236, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7272/8562 [29:32<05:07,  4.19it/s, loss=16.632208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7273/8562 [29:32<05:06,  4.20it/s, loss=16.632208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7273/8562 [29:33<05:06,  4.20it/s, loss=16.632613, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7274/8562 [29:33<05:08,  4.17it/s, loss=16.632613, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7274/8562 [29:33<05:08,  4.17it/s, loss=16.632461, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7275/8562 [29:33<05:07,  4.18it/s, loss=16.632461, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7275/8562 [29:33<05:07,  4.18it/s, loss=16.632295, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7276/8562 [29:33<05:08,  4.17it/s, loss=16.632295, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7276/8562 [29:33<05:08,  4.17it/s, loss=16.632132, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7277/8562 [29:33<05:05,  4.21it/s, loss=16.632132, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▍ | 7277/8562 [29:34<05:05,  4.21it/s, loss=16.631702, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7278/8562 [29:34<05:09,  4.14it/s, loss=16.631702, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7278/8562 [29:34<05:09,  4.14it/s, loss=16.632115, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7279/8562 [29:34<05:08,  4.16it/s, loss=16.632115, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7279/8562 [29:34<05:08,  4.16it/s, loss=16.631648, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7280/8562 [29:34<05:05,  4.20it/s, loss=16.631648, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7280/8562 [29:34<05:05,  4.20it/s, loss=16.631882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7281/8562 [29:34<05:05,  4.19it/s, loss=16.631882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7281/8562 [29:35<05:05,  4.19it/s, loss=16.631727, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7282/8562 [29:35<05:03,  4.22it/s, loss=16.631727, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7282/8562 [29:35<05:03,  4.22it/s, loss=16.631994, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7283/8562 [29:35<05:03,  4.22it/s, loss=16.631994, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7283/8562 [29:35<05:03,  4.22it/s, loss=16.632109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7284/8562 [29:35<05:04,  4.20it/s, loss=16.632109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7284/8562 [29:35<05:04,  4.20it/s, loss=16.632376, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7285/8562 [29:35<05:02,  4.22it/s, loss=16.632376, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7285/8562 [29:35<05:02,  4.22it/s, loss=16.632542, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7286/8562 [29:35<05:06,  4.17it/s, loss=16.632542, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7286/8562 [29:36<05:06,  4.17it/s, loss=16.631881, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7287/8562 [29:36<05:03,  4.21it/s, loss=16.631881, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7287/8562 [29:36<05:03,  4.21it/s, loss=16.631235, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7288/8562 [29:36<05:01,  4.22it/s, loss=16.631235, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7288/8562 [29:36<05:01,  4.22it/s, loss=16.630946, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7289/8562 [29:36<05:04,  4.18it/s, loss=16.630946, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7289/8562 [29:36<05:04,  4.18it/s, loss=16.631221, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7290/8562 [29:36<05:05,  4.16it/s, loss=16.631221, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7290/8562 [29:37<05:05,  4.16it/s, loss=16.631302, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7291/8562 [29:37<05:03,  4.19it/s, loss=16.631302, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7291/8562 [29:37<05:03,  4.19it/s, loss=16.631332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7292/8562 [29:37<05:00,  4.22it/s, loss=16.631332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7292/8562 [29:37<05:00,  4.22it/s, loss=16.631891, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7293/8562 [29:37<05:06,  4.13it/s, loss=16.631891, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7293/8562 [29:37<05:06,  4.13it/s, loss=16.631219, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7294/8562 [29:37<05:05,  4.15it/s, loss=16.631219, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7294/8562 [29:38<05:05,  4.15it/s, loss=16.631401, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7295/8562 [29:38<05:03,  4.17it/s, loss=16.631401, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7295/8562 [29:38<05:03,  4.17it/s, loss=16.631050, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7296/8562 [29:38<05:01,  4.20it/s, loss=16.631050, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7296/8562 [29:38<05:01,  4.20it/s, loss=16.630726, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7297/8562 [29:38<04:59,  4.22it/s, loss=16.630726, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7297/8562 [29:38<04:59,  4.22it/s, loss=16.631120, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7298/8562 [29:38<04:58,  4.23it/s, loss=16.631120, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7298/8562 [29:39<04:58,  4.23it/s, loss=16.631455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7299/8562 [29:39<04:57,  4.24it/s, loss=16.631455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7299/8562 [29:39<04:57,  4.24it/s, loss=16.631893, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7300/8562 [29:39<04:57,  4.24it/s, loss=16.631893, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7300/8562 [29:39<04:57,  4.24it/s, loss=16.631927, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7301/8562 [29:39<04:56,  4.26it/s, loss=16.631927, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7301/8562 [29:39<04:56,  4.26it/s, loss=16.632504, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7302/8562 [29:39<04:54,  4.28it/s, loss=16.632504, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7302/8562 [29:39<04:54,  4.28it/s, loss=16.632455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7303/8562 [29:39<04:54,  4.28it/s, loss=16.632455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7303/8562 [29:40<04:54,  4.28it/s, loss=16.632884, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7304/8562 [29:40<04:52,  4.31it/s, loss=16.632884, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7304/8562 [29:40<04:52,  4.31it/s, loss=16.632895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7305/8562 [29:40<04:53,  4.29it/s, loss=16.632895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7305/8562 [29:40<04:53,  4.29it/s, loss=16.633531, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7306/8562 [29:40<04:54,  4.26it/s, loss=16.633531, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7306/8562 [29:40<04:54,  4.26it/s, loss=16.632725, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7307/8562 [29:40<05:01,  4.16it/s, loss=16.632725, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7307/8562 [29:41<05:01,  4.16it/s, loss=16.632869, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7308/8562 [29:41<04:59,  4.18it/s, loss=16.632869, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7308/8562 [29:41<04:59,  4.18it/s, loss=16.632979, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7309/8562 [29:41<04:57,  4.22it/s, loss=16.632979, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7309/8562 [29:41<04:57,  4.22it/s, loss=16.633190, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7310/8562 [29:41<04:55,  4.24it/s, loss=16.633190, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7310/8562 [29:41<04:55,  4.24it/s, loss=16.633515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7311/8562 [29:41<04:55,  4.23it/s, loss=16.633515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7311/8562 [29:42<04:55,  4.23it/s, loss=16.633197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7312/8562 [29:42<04:55,  4.22it/s, loss=16.633197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7312/8562 [29:42<04:55,  4.22it/s, loss=16.633286, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7313/8562 [29:42<04:57,  4.19it/s, loss=16.633286, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7313/8562 [29:42<04:57,  4.19it/s, loss=16.633129, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7314/8562 [29:42<04:58,  4.19it/s, loss=16.633129, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7314/8562 [29:42<04:58,  4.19it/s, loss=16.632902, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7315/8562 [29:42<04:54,  4.23it/s, loss=16.632902, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7315/8562 [29:43<04:54,  4.23it/s, loss=16.632673, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7316/8562 [29:43<04:53,  4.25it/s, loss=16.632673, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7316/8562 [29:43<04:53,  4.25it/s, loss=16.631930, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7317/8562 [29:43<04:53,  4.24it/s, loss=16.631930, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7317/8562 [29:43<04:53,  4.24it/s, loss=16.632451, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7318/8562 [29:43<04:51,  4.26it/s, loss=16.632451, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7318/8562 [29:43<04:51,  4.26it/s, loss=16.632438, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7319/8562 [29:43<04:55,  4.21it/s, loss=16.632438, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7319/8562 [29:44<04:55,  4.21it/s, loss=16.632713, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7320/8562 [29:44<04:55,  4.20it/s, loss=16.632713, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  85%|████████▌ | 7320/8562 [29:44<04:55,  4.20it/s, loss=16.632315, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7321/8562 [29:44<04:53,  4.22it/s, loss=16.632315, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7321/8562 [29:44<04:53,  4.22it/s, loss=16.632283, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7322/8562 [29:44<04:53,  4.23it/s, loss=16.632283, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7322/8562 [29:44<04:53,  4.23it/s, loss=16.632706, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7323/8562 [29:44<04:57,  4.17it/s, loss=16.632706, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7323/8562 [29:44<04:57,  4.17it/s, loss=16.632288, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7324/8562 [29:44<04:55,  4.19it/s, loss=16.632288, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7324/8562 [29:45<04:55,  4.19it/s, loss=16.632532, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7325/8562 [29:45<04:55,  4.19it/s, loss=16.632532, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7325/8562 [29:45<04:55,  4.19it/s, loss=16.632518, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7326/8562 [29:45<04:54,  4.20it/s, loss=16.632518, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7326/8562 [29:45<04:54,  4.20it/s, loss=16.632458, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7327/8562 [29:45<04:53,  4.21it/s, loss=16.632458, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7327/8562 [29:45<04:53,  4.21it/s, loss=16.632366, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7328/8562 [29:45<04:55,  4.17it/s, loss=16.632366, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7328/8562 [29:46<04:55,  4.17it/s, loss=16.632746, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7329/8562 [29:46<04:53,  4.20it/s, loss=16.632746, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7329/8562 [29:46<04:53,  4.20it/s, loss=16.633131, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7330/8562 [29:46<04:54,  4.18it/s, loss=16.633131, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7330/8562 [29:46<04:54,  4.18it/s, loss=16.633315, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7331/8562 [29:46<04:57,  4.14it/s, loss=16.633315, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7331/8562 [29:46<04:57,  4.14it/s, loss=16.633819, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7332/8562 [29:46<04:56,  4.15it/s, loss=16.633819, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7332/8562 [29:47<04:56,  4.15it/s, loss=16.634014, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7333/8562 [29:47<04:52,  4.20it/s, loss=16.634014, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7333/8562 [29:47<04:52,  4.20it/s, loss=16.634379, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7334/8562 [29:47<04:51,  4.22it/s, loss=16.634379, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7334/8562 [29:47<04:51,  4.22it/s, loss=16.634814, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7335/8562 [29:47<04:50,  4.23it/s, loss=16.634814, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7335/8562 [29:47<04:50,  4.23it/s, loss=16.635148, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7336/8562 [29:47<04:48,  4.25it/s, loss=16.635148, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7336/8562 [29:48<04:48,  4.25it/s, loss=16.635237, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7337/8562 [29:48<04:47,  4.27it/s, loss=16.635237, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7337/8562 [29:48<04:47,  4.27it/s, loss=16.635190, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7338/8562 [29:48<04:47,  4.25it/s, loss=16.635190, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7338/8562 [29:48<04:47,  4.25it/s, loss=16.635420, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7339/8562 [29:48<04:46,  4.27it/s, loss=16.635420, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7339/8562 [29:48<04:46,  4.27it/s, loss=16.634973, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7340/8562 [29:48<04:47,  4.25it/s, loss=16.634973, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7340/8562 [29:48<04:47,  4.25it/s, loss=16.635200, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7341/8562 [29:48<04:45,  4.27it/s, loss=16.635200, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7341/8562 [29:49<04:45,  4.27it/s, loss=16.635504, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7342/8562 [29:49<04:45,  4.27it/s, loss=16.635504, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7342/8562 [29:49<04:45,  4.27it/s, loss=16.634666, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7343/8562 [29:49<04:46,  4.26it/s, loss=16.634666, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7343/8562 [29:49<04:46,  4.26it/s, loss=16.634918, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7344/8562 [29:49<04:44,  4.28it/s, loss=16.634918, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7344/8562 [29:49<04:44,  4.28it/s, loss=16.634908, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7345/8562 [29:49<04:47,  4.23it/s, loss=16.634908, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7345/8562 [29:50<04:47,  4.23it/s, loss=16.634696, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7346/8562 [29:50<04:45,  4.26it/s, loss=16.634696, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7346/8562 [29:50<04:45,  4.26it/s, loss=16.634426, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7347/8562 [29:50<04:48,  4.21it/s, loss=16.634426, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7347/8562 [29:50<04:48,  4.21it/s, loss=16.634759, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7348/8562 [29:50<04:47,  4.23it/s, loss=16.634759, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7348/8562 [29:50<04:47,  4.23it/s, loss=16.634654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7349/8562 [29:50<04:50,  4.17it/s, loss=16.634654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7349/8562 [29:51<04:50,  4.17it/s, loss=16.635000, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7350/8562 [29:51<04:48,  4.20it/s, loss=16.635000, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7350/8562 [29:51<04:48,  4.20it/s, loss=16.635283, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7351/8562 [29:51<04:46,  4.23it/s, loss=16.635283, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7351/8562 [29:51<04:46,  4.23it/s, loss=16.635798, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7352/8562 [29:51<04:45,  4.25it/s, loss=16.635798, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7352/8562 [29:51<04:45,  4.25it/s, loss=16.635581, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7353/8562 [29:51<04:44,  4.25it/s, loss=16.635581, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7353/8562 [29:52<04:44,  4.25it/s, loss=16.635849, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7354/8562 [29:52<04:43,  4.26it/s, loss=16.635849, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7354/8562 [29:52<04:43,  4.26it/s, loss=16.635503, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7355/8562 [29:52<04:44,  4.24it/s, loss=16.635503, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7355/8562 [29:52<04:44,  4.24it/s, loss=16.635070, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7356/8562 [29:52<04:42,  4.27it/s, loss=16.635070, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7356/8562 [29:52<04:42,  4.27it/s, loss=16.633873, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7357/8562 [29:52<04:47,  4.19it/s, loss=16.633873, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7357/8562 [29:53<04:47,  4.19it/s, loss=16.633460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7358/8562 [29:53<04:46,  4.20it/s, loss=16.633460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7358/8562 [29:53<04:46,  4.20it/s, loss=16.633705, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7359/8562 [29:53<04:44,  4.23it/s, loss=16.633705, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7359/8562 [29:53<04:44,  4.23it/s, loss=16.633872, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7360/8562 [29:53<04:44,  4.22it/s, loss=16.633872, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7360/8562 [29:53<04:44,  4.22it/s, loss=16.634422, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7361/8562 [29:53<04:45,  4.21it/s, loss=16.634422, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7361/8562 [29:53<04:45,  4.21it/s, loss=16.634395, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7362/8562 [29:53<04:44,  4.22it/s, loss=16.634395, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7362/8562 [29:54<04:44,  4.22it/s, loss=16.634255, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7363/8562 [29:54<04:43,  4.23it/s, loss=16.634255, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7363/8562 [29:54<04:43,  4.23it/s, loss=16.634411, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7364/8562 [29:54<04:41,  4.25it/s, loss=16.634411, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7364/8562 [29:54<04:41,  4.25it/s, loss=16.634527, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7365/8562 [29:54<04:41,  4.25it/s, loss=16.634527, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7365/8562 [29:54<04:41,  4.25it/s, loss=16.634381, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7366/8562 [29:54<04:42,  4.23it/s, loss=16.634381, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7366/8562 [29:55<04:42,  4.23it/s, loss=16.634508, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7367/8562 [29:55<04:42,  4.23it/s, loss=16.634508, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7367/8562 [29:55<04:42,  4.23it/s, loss=16.635015, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7368/8562 [29:55<04:43,  4.21it/s, loss=16.635015, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7368/8562 [29:55<04:43,  4.21it/s, loss=16.635259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7369/8562 [29:55<04:41,  4.24it/s, loss=16.635259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7369/8562 [29:55<04:41,  4.24it/s, loss=16.635650, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7370/8562 [29:55<04:42,  4.22it/s, loss=16.635650, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7370/8562 [29:56<04:42,  4.22it/s, loss=16.635320, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7371/8562 [29:56<04:47,  4.14it/s, loss=16.635320, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7371/8562 [29:56<04:47,  4.14it/s, loss=16.635406, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7372/8562 [29:56<04:43,  4.20it/s, loss=16.635406, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7372/8562 [29:56<04:43,  4.20it/s, loss=16.635604, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7373/8562 [29:56<04:41,  4.22it/s, loss=16.635604, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7373/8562 [29:56<04:41,  4.22it/s, loss=16.635514, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7374/8562 [29:56<04:46,  4.15it/s, loss=16.635514, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7374/8562 [29:57<04:46,  4.15it/s, loss=16.635831, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7375/8562 [29:57<04:42,  4.21it/s, loss=16.635831, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7375/8562 [29:57<04:42,  4.21it/s, loss=16.636074, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7376/8562 [29:57<04:39,  4.25it/s, loss=16.636074, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7376/8562 [29:57<04:39,  4.25it/s, loss=16.636178, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7377/8562 [29:57<04:37,  4.27it/s, loss=16.636178, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7377/8562 [29:57<04:37,  4.27it/s, loss=16.635848, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7378/8562 [29:57<04:36,  4.28it/s, loss=16.635848, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7378/8562 [29:57<04:36,  4.28it/s, loss=16.635454, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7379/8562 [29:57<04:36,  4.28it/s, loss=16.635454, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7379/8562 [29:58<04:36,  4.28it/s, loss=16.635970, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7380/8562 [29:58<04:36,  4.28it/s, loss=16.635970, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7380/8562 [29:58<04:36,  4.28it/s, loss=16.636330, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7381/8562 [29:58<04:34,  4.30it/s, loss=16.636330, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7381/8562 [29:58<04:34,  4.30it/s, loss=16.636395, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7382/8562 [29:58<04:35,  4.28it/s, loss=16.636395, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7382/8562 [29:58<04:35,  4.28it/s, loss=16.636442, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7383/8562 [29:58<04:36,  4.27it/s, loss=16.636442, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7383/8562 [29:59<04:36,  4.27it/s, loss=16.636287, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7384/8562 [29:59<04:35,  4.28it/s, loss=16.636287, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▌ | 7384/8562 [29:59<04:35,  4.28it/s, loss=16.636511, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7385/8562 [29:59<04:52,  4.03it/s, loss=16.636511, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7385/8562 [29:59<04:52,  4.03it/s, loss=16.636032, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7386/8562 [29:59<04:46,  4.10it/s, loss=16.636032, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7386/8562 [29:59<04:46,  4.10it/s, loss=16.635788, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7387/8562 [29:59<04:43,  4.14it/s, loss=16.635788, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7387/8562 [30:00<04:43,  4.14it/s, loss=16.635883, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7388/8562 [30:00<04:41,  4.17it/s, loss=16.635883, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7388/8562 [30:00<04:41,  4.17it/s, loss=16.636381, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7389/8562 [30:00<04:39,  4.20it/s, loss=16.636381, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7389/8562 [30:00<04:39,  4.20it/s, loss=16.636346, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7390/8562 [30:00<04:38,  4.21it/s, loss=16.636346, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7390/8562 [30:00<04:38,  4.21it/s, loss=16.636020, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7391/8562 [30:00<04:37,  4.22it/s, loss=16.636020, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7391/8562 [30:01<04:37,  4.22it/s, loss=16.636132, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7392/8562 [30:01<04:36,  4.23it/s, loss=16.636132, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7392/8562 [30:01<04:36,  4.23it/s, loss=16.636236, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7393/8562 [30:01<04:37,  4.22it/s, loss=16.636236, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7393/8562 [30:01<04:37,  4.22it/s, loss=16.636498, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7394/8562 [30:01<04:34,  4.26it/s, loss=16.636498, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7394/8562 [30:01<04:34,  4.26it/s, loss=16.636560, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7395/8562 [30:01<04:33,  4.27it/s, loss=16.636560, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7395/8562 [30:02<04:33,  4.27it/s, loss=16.636854, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7396/8562 [30:02<04:39,  4.17it/s, loss=16.636854, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7396/8562 [30:02<04:39,  4.17it/s, loss=16.636054, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7397/8562 [30:02<04:38,  4.18it/s, loss=16.636054, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7397/8562 [30:02<04:38,  4.18it/s, loss=16.636190, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7398/8562 [30:02<04:42,  4.12it/s, loss=16.636190, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7398/8562 [30:02<04:42,  4.12it/s, loss=16.636515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7399/8562 [30:02<04:45,  4.08it/s, loss=16.636515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7399/8562 [30:03<04:45,  4.08it/s, loss=16.637151, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7400/8562 [30:03<04:41,  4.13it/s, loss=16.637151, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7400/8562 [30:03<04:41,  4.13it/s, loss=16.637373, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7401/8562 [30:03<04:43,  4.09it/s, loss=16.637373, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7401/8562 [30:03<04:43,  4.09it/s, loss=16.636901, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7402/8562 [30:03<04:40,  4.14it/s, loss=16.636901, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7402/8562 [30:03<04:40,  4.14it/s, loss=16.636413, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7403/8562 [30:03<04:39,  4.14it/s, loss=16.636413, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7403/8562 [30:03<04:39,  4.14it/s, loss=16.636573, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7404/8562 [30:03<04:42,  4.10it/s, loss=16.636573, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7404/8562 [30:04<04:42,  4.10it/s, loss=16.636853, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7405/8562 [30:04<04:38,  4.16it/s, loss=16.636853, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7405/8562 [30:04<04:38,  4.16it/s, loss=16.636753, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7406/8562 [30:04<04:35,  4.19it/s, loss=16.636753, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  86%|████████▋ | 7406/8562 [30:04<04:35,  4.19it/s, loss=16.636998, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7407/8562 [30:04<04:34,  4.22it/s, loss=16.636998, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7407/8562 [30:04<04:34,  4.22it/s, loss=16.635899, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7408/8562 [30:04<04:34,  4.20it/s, loss=16.635899, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7408/8562 [30:05<04:34,  4.20it/s, loss=16.635559, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7409/8562 [30:05<04:36,  4.17it/s, loss=16.635559, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7409/8562 [30:05<04:36,  4.17it/s, loss=16.635704, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7410/8562 [30:05<04:34,  4.20it/s, loss=16.635704, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7410/8562 [30:05<04:34,  4.20it/s, loss=16.635905, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7411/8562 [30:05<04:33,  4.21it/s, loss=16.635905, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7411/8562 [30:05<04:33,  4.21it/s, loss=16.636340, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7412/8562 [30:05<04:39,  4.12it/s, loss=16.636340, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7412/8562 [30:06<04:39,  4.12it/s, loss=16.635849, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7413/8562 [30:06<04:36,  4.16it/s, loss=16.635849, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7413/8562 [30:06<04:36,  4.16it/s, loss=16.636221, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7414/8562 [30:06<04:33,  4.19it/s, loss=16.636221, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7414/8562 [30:06<04:33,  4.19it/s, loss=16.636686, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7415/8562 [30:06<04:32,  4.21it/s, loss=16.636686, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7415/8562 [30:06<04:32,  4.21it/s, loss=16.637063, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7416/8562 [30:06<04:30,  4.24it/s, loss=16.637063, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7416/8562 [30:07<04:30,  4.24it/s, loss=16.637284, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7417/8562 [30:07<04:29,  4.25it/s, loss=16.637284, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7417/8562 [30:07<04:29,  4.25it/s, loss=16.637567, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7418/8562 [30:07<04:28,  4.26it/s, loss=16.637567, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7418/8562 [30:07<04:28,  4.26it/s, loss=16.637943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7419/8562 [30:07<04:38,  4.11it/s, loss=16.637943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7419/8562 [30:07<04:38,  4.11it/s, loss=16.637453, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7420/8562 [30:07<04:34,  4.17it/s, loss=16.637453, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7420/8562 [30:08<04:34,  4.17it/s, loss=16.636913, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7421/8562 [30:08<04:31,  4.20it/s, loss=16.636913, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7421/8562 [30:08<04:31,  4.20it/s, loss=16.636159, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7422/8562 [30:08<04:30,  4.22it/s, loss=16.636159, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7422/8562 [30:08<04:30,  4.22it/s, loss=16.635964, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7423/8562 [30:08<04:27,  4.26it/s, loss=16.635964, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7423/8562 [30:08<04:27,  4.26it/s, loss=16.635717, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7424/8562 [30:08<04:26,  4.27it/s, loss=16.635717, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7424/8562 [30:08<04:26,  4.27it/s, loss=16.635869, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7425/8562 [30:08<04:27,  4.25it/s, loss=16.635869, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7425/8562 [30:09<04:27,  4.25it/s, loss=16.635759, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7426/8562 [30:09<04:27,  4.25it/s, loss=16.635759, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7426/8562 [30:09<04:27,  4.25it/s, loss=16.635903, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7427/8562 [30:09<04:26,  4.25it/s, loss=16.635903, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7427/8562 [30:09<04:26,  4.25it/s, loss=16.635724, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7428/8562 [30:09<04:26,  4.25it/s, loss=16.635724, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7428/8562 [30:09<04:26,  4.25it/s, loss=16.635782, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7429/8562 [30:09<04:26,  4.24it/s, loss=16.635782, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7429/8562 [30:10<04:26,  4.24it/s, loss=16.635615, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7430/8562 [30:10<04:25,  4.26it/s, loss=16.635615, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7430/8562 [30:10<04:25,  4.26it/s, loss=16.634933, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7431/8562 [30:10<04:25,  4.26it/s, loss=16.634933, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7431/8562 [30:10<04:25,  4.26it/s, loss=16.635255, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7432/8562 [30:10<04:24,  4.27it/s, loss=16.635255, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7432/8562 [30:10<04:24,  4.27it/s, loss=16.635475, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7433/8562 [30:10<04:23,  4.28it/s, loss=16.635475, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7433/8562 [30:11<04:23,  4.28it/s, loss=16.635805, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7434/8562 [30:11<04:22,  4.29it/s, loss=16.635805, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7434/8562 [30:11<04:22,  4.29it/s, loss=16.635677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7435/8562 [30:11<04:25,  4.25it/s, loss=16.635677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7435/8562 [30:11<04:25,  4.25it/s, loss=16.634908, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7436/8562 [30:11<04:23,  4.28it/s, loss=16.634908, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7436/8562 [30:11<04:23,  4.28it/s, loss=16.635234, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7437/8562 [30:11<04:26,  4.21it/s, loss=16.635234, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7437/8562 [30:12<04:26,  4.21it/s, loss=16.635550, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7438/8562 [30:12<04:25,  4.24it/s, loss=16.635550, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7438/8562 [30:12<04:25,  4.24it/s, loss=16.635206, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7439/8562 [30:12<04:31,  4.13it/s, loss=16.635206, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7439/8562 [30:12<04:31,  4.13it/s, loss=16.635316, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7440/8562 [30:12<04:28,  4.18it/s, loss=16.635316, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7440/8562 [30:12<04:28,  4.18it/s, loss=16.635875, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7441/8562 [30:12<04:26,  4.21it/s, loss=16.635875, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7441/8562 [30:12<04:26,  4.21it/s, loss=16.636159, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7442/8562 [30:12<04:26,  4.20it/s, loss=16.636159, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7442/8562 [30:13<04:26,  4.20it/s, loss=16.636136, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7443/8562 [30:13<04:33,  4.10it/s, loss=16.636136, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7443/8562 [30:13<04:33,  4.10it/s, loss=16.636827, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7444/8562 [30:13<05:05,  3.66it/s, loss=16.636827, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7444/8562 [30:13<05:05,  3.66it/s, loss=16.637192, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7445/8562 [30:13<05:27,  3.41it/s, loss=16.637192, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7445/8562 [30:14<05:27,  3.41it/s, loss=16.637342, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7446/8562 [30:14<05:53,  3.16it/s, loss=16.637342, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7446/8562 [30:14<05:53,  3.16it/s, loss=16.637111, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7447/8562 [30:14<05:47,  3.21it/s, loss=16.637111, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7447/8562 [30:14<05:47,  3.21it/s, loss=16.636601, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7448/8562 [30:14<06:07,  3.03it/s, loss=16.636601, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7448/8562 [30:15<06:07,  3.03it/s, loss=16.636481, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7449/8562 [30:15<06:00,  3.08it/s, loss=16.636481, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7449/8562 [30:15<06:00,  3.08it/s, loss=16.636636, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7450/8562 [30:15<05:37,  3.29it/s, loss=16.636636, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7450/8562 [30:15<05:37,  3.29it/s, loss=16.636110, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7451/8562 [30:15<05:48,  3.19it/s, loss=16.636110, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7451/8562 [30:16<05:48,  3.19it/s, loss=16.636114, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7452/8562 [30:16<05:35,  3.31it/s, loss=16.636114, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7452/8562 [30:16<05:35,  3.31it/s, loss=16.635980, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7453/8562 [30:16<05:44,  3.22it/s, loss=16.635980, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7453/8562 [30:16<05:44,  3.22it/s, loss=16.636500, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7454/8562 [30:16<05:55,  3.12it/s, loss=16.636500, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7454/8562 [30:17<05:55,  3.12it/s, loss=16.635858, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7455/8562 [30:17<05:36,  3.29it/s, loss=16.635858, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7455/8562 [30:17<05:36,  3.29it/s, loss=16.635544, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7456/8562 [30:17<05:51,  3.15it/s, loss=16.635544, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7456/8562 [30:17<05:51,  3.15it/s, loss=16.635970, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7457/8562 [30:17<06:08,  3.00it/s, loss=16.635970, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7457/8562 [30:18<06:08,  3.00it/s, loss=16.636626, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7458/8562 [30:18<05:58,  3.08it/s, loss=16.636626, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7458/8562 [30:18<05:58,  3.08it/s, loss=16.636748, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7459/8562 [30:18<05:52,  3.13it/s, loss=16.636748, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7459/8562 [30:18<05:52,  3.13it/s, loss=16.636816, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7460/8562 [30:18<05:42,  3.22it/s, loss=16.636816, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7460/8562 [30:19<05:42,  3.22it/s, loss=16.636813, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7461/8562 [30:19<05:43,  3.20it/s, loss=16.636813, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7461/8562 [30:19<05:43,  3.20it/s, loss=16.637462, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7462/8562 [30:19<05:48,  3.16it/s, loss=16.637462, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7462/8562 [30:19<05:48,  3.16it/s, loss=16.636922, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7463/8562 [30:19<05:45,  3.18it/s, loss=16.636922, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7463/8562 [30:20<05:45,  3.18it/s, loss=16.636969, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7464/8562 [30:20<06:11,  2.95it/s, loss=16.636969, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7464/8562 [30:20<06:11,  2.95it/s, loss=16.637323, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7465/8562 [30:20<06:04,  3.01it/s, loss=16.637323, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7465/8562 [30:20<06:04,  3.01it/s, loss=16.637173, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7466/8562 [30:20<05:56,  3.07it/s, loss=16.637173, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7466/8562 [30:20<05:56,  3.07it/s, loss=16.637457, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7467/8562 [30:20<05:53,  3.10it/s, loss=16.637457, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7467/8562 [30:21<05:53,  3.10it/s, loss=16.637788, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7468/8562 [30:21<05:39,  3.22it/s, loss=16.637788, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7468/8562 [30:21<05:39,  3.22it/s, loss=16.637951, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7469/8562 [30:21<05:16,  3.45it/s, loss=16.637951, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7469/8562 [30:21<05:16,  3.45it/s, loss=16.637920, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7470/8562 [30:21<04:57,  3.67it/s, loss=16.637920, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7470/8562 [30:21<04:57,  3.67it/s, loss=16.637298, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7471/8562 [30:21<04:46,  3.81it/s, loss=16.637298, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7471/8562 [30:22<04:46,  3.81it/s, loss=16.637458, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7472/8562 [30:22<04:38,  3.91it/s, loss=16.637458, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7472/8562 [30:22<04:38,  3.91it/s, loss=16.636981, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7473/8562 [30:22<04:32,  4.00it/s, loss=16.636981, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7473/8562 [30:22<04:32,  4.00it/s, loss=16.637540, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7474/8562 [30:22<04:35,  3.95it/s, loss=16.637540, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7474/8562 [30:22<04:35,  3.95it/s, loss=16.637458, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7475/8562 [30:22<04:31,  4.01it/s, loss=16.637458, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7475/8562 [30:23<04:31,  4.01it/s, loss=16.637034, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7476/8562 [30:23<04:27,  4.06it/s, loss=16.637034, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7476/8562 [30:23<04:27,  4.06it/s, loss=16.636970, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7477/8562 [30:23<04:24,  4.11it/s, loss=16.636970, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7477/8562 [30:23<04:24,  4.11it/s, loss=16.636793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7478/8562 [30:23<04:22,  4.13it/s, loss=16.636793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7478/8562 [30:23<04:22,  4.13it/s, loss=16.637066, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7479/8562 [30:23<04:24,  4.10it/s, loss=16.637066, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7479/8562 [30:24<04:24,  4.10it/s, loss=16.636685, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7480/8562 [30:24<04:21,  4.13it/s, loss=16.636685, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7480/8562 [30:24<04:21,  4.13it/s, loss=16.637149, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7481/8562 [30:24<04:20,  4.15it/s, loss=16.637149, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7481/8562 [30:24<04:20,  4.15it/s, loss=16.637061, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7482/8562 [30:24<04:19,  4.17it/s, loss=16.637061, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7482/8562 [30:24<04:19,  4.17it/s, loss=16.637219, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7483/8562 [30:24<04:21,  4.12it/s, loss=16.637219, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7483/8562 [30:25<04:21,  4.12it/s, loss=16.637395, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7484/8562 [30:25<04:19,  4.15it/s, loss=16.637395, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7484/8562 [30:25<04:19,  4.15it/s, loss=16.637741, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7485/8562 [30:25<04:20,  4.14it/s, loss=16.637741, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7485/8562 [30:25<04:20,  4.14it/s, loss=16.638403, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7486/8562 [30:25<04:17,  4.17it/s, loss=16.638403, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7486/8562 [30:25<04:17,  4.17it/s, loss=16.638397, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7487/8562 [30:25<04:16,  4.19it/s, loss=16.638397, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7487/8562 [30:26<04:16,  4.19it/s, loss=16.638083, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7488/8562 [30:26<04:13,  4.23it/s, loss=16.638083, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7488/8562 [30:26<04:13,  4.23it/s, loss=16.637256, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7489/8562 [30:26<04:12,  4.24it/s, loss=16.637256, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7489/8562 [30:26<04:12,  4.24it/s, loss=16.637782, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7490/8562 [30:26<04:11,  4.27it/s, loss=16.637782, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7490/8562 [30:26<04:11,  4.27it/s, loss=16.637424, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7491/8562 [30:26<04:11,  4.25it/s, loss=16.637424, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  87%|████████▋ | 7491/8562 [30:27<04:11,  4.25it/s, loss=16.637861, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7492/8562 [30:27<04:15,  4.19it/s, loss=16.637861, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7492/8562 [30:27<04:15,  4.19it/s, loss=16.638027, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7493/8562 [30:27<04:12,  4.23it/s, loss=16.638027, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7493/8562 [30:27<04:12,  4.23it/s, loss=16.638519, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7494/8562 [30:27<04:13,  4.22it/s, loss=16.638519, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7494/8562 [30:27<04:13,  4.22it/s, loss=16.638498, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7495/8562 [30:27<04:12,  4.23it/s, loss=16.638498, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7495/8562 [30:27<04:12,  4.23it/s, loss=16.638607, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7496/8562 [30:27<04:12,  4.22it/s, loss=16.638607, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7496/8562 [30:28<04:12,  4.22it/s, loss=16.638413, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7497/8562 [30:28<04:11,  4.24it/s, loss=16.638413, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7497/8562 [30:28<04:11,  4.24it/s, loss=16.638243, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7498/8562 [30:28<04:10,  4.24it/s, loss=16.638243, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7498/8562 [30:28<04:10,  4.24it/s, loss=16.637793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7499/8562 [30:28<04:10,  4.24it/s, loss=16.637793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7499/8562 [30:28<04:10,  4.24it/s, loss=16.637773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7500/8562 [30:28<04:13,  4.19it/s, loss=16.637773, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7500/8562 [30:29<04:13,  4.19it/s, loss=16.637519, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7501/8562 [30:29<04:13,  4.19it/s, loss=16.637519, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7501/8562 [30:29<04:13,  4.19it/s, loss=16.637545, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7502/8562 [30:29<04:11,  4.21it/s, loss=16.637545, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7502/8562 [30:29<04:11,  4.21it/s, loss=16.637486, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7503/8562 [30:29<04:11,  4.22it/s, loss=16.637486, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7503/8562 [30:29<04:11,  4.22it/s, loss=16.637257, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7504/8562 [30:29<04:09,  4.24it/s, loss=16.637257, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7504/8562 [30:30<04:09,  4.24it/s, loss=16.637210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7505/8562 [30:30<04:10,  4.21it/s, loss=16.637210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7505/8562 [30:30<04:10,  4.21it/s, loss=16.637276, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7506/8562 [30:30<04:10,  4.22it/s, loss=16.637276, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7506/8562 [30:30<04:10,  4.22it/s, loss=16.637853, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7507/8562 [30:30<04:11,  4.20it/s, loss=16.637853, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7507/8562 [30:30<04:11,  4.20it/s, loss=16.638134, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7508/8562 [30:30<04:10,  4.20it/s, loss=16.638134, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7508/8562 [30:31<04:10,  4.20it/s, loss=16.637882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7509/8562 [30:31<04:16,  4.11it/s, loss=16.637882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7509/8562 [30:31<04:16,  4.11it/s, loss=16.638518, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7510/8562 [30:31<04:21,  4.02it/s, loss=16.638518, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7510/8562 [30:31<04:21,  4.02it/s, loss=16.637891, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7511/8562 [30:31<04:18,  4.06it/s, loss=16.637891, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7511/8562 [30:31<04:18,  4.06it/s, loss=16.637954, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7512/8562 [30:31<04:16,  4.09it/s, loss=16.637954, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7512/8562 [30:32<04:16,  4.09it/s, loss=16.637972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7513/8562 [30:32<04:20,  4.03it/s, loss=16.637972, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7513/8562 [30:32<04:20,  4.03it/s, loss=16.638143, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7514/8562 [30:32<04:17,  4.07it/s, loss=16.638143, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7514/8562 [30:32<04:17,  4.07it/s, loss=16.637861, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7515/8562 [30:32<04:13,  4.12it/s, loss=16.637861, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7515/8562 [30:32<04:13,  4.12it/s, loss=16.637705, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7516/8562 [30:32<04:14,  4.10it/s, loss=16.637705, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7516/8562 [30:33<04:14,  4.10it/s, loss=16.638115, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7517/8562 [30:33<04:11,  4.15it/s, loss=16.638115, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7517/8562 [30:33<04:11,  4.15it/s, loss=16.638033, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7518/8562 [30:33<04:10,  4.16it/s, loss=16.638033, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7518/8562 [30:33<04:10,  4.16it/s, loss=16.637281, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7519/8562 [30:33<04:10,  4.16it/s, loss=16.637281, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7519/8562 [30:33<04:10,  4.16it/s, loss=16.637397, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7520/8562 [30:33<04:10,  4.17it/s, loss=16.637397, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7520/8562 [30:33<04:10,  4.17it/s, loss=16.637583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7521/8562 [30:33<04:09,  4.17it/s, loss=16.637583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7521/8562 [30:34<04:09,  4.17it/s, loss=16.637834, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7522/8562 [30:34<04:08,  4.19it/s, loss=16.637834, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7522/8562 [30:34<04:08,  4.19it/s, loss=16.638398, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7523/8562 [30:34<04:07,  4.19it/s, loss=16.638398, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7523/8562 [30:34<04:07,  4.19it/s, loss=16.638596, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7524/8562 [30:34<04:06,  4.21it/s, loss=16.638596, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7524/8562 [30:34<04:06,  4.21it/s, loss=16.638706, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7525/8562 [30:34<04:07,  4.20it/s, loss=16.638706, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7525/8562 [30:35<04:07,  4.20it/s, loss=16.638871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7526/8562 [30:35<04:05,  4.22it/s, loss=16.638871, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7526/8562 [30:35<04:05,  4.22it/s, loss=16.638781, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7527/8562 [30:35<04:21,  3.96it/s, loss=16.638781, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7527/8562 [30:35<04:21,  3.96it/s, loss=16.638313, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7528/8562 [30:35<04:22,  3.94it/s, loss=16.638313, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7528/8562 [30:35<04:22,  3.94it/s, loss=16.638503, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7529/8562 [30:35<04:18,  4.00it/s, loss=16.638503, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7529/8562 [30:36<04:18,  4.00it/s, loss=16.638793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7530/8562 [30:36<04:13,  4.06it/s, loss=16.638793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7530/8562 [30:36<04:13,  4.06it/s, loss=16.638389, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7531/8562 [30:36<04:09,  4.13it/s, loss=16.638389, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7531/8562 [30:36<04:09,  4.13it/s, loss=16.637923, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7532/8562 [30:36<04:08,  4.14it/s, loss=16.637923, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7532/8562 [30:36<04:08,  4.14it/s, loss=16.638300, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7533/8562 [30:36<04:10,  4.10it/s, loss=16.638300, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7533/8562 [30:37<04:10,  4.10it/s, loss=16.637837, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7534/8562 [30:37<04:07,  4.15it/s, loss=16.637837, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7534/8562 [30:37<04:07,  4.15it/s, loss=16.637789, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7535/8562 [30:37<04:09,  4.12it/s, loss=16.637789, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7535/8562 [30:37<04:09,  4.12it/s, loss=16.637382, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7536/8562 [30:37<04:08,  4.14it/s, loss=16.637382, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7536/8562 [30:37<04:08,  4.14it/s, loss=16.637934, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7537/8562 [30:37<04:05,  4.17it/s, loss=16.637934, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7537/8562 [30:38<04:05,  4.17it/s, loss=16.637998, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7538/8562 [30:38<04:04,  4.19it/s, loss=16.637998, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7538/8562 [30:38<04:04,  4.19it/s, loss=16.638026, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7539/8562 [30:38<04:02,  4.21it/s, loss=16.638026, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7539/8562 [30:38<04:02,  4.21it/s, loss=16.638446, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7540/8562 [30:38<04:03,  4.20it/s, loss=16.638446, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7540/8562 [30:38<04:03,  4.20it/s, loss=16.638334, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7541/8562 [30:38<04:01,  4.23it/s, loss=16.638334, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7541/8562 [30:39<04:01,  4.23it/s, loss=16.638056, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7542/8562 [30:39<04:00,  4.24it/s, loss=16.638056, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7542/8562 [30:39<04:00,  4.24it/s, loss=16.637776, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7543/8562 [30:39<03:59,  4.25it/s, loss=16.637776, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7543/8562 [30:39<03:59,  4.25it/s, loss=16.637833, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7544/8562 [30:39<03:58,  4.27it/s, loss=16.637833, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7544/8562 [30:39<03:58,  4.27it/s, loss=16.637210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7545/8562 [30:39<03:59,  4.25it/s, loss=16.637210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7545/8562 [30:39<03:59,  4.25it/s, loss=16.637595, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7546/8562 [30:39<04:00,  4.22it/s, loss=16.637595, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7546/8562 [30:40<04:00,  4.22it/s, loss=16.637855, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7547/8562 [30:40<04:00,  4.21it/s, loss=16.637855, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7547/8562 [30:40<04:00,  4.21it/s, loss=16.638014, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7548/8562 [30:40<03:59,  4.23it/s, loss=16.638014, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7548/8562 [30:40<03:59,  4.23it/s, loss=16.637270, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7549/8562 [30:40<04:00,  4.20it/s, loss=16.637270, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7549/8562 [30:40<04:00,  4.20it/s, loss=16.637018, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7550/8562 [30:40<04:00,  4.21it/s, loss=16.637018, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7550/8562 [30:41<04:00,  4.21it/s, loss=16.637068, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7551/8562 [30:41<03:58,  4.24it/s, loss=16.637068, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7551/8562 [30:41<03:58,  4.24it/s, loss=16.636907, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7552/8562 [30:41<03:58,  4.24it/s, loss=16.636907, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7552/8562 [30:41<03:58,  4.24it/s, loss=16.637617, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7553/8562 [30:41<03:58,  4.23it/s, loss=16.637617, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7553/8562 [30:41<03:58,  4.23it/s, loss=16.637596, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7554/8562 [30:41<03:57,  4.24it/s, loss=16.637596, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7554/8562 [30:42<03:57,  4.24it/s, loss=16.637541, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7555/8562 [30:42<03:56,  4.26it/s, loss=16.637541, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7555/8562 [30:42<03:56,  4.26it/s, loss=16.637320, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7556/8562 [30:42<03:55,  4.28it/s, loss=16.637320, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7556/8562 [30:42<03:55,  4.28it/s, loss=16.637789, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7557/8562 [30:42<03:55,  4.27it/s, loss=16.637789, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7557/8562 [30:42<03:55,  4.27it/s, loss=16.638115, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7558/8562 [30:42<03:55,  4.26it/s, loss=16.638115, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7558/8562 [30:43<03:55,  4.26it/s, loss=16.637644, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7559/8562 [30:43<03:57,  4.23it/s, loss=16.637644, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7559/8562 [30:43<03:57,  4.23it/s, loss=16.637011, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7560/8562 [30:43<03:56,  4.23it/s, loss=16.637011, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7560/8562 [30:43<03:56,  4.23it/s, loss=16.636866, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7561/8562 [30:43<04:02,  4.13it/s, loss=16.636866, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7561/8562 [30:43<04:02,  4.13it/s, loss=16.636680, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7562/8562 [30:43<04:00,  4.16it/s, loss=16.636680, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7562/8562 [30:44<04:00,  4.16it/s, loss=16.636176, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7563/8562 [30:44<04:03,  4.11it/s, loss=16.636176, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7563/8562 [30:44<04:03,  4.11it/s, loss=16.636191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7564/8562 [30:44<04:01,  4.13it/s, loss=16.636191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7564/8562 [30:44<04:01,  4.13it/s, loss=16.636129, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7565/8562 [30:44<04:00,  4.15it/s, loss=16.636129, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7565/8562 [30:44<04:00,  4.15it/s, loss=16.635460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7566/8562 [30:44<04:04,  4.07it/s, loss=16.635460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7566/8562 [30:44<04:04,  4.07it/s, loss=16.635729, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7567/8562 [30:45<04:02,  4.11it/s, loss=16.635729, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7567/8562 [30:45<04:02,  4.11it/s, loss=16.636169, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7568/8562 [30:45<03:59,  4.15it/s, loss=16.636169, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7568/8562 [30:45<03:59,  4.15it/s, loss=16.635559, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7569/8562 [30:45<04:01,  4.11it/s, loss=16.635559, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7569/8562 [30:45<04:01,  4.11it/s, loss=16.635813, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7570/8562 [30:45<03:59,  4.14it/s, loss=16.635813, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7570/8562 [30:45<03:59,  4.14it/s, loss=16.635315, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7571/8562 [30:45<03:57,  4.17it/s, loss=16.635315, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7571/8562 [30:46<03:57,  4.17it/s, loss=16.635614, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7572/8562 [30:46<03:56,  4.19it/s, loss=16.635614, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7572/8562 [30:46<03:56,  4.19it/s, loss=16.635034, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7573/8562 [30:46<03:56,  4.18it/s, loss=16.635034, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7573/8562 [30:46<03:56,  4.18it/s, loss=16.634786, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7574/8562 [30:46<03:53,  4.23it/s, loss=16.634786, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7574/8562 [30:46<03:53,  4.23it/s, loss=16.635393, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7575/8562 [30:46<03:53,  4.22it/s, loss=16.635393, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7575/8562 [30:47<03:53,  4.22it/s, loss=16.635297, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7576/8562 [30:47<03:51,  4.25it/s, loss=16.635297, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7576/8562 [30:47<03:51,  4.25it/s, loss=16.635528, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7577/8562 [30:47<03:50,  4.27it/s, loss=16.635528, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  88%|████████▊ | 7577/8562 [30:47<03:50,  4.27it/s, loss=16.635358, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7578/8562 [30:47<03:49,  4.29it/s, loss=16.635358, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7578/8562 [30:47<03:49,  4.29it/s, loss=16.635594, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7579/8562 [30:47<03:50,  4.26it/s, loss=16.635594, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7579/8562 [30:48<03:50,  4.26it/s, loss=16.635499, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7580/8562 [30:48<03:51,  4.25it/s, loss=16.635499, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7580/8562 [30:48<03:51,  4.25it/s, loss=16.635562, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7581/8562 [30:48<03:50,  4.26it/s, loss=16.635562, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7581/8562 [30:48<03:50,  4.26it/s, loss=16.635548, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7582/8562 [30:48<03:50,  4.26it/s, loss=16.635548, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7582/8562 [30:48<03:50,  4.26it/s, loss=16.634992, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7583/8562 [30:48<03:50,  4.24it/s, loss=16.634992, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7583/8562 [30:49<03:50,  4.24it/s, loss=16.634879, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7584/8562 [30:49<03:51,  4.22it/s, loss=16.634879, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7584/8562 [30:49<03:51,  4.22it/s, loss=16.634934, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7585/8562 [30:49<03:50,  4.24it/s, loss=16.634934, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7585/8562 [30:49<03:50,  4.24it/s, loss=16.633650, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7586/8562 [30:49<03:51,  4.21it/s, loss=16.633650, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7586/8562 [30:49<03:51,  4.21it/s, loss=16.634120, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7587/8562 [30:49<03:51,  4.22it/s, loss=16.634120, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7587/8562 [30:49<03:51,  4.22it/s, loss=16.634280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7588/8562 [30:49<03:49,  4.24it/s, loss=16.634280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7588/8562 [30:50<03:49,  4.24it/s, loss=16.634728, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7589/8562 [30:50<03:49,  4.24it/s, loss=16.634728, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7589/8562 [30:50<03:49,  4.24it/s, loss=16.634796, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7590/8562 [30:50<03:50,  4.22it/s, loss=16.634796, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7590/8562 [30:50<03:50,  4.22it/s, loss=16.634377, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7591/8562 [30:50<03:53,  4.16it/s, loss=16.634377, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7591/8562 [30:50<03:53,  4.16it/s, loss=16.633876, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7592/8562 [30:50<03:52,  4.17it/s, loss=16.633876, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7592/8562 [30:51<03:52,  4.17it/s, loss=16.634026, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7593/8562 [30:51<03:50,  4.20it/s, loss=16.634026, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7593/8562 [30:51<03:50,  4.20it/s, loss=16.634411, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7594/8562 [30:51<03:49,  4.22it/s, loss=16.634411, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7594/8562 [30:51<03:49,  4.22it/s, loss=16.634418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7595/8562 [30:51<03:48,  4.23it/s, loss=16.634418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7595/8562 [30:51<03:48,  4.23it/s, loss=16.634553, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7596/8562 [30:51<03:46,  4.26it/s, loss=16.634553, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7596/8562 [30:52<03:46,  4.26it/s, loss=16.635197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7597/8562 [30:52<03:45,  4.27it/s, loss=16.635197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7597/8562 [30:52<03:45,  4.27it/s, loss=16.635502, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7598/8562 [30:52<03:46,  4.25it/s, loss=16.635502, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▊ | 7598/8562 [30:52<03:46,  4.25it/s, loss=16.635134, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7599/8562 [30:52<03:46,  4.26it/s, loss=16.635134, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7599/8562 [30:52<03:46,  4.26it/s, loss=16.635253, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7600/8562 [30:52<03:46,  4.25it/s, loss=16.635253, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7600/8562 [30:53<03:46,  4.25it/s, loss=16.635169, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7601/8562 [30:53<03:45,  4.26it/s, loss=16.635169, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7601/8562 [30:53<03:45,  4.26it/s, loss=16.635599, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7602/8562 [30:53<03:46,  4.25it/s, loss=16.635599, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7602/8562 [30:53<03:46,  4.25it/s, loss=16.634938, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7603/8562 [30:53<03:45,  4.24it/s, loss=16.634938, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7603/8562 [30:53<03:45,  4.24it/s, loss=16.635083, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7604/8562 [30:53<03:46,  4.24it/s, loss=16.635083, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7604/8562 [30:53<03:46,  4.24it/s, loss=16.635527, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7605/8562 [30:54<03:52,  4.12it/s, loss=16.635527, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7605/8562 [30:54<03:52,  4.12it/s, loss=16.635048, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7606/8562 [30:54<03:50,  4.15it/s, loss=16.635048, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7606/8562 [30:54<03:50,  4.15it/s, loss=16.635011, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7607/8562 [30:54<03:49,  4.15it/s, loss=16.635011, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7607/8562 [30:54<03:49,  4.15it/s, loss=16.634755, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7608/8562 [30:54<03:50,  4.13it/s, loss=16.634755, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7608/8562 [30:54<03:50,  4.13it/s, loss=16.633338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7609/8562 [30:54<03:52,  4.10it/s, loss=16.633338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7609/8562 [30:55<03:52,  4.10it/s, loss=16.633314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7610/8562 [30:55<03:50,  4.14it/s, loss=16.633314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7610/8562 [30:55<03:50,  4.14it/s, loss=16.633313, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7611/8562 [30:55<03:47,  4.17it/s, loss=16.633313, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7611/8562 [30:55<03:47,  4.17it/s, loss=16.633249, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7612/8562 [30:55<03:46,  4.19it/s, loss=16.633249, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7612/8562 [30:55<03:46,  4.19it/s, loss=16.633393, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7613/8562 [30:55<03:50,  4.12it/s, loss=16.633393, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7613/8562 [30:56<03:50,  4.12it/s, loss=16.633536, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7614/8562 [30:56<03:47,  4.16it/s, loss=16.633536, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7614/8562 [30:56<03:47,  4.16it/s, loss=16.633837, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7615/8562 [30:56<03:45,  4.19it/s, loss=16.633837, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7615/8562 [30:56<03:45,  4.19it/s, loss=16.633656, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7616/8562 [30:56<03:44,  4.21it/s, loss=16.633656, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7616/8562 [30:56<03:44,  4.21it/s, loss=16.633902, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7617/8562 [30:56<03:47,  4.14it/s, loss=16.633902, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7617/8562 [30:57<03:47,  4.14it/s, loss=16.633884, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7618/8562 [30:57<03:50,  4.10it/s, loss=16.633884, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7618/8562 [30:57<03:50,  4.10it/s, loss=16.632471, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7619/8562 [30:57<03:48,  4.13it/s, loss=16.632471, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7619/8562 [30:57<03:48,  4.13it/s, loss=16.632782, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7620/8562 [30:57<03:45,  4.18it/s, loss=16.632782, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7620/8562 [30:57<03:45,  4.18it/s, loss=16.632323, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7621/8562 [30:57<03:48,  4.12it/s, loss=16.632323, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7621/8562 [30:58<03:48,  4.12it/s, loss=16.631403, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7622/8562 [30:58<03:45,  4.16it/s, loss=16.631403, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7622/8562 [30:58<03:45,  4.16it/s, loss=16.631191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7623/8562 [30:58<03:44,  4.19it/s, loss=16.631191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7623/8562 [30:58<03:44,  4.19it/s, loss=16.631197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7624/8562 [30:58<03:42,  4.21it/s, loss=16.631197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7624/8562 [30:58<03:42,  4.21it/s, loss=16.631332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7625/8562 [30:58<03:42,  4.21it/s, loss=16.631332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7625/8562 [30:59<03:42,  4.21it/s, loss=16.631270, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7626/8562 [30:59<03:47,  4.11it/s, loss=16.631270, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7626/8562 [30:59<03:47,  4.11it/s, loss=16.631128, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7627/8562 [30:59<03:45,  4.15it/s, loss=16.631128, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7627/8562 [30:59<03:45,  4.15it/s, loss=16.631407, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7628/8562 [30:59<03:43,  4.17it/s, loss=16.631407, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7628/8562 [30:59<03:43,  4.17it/s, loss=16.630965, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7629/8562 [30:59<03:42,  4.20it/s, loss=16.630965, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7629/8562 [30:59<03:42,  4.20it/s, loss=16.630134, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7630/8562 [30:59<03:41,  4.22it/s, loss=16.630134, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7630/8562 [31:00<03:41,  4.22it/s, loss=16.630115, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7631/8562 [31:00<03:40,  4.23it/s, loss=16.630115, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7631/8562 [31:00<03:40,  4.23it/s, loss=16.629945, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7632/8562 [31:00<03:45,  4.13it/s, loss=16.629945, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7632/8562 [31:00<03:45,  4.13it/s, loss=16.630041, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7633/8562 [31:00<03:42,  4.18it/s, loss=16.630041, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7633/8562 [31:00<03:42,  4.18it/s, loss=16.629917, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7634/8562 [31:00<03:40,  4.20it/s, loss=16.629917, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7634/8562 [31:01<03:40,  4.20it/s, loss=16.629798, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7635/8562 [31:01<03:39,  4.23it/s, loss=16.629798, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7635/8562 [31:01<03:39,  4.23it/s, loss=16.630026, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7636/8562 [31:01<03:43,  4.15it/s, loss=16.630026, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7636/8562 [31:01<03:43,  4.15it/s, loss=16.629877, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7637/8562 [31:01<03:46,  4.09it/s, loss=16.629877, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7637/8562 [31:01<03:46,  4.09it/s, loss=16.629852, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7638/8562 [31:01<03:44,  4.12it/s, loss=16.629852, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7638/8562 [31:02<03:44,  4.12it/s, loss=16.629568, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7639/8562 [31:02<03:43,  4.14it/s, loss=16.629568, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7639/8562 [31:02<03:43,  4.14it/s, loss=16.630187, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7640/8562 [31:02<03:42,  4.15it/s, loss=16.630187, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7640/8562 [31:02<03:42,  4.15it/s, loss=16.629789, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7641/8562 [31:02<03:44,  4.11it/s, loss=16.629789, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7641/8562 [31:02<03:44,  4.11it/s, loss=16.629881, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7642/8562 [31:02<03:42,  4.14it/s, loss=16.629881, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7642/8562 [31:03<03:42,  4.14it/s, loss=16.630274, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7643/8562 [31:03<03:43,  4.11it/s, loss=16.630274, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7643/8562 [31:03<03:43,  4.11it/s, loss=16.630473, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7644/8562 [31:03<03:40,  4.16it/s, loss=16.630473, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7644/8562 [31:03<03:40,  4.16it/s, loss=16.630236, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7645/8562 [31:03<03:38,  4.19it/s, loss=16.630236, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7645/8562 [31:03<03:38,  4.19it/s, loss=16.630479, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7646/8562 [31:03<03:40,  4.16it/s, loss=16.630479, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7646/8562 [31:04<03:40,  4.16it/s, loss=16.630485, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7647/8562 [31:04<03:38,  4.19it/s, loss=16.630485, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7647/8562 [31:04<03:38,  4.19it/s, loss=16.630632, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7648/8562 [31:04<03:38,  4.17it/s, loss=16.630632, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7648/8562 [31:04<03:38,  4.17it/s, loss=16.630681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7649/8562 [31:04<03:37,  4.20it/s, loss=16.630681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7649/8562 [31:04<03:37,  4.20it/s, loss=16.631155, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7650/8562 [31:04<03:35,  4.23it/s, loss=16.631155, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7650/8562 [31:05<03:35,  4.23it/s, loss=16.631297, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7651/8562 [31:05<03:35,  4.24it/s, loss=16.631297, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7651/8562 [31:05<03:35,  4.24it/s, loss=16.631069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7652/8562 [31:05<03:34,  4.25it/s, loss=16.631069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7652/8562 [31:05<03:34,  4.25it/s, loss=16.631274, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7653/8562 [31:05<03:32,  4.28it/s, loss=16.631274, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7653/8562 [31:05<03:32,  4.28it/s, loss=16.631386, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7654/8562 [31:05<03:32,  4.28it/s, loss=16.631386, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7654/8562 [31:05<03:32,  4.28it/s, loss=16.631409, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7655/8562 [31:05<03:31,  4.28it/s, loss=16.631409, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7655/8562 [31:06<03:31,  4.28it/s, loss=16.631494, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7656/8562 [31:06<03:33,  4.25it/s, loss=16.631494, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7656/8562 [31:06<03:33,  4.25it/s, loss=16.631895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7657/8562 [31:06<03:32,  4.25it/s, loss=16.631895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7657/8562 [31:06<03:32,  4.25it/s, loss=16.631555, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7658/8562 [31:06<03:33,  4.24it/s, loss=16.631555, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7658/8562 [31:06<03:33,  4.24it/s, loss=16.631030, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7659/8562 [31:06<03:33,  4.24it/s, loss=16.631030, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7659/8562 [31:07<03:33,  4.24it/s, loss=16.631407, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7660/8562 [31:07<03:34,  4.20it/s, loss=16.631407, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7660/8562 [31:07<03:34,  4.20it/s, loss=16.631590, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7661/8562 [31:07<03:33,  4.22it/s, loss=16.631590, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7661/8562 [31:07<03:33,  4.22it/s, loss=16.632001, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7662/8562 [31:07<03:32,  4.24it/s, loss=16.632001, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  89%|████████▉ | 7662/8562 [31:07<03:32,  4.24it/s, loss=16.631757, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7663/8562 [31:07<03:32,  4.24it/s, loss=16.631757, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7663/8562 [31:08<03:32,  4.24it/s, loss=16.631689, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7664/8562 [31:08<03:32,  4.23it/s, loss=16.631689, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7664/8562 [31:08<03:32,  4.23it/s, loss=16.631624, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7665/8562 [31:08<03:32,  4.23it/s, loss=16.631624, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7665/8562 [31:08<03:32,  4.23it/s, loss=16.631832, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7666/8562 [31:08<03:31,  4.24it/s, loss=16.631832, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7666/8562 [31:08<03:31,  4.24it/s, loss=16.631828, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7667/8562 [31:08<03:30,  4.25it/s, loss=16.631828, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7667/8562 [31:09<03:30,  4.25it/s, loss=16.632273, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7668/8562 [31:09<03:29,  4.27it/s, loss=16.632273, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7668/8562 [31:09<03:29,  4.27it/s, loss=16.632604, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7669/8562 [31:09<03:29,  4.26it/s, loss=16.632604, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7669/8562 [31:09<03:29,  4.26it/s, loss=16.632196, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7670/8562 [31:09<03:32,  4.20it/s, loss=16.632196, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7670/8562 [31:09<03:32,  4.20it/s, loss=16.631806, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7671/8562 [31:09<03:32,  4.20it/s, loss=16.631806, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7671/8562 [31:09<03:32,  4.20it/s, loss=16.631588, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7672/8562 [31:09<03:32,  4.18it/s, loss=16.631588, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7672/8562 [31:10<03:32,  4.18it/s, loss=16.630878, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7673/8562 [31:10<03:36,  4.10it/s, loss=16.630878, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7673/8562 [31:10<03:36,  4.10it/s, loss=16.631417, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7674/8562 [31:10<03:35,  4.12it/s, loss=16.631417, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7674/8562 [31:10<03:35,  4.12it/s, loss=16.631096, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7675/8562 [31:10<03:38,  4.07it/s, loss=16.631096, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7675/8562 [31:10<03:38,  4.07it/s, loss=16.631183, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7676/8562 [31:10<03:35,  4.11it/s, loss=16.631183, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7676/8562 [31:11<03:35,  4.11it/s, loss=16.630714, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7677/8562 [31:11<03:34,  4.13it/s, loss=16.630714, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7677/8562 [31:11<03:34,  4.13it/s, loss=16.631185, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7678/8562 [31:11<03:32,  4.16it/s, loss=16.631185, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7678/8562 [31:11<03:32,  4.16it/s, loss=16.631452, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7679/8562 [31:11<03:32,  4.16it/s, loss=16.631452, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7679/8562 [31:11<03:32,  4.16it/s, loss=16.630440, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7680/8562 [31:11<03:30,  4.18it/s, loss=16.630440, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7680/8562 [31:12<03:30,  4.18it/s, loss=16.630875, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7681/8562 [31:12<03:33,  4.12it/s, loss=16.630875, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7681/8562 [31:12<03:33,  4.12it/s, loss=16.631014, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7682/8562 [31:12<03:31,  4.16it/s, loss=16.631014, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7682/8562 [31:12<03:31,  4.16it/s, loss=16.631460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7683/8562 [31:12<03:30,  4.17it/s, loss=16.631460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7683/8562 [31:12<03:30,  4.17it/s, loss=16.631732, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7684/8562 [31:12<03:30,  4.18it/s, loss=16.631732, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7684/8562 [31:13<03:30,  4.18it/s, loss=16.631503, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7685/8562 [31:13<03:34,  4.08it/s, loss=16.631503, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7685/8562 [31:13<03:34,  4.08it/s, loss=16.631344, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7686/8562 [31:13<03:32,  4.13it/s, loss=16.631344, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7686/8562 [31:13<03:32,  4.13it/s, loss=16.631291, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7687/8562 [31:13<03:31,  4.14it/s, loss=16.631291, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7687/8562 [31:13<03:31,  4.14it/s, loss=16.631465, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7688/8562 [31:13<03:30,  4.15it/s, loss=16.631465, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7688/8562 [31:14<03:30,  4.15it/s, loss=16.631193, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7689/8562 [31:14<03:29,  4.17it/s, loss=16.631193, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7689/8562 [31:14<03:29,  4.17it/s, loss=16.630803, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7690/8562 [31:14<03:27,  4.19it/s, loss=16.630803, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7690/8562 [31:14<03:27,  4.19it/s, loss=16.631246, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7691/8562 [31:14<03:26,  4.21it/s, loss=16.631246, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7691/8562 [31:14<03:26,  4.21it/s, loss=16.630494, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7692/8562 [31:14<03:25,  4.22it/s, loss=16.630494, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7692/8562 [31:15<03:25,  4.22it/s, loss=16.630572, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7693/8562 [31:15<03:29,  4.15it/s, loss=16.630572, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7693/8562 [31:15<03:29,  4.15it/s, loss=16.630834, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7694/8562 [31:15<03:28,  4.16it/s, loss=16.630834, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7694/8562 [31:15<03:28,  4.16it/s, loss=16.631322, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7695/8562 [31:15<03:26,  4.21it/s, loss=16.631322, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7695/8562 [31:15<03:26,  4.21it/s, loss=16.630883, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7696/8562 [31:15<03:31,  4.10it/s, loss=16.630883, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7696/8562 [31:16<03:31,  4.10it/s, loss=16.631215, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7697/8562 [31:16<03:29,  4.13it/s, loss=16.631215, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7697/8562 [31:16<03:29,  4.13it/s, loss=16.630818, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7698/8562 [31:16<03:33,  4.04it/s, loss=16.630818, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7698/8562 [31:16<03:33,  4.04it/s, loss=16.631121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7699/8562 [31:16<03:52,  3.71it/s, loss=16.631121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7699/8562 [31:16<03:52,  3.71it/s, loss=16.631456, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7700/8562 [31:16<03:52,  3.71it/s, loss=16.631456, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7700/8562 [31:17<03:52,  3.71it/s, loss=16.630761, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7701/8562 [31:17<03:50,  3.73it/s, loss=16.630761, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7701/8562 [31:17<03:50,  3.73it/s, loss=16.630675, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7702/8562 [31:17<04:22,  3.28it/s, loss=16.630675, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7702/8562 [31:17<04:22,  3.28it/s, loss=16.630903, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7703/8562 [31:17<04:17,  3.34it/s, loss=16.630903, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7703/8562 [31:18<04:17,  3.34it/s, loss=16.630977, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7704/8562 [31:18<04:41,  3.05it/s, loss=16.630977, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7704/8562 [31:18<04:41,  3.05it/s, loss=16.631044, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7705/8562 [31:18<04:48,  2.97it/s, loss=16.631044, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|████████▉ | 7705/8562 [31:18<04:48,  2.97it/s, loss=16.630237, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7706/8562 [31:18<05:05,  2.80it/s, loss=16.630237, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7706/8562 [31:19<05:05,  2.80it/s, loss=16.629502, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7707/8562 [31:19<04:53,  2.91it/s, loss=16.629502, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7707/8562 [31:19<04:53,  2.91it/s, loss=16.629327, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7708/8562 [31:19<04:51,  2.93it/s, loss=16.629327, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7708/8562 [31:20<04:51,  2.93it/s, loss=16.629757, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7709/8562 [31:20<05:01,  2.83it/s, loss=16.629757, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7709/8562 [31:20<05:01,  2.83it/s, loss=16.629904, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7710/8562 [31:20<05:10,  2.74it/s, loss=16.629904, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7710/8562 [31:20<05:10,  2.74it/s, loss=16.630135, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7711/8562 [31:20<05:04,  2.79it/s, loss=16.630135, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7711/8562 [31:21<05:04,  2.79it/s, loss=16.630653, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7712/8562 [31:21<05:06,  2.77it/s, loss=16.630653, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7712/8562 [31:21<05:06,  2.77it/s, loss=16.630979, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7713/8562 [31:21<05:18,  2.66it/s, loss=16.630979, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7713/8562 [31:21<05:18,  2.66it/s, loss=16.630698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7714/8562 [31:21<05:04,  2.78it/s, loss=16.630698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7714/8562 [31:22<05:04,  2.78it/s, loss=16.629664, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7715/8562 [31:22<04:56,  2.85it/s, loss=16.629664, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7715/8562 [31:22<04:56,  2.85it/s, loss=16.630121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7716/8562 [31:22<04:53,  2.88it/s, loss=16.630121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7716/8562 [31:22<04:53,  2.88it/s, loss=16.630338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7717/8562 [31:22<04:58,  2.83it/s, loss=16.630338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7717/8562 [31:23<04:58,  2.83it/s, loss=16.630199, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7718/8562 [31:23<04:54,  2.86it/s, loss=16.630199, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7718/8562 [31:23<04:54,  2.86it/s, loss=16.630799, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7719/8562 [31:23<04:50,  2.90it/s, loss=16.630799, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7719/8562 [31:23<04:50,  2.90it/s, loss=16.631191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7720/8562 [31:23<04:49,  2.91it/s, loss=16.631191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7720/8562 [31:24<04:49,  2.91it/s, loss=16.630746, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7721/8562 [31:24<04:43,  2.97it/s, loss=16.630746, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7721/8562 [31:24<04:43,  2.97it/s, loss=16.630866, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7722/8562 [31:24<04:17,  3.26it/s, loss=16.630866, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7722/8562 [31:24<04:17,  3.26it/s, loss=16.630783, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7723/8562 [31:24<04:01,  3.48it/s, loss=16.630783, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7723/8562 [31:24<04:01,  3.48it/s, loss=16.630269, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7724/8562 [31:24<03:48,  3.67it/s, loss=16.630269, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7724/8562 [31:25<03:48,  3.67it/s, loss=16.630679, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7725/8562 [31:25<03:40,  3.79it/s, loss=16.630679, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7725/8562 [31:25<03:40,  3.79it/s, loss=16.631006, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7726/8562 [31:25<03:34,  3.89it/s, loss=16.631006, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7726/8562 [31:25<03:34,  3.89it/s, loss=16.630847, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7727/8562 [31:25<03:32,  3.93it/s, loss=16.630847, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7727/8562 [31:25<03:32,  3.93it/s, loss=16.630681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7728/8562 [31:25<03:31,  3.95it/s, loss=16.630681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7728/8562 [31:26<03:31,  3.95it/s, loss=16.630768, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7729/8562 [31:26<03:28,  3.99it/s, loss=16.630768, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7729/8562 [31:26<03:28,  3.99it/s, loss=16.630684, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7730/8562 [31:26<03:25,  4.06it/s, loss=16.630684, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7730/8562 [31:26<03:25,  4.06it/s, loss=16.630862, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7731/8562 [31:26<03:23,  4.09it/s, loss=16.630862, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7731/8562 [31:26<03:23,  4.09it/s, loss=16.631261, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7732/8562 [31:26<03:25,  4.05it/s, loss=16.631261, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7732/8562 [31:27<03:25,  4.05it/s, loss=16.631697, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7733/8562 [31:27<03:24,  4.06it/s, loss=16.631697, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7733/8562 [31:27<03:24,  4.06it/s, loss=16.632196, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7734/8562 [31:27<03:22,  4.09it/s, loss=16.632196, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7734/8562 [31:27<03:22,  4.09it/s, loss=16.632635, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7735/8562 [31:27<03:22,  4.08it/s, loss=16.632635, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7735/8562 [31:27<03:22,  4.08it/s, loss=16.632752, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7736/8562 [31:27<03:19,  4.14it/s, loss=16.632752, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7736/8562 [31:28<03:19,  4.14it/s, loss=16.633085, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7737/8562 [31:28<03:19,  4.14it/s, loss=16.633085, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7737/8562 [31:28<03:19,  4.14it/s, loss=16.632663, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7738/8562 [31:28<03:22,  4.07it/s, loss=16.632663, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7738/8562 [31:28<03:22,  4.07it/s, loss=16.633283, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7739/8562 [31:28<03:19,  4.13it/s, loss=16.633283, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7739/8562 [31:28<03:19,  4.13it/s, loss=16.634070, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7740/8562 [31:28<03:20,  4.10it/s, loss=16.634070, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7740/8562 [31:29<03:20,  4.10it/s, loss=16.633975, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7741/8562 [31:29<03:18,  4.14it/s, loss=16.633975, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7741/8562 [31:29<03:18,  4.14it/s, loss=16.634152, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7742/8562 [31:29<03:17,  4.16it/s, loss=16.634152, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7742/8562 [31:29<03:17,  4.16it/s, loss=16.634441, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7743/8562 [31:29<03:15,  4.19it/s, loss=16.634441, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7743/8562 [31:29<03:15,  4.19it/s, loss=16.634748, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7744/8562 [31:29<03:14,  4.21it/s, loss=16.634748, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7744/8562 [31:30<03:14,  4.21it/s, loss=16.635077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7745/8562 [31:30<03:13,  4.22it/s, loss=16.635077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7745/8562 [31:30<03:13,  4.22it/s, loss=16.635145, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7746/8562 [31:30<03:13,  4.23it/s, loss=16.635145, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7746/8562 [31:30<03:13,  4.23it/s, loss=16.635130, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7747/8562 [31:30<03:12,  4.24it/s, loss=16.635130, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7747/8562 [31:30<03:12,  4.24it/s, loss=16.635222, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7748/8562 [31:30<03:14,  4.19it/s, loss=16.635222, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  90%|█████████ | 7748/8562 [31:30<03:14,  4.19it/s, loss=16.635561, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7749/8562 [31:30<03:13,  4.20it/s, loss=16.635561, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7749/8562 [31:31<03:13,  4.20it/s, loss=16.635319, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7750/8562 [31:31<03:14,  4.19it/s, loss=16.635319, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7750/8562 [31:31<03:14,  4.19it/s, loss=16.635418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7751/8562 [31:31<03:13,  4.20it/s, loss=16.635418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7751/8562 [31:31<03:13,  4.20it/s, loss=16.635113, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7752/8562 [31:31<03:12,  4.21it/s, loss=16.635113, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7752/8562 [31:31<03:12,  4.21it/s, loss=16.635148, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7753/8562 [31:31<03:14,  4.17it/s, loss=16.635148, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7753/8562 [31:32<03:14,  4.17it/s, loss=16.635681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7754/8562 [31:32<03:17,  4.09it/s, loss=16.635681, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7754/8562 [31:32<03:17,  4.09it/s, loss=16.635789, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7755/8562 [31:32<03:15,  4.13it/s, loss=16.635789, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7755/8562 [31:32<03:15,  4.13it/s, loss=16.635955, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7756/8562 [31:32<03:15,  4.13it/s, loss=16.635955, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7756/8562 [31:32<03:15,  4.13it/s, loss=16.636405, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7757/8562 [31:32<03:15,  4.11it/s, loss=16.636405, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7757/8562 [31:33<03:15,  4.11it/s, loss=16.636468, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7758/8562 [31:33<03:14,  4.14it/s, loss=16.636468, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7758/8562 [31:33<03:14,  4.14it/s, loss=16.637011, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7759/8562 [31:33<03:13,  4.15it/s, loss=16.637011, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7759/8562 [31:33<03:13,  4.15it/s, loss=16.637133, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7760/8562 [31:33<03:12,  4.16it/s, loss=16.637133, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7760/8562 [31:33<03:12,  4.16it/s, loss=16.636950, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7761/8562 [31:33<03:14,  4.11it/s, loss=16.636950, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7761/8562 [31:34<03:14,  4.11it/s, loss=16.636737, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7762/8562 [31:34<03:13,  4.13it/s, loss=16.636737, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7762/8562 [31:34<03:13,  4.13it/s, loss=16.637211, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7763/8562 [31:34<03:11,  4.18it/s, loss=16.637211, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7763/8562 [31:34<03:11,  4.18it/s, loss=16.636814, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7764/8562 [31:34<03:11,  4.16it/s, loss=16.636814, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7764/8562 [31:34<03:11,  4.16it/s, loss=16.637377, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7765/8562 [31:34<03:10,  4.19it/s, loss=16.637377, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7765/8562 [31:35<03:10,  4.19it/s, loss=16.637729, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7766/8562 [31:35<03:09,  4.21it/s, loss=16.637729, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7766/8562 [31:35<03:09,  4.21it/s, loss=16.637576, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7767/8562 [31:35<03:08,  4.22it/s, loss=16.637576, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7767/8562 [31:35<03:08,  4.22it/s, loss=16.637933, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7768/8562 [31:35<03:08,  4.22it/s, loss=16.637933, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7768/8562 [31:35<03:08,  4.22it/s, loss=16.638274, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7769/8562 [31:35<03:06,  4.25it/s, loss=16.638274, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7769/8562 [31:35<03:06,  4.25it/s, loss=16.638013, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7770/8562 [31:35<03:06,  4.24it/s, loss=16.638013, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7770/8562 [31:36<03:06,  4.24it/s, loss=16.638442, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7771/8562 [31:36<03:08,  4.20it/s, loss=16.638442, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7771/8562 [31:36<03:08,  4.20it/s, loss=16.638212, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7772/8562 [31:36<03:07,  4.22it/s, loss=16.638212, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7772/8562 [31:36<03:07,  4.22it/s, loss=16.638351, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7773/8562 [31:36<03:09,  4.16it/s, loss=16.638351, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7773/8562 [31:36<03:09,  4.16it/s, loss=16.637793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7774/8562 [31:36<03:08,  4.19it/s, loss=16.637793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7774/8562 [31:37<03:08,  4.19it/s, loss=16.637373, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7775/8562 [31:37<03:09,  4.16it/s, loss=16.637373, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7775/8562 [31:37<03:09,  4.16it/s, loss=16.637900, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7776/8562 [31:37<03:07,  4.19it/s, loss=16.637900, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7776/8562 [31:37<03:07,  4.19it/s, loss=16.638367, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7777/8562 [31:37<03:23,  3.86it/s, loss=16.638367, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7777/8562 [31:37<03:23,  3.86it/s, loss=16.639056, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7778/8562 [31:37<03:18,  3.94it/s, loss=16.639056, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7778/8562 [31:38<03:18,  3.94it/s, loss=16.638981, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7779/8562 [31:38<03:16,  3.99it/s, loss=16.638981, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7779/8562 [31:38<03:16,  3.99it/s, loss=16.638889, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7780/8562 [31:38<03:12,  4.07it/s, loss=16.638889, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7780/8562 [31:38<03:12,  4.07it/s, loss=16.638503, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7781/8562 [31:38<03:08,  4.14it/s, loss=16.638503, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7781/8562 [31:38<03:08,  4.14it/s, loss=16.638998, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7782/8562 [31:38<03:07,  4.17it/s, loss=16.638998, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7782/8562 [31:39<03:07,  4.17it/s, loss=16.638708, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7783/8562 [31:39<03:05,  4.20it/s, loss=16.638708, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7783/8562 [31:39<03:05,  4.20it/s, loss=16.638515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7784/8562 [31:39<03:04,  4.21it/s, loss=16.638515, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7784/8562 [31:39<03:04,  4.21it/s, loss=16.638365, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7785/8562 [31:39<03:08,  4.13it/s, loss=16.638365, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7785/8562 [31:39<03:08,  4.13it/s, loss=16.637881, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7786/8562 [31:39<03:07,  4.14it/s, loss=16.637881, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7786/8562 [31:40<03:07,  4.14it/s, loss=16.638046, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7787/8562 [31:40<03:05,  4.18it/s, loss=16.638046, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7787/8562 [31:40<03:05,  4.18it/s, loss=16.638126, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7788/8562 [31:40<03:03,  4.21it/s, loss=16.638126, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7788/8562 [31:40<03:03,  4.21it/s, loss=16.637761, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7789/8562 [31:40<03:02,  4.23it/s, loss=16.637761, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7789/8562 [31:40<03:02,  4.23it/s, loss=16.637736, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7790/8562 [31:40<03:04,  4.20it/s, loss=16.637736, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7790/8562 [31:41<03:04,  4.20it/s, loss=16.637241, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7791/8562 [31:41<03:03,  4.19it/s, loss=16.637241, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7791/8562 [31:41<03:03,  4.19it/s, loss=16.636457, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7792/8562 [31:41<03:02,  4.21it/s, loss=16.636457, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7792/8562 [31:41<03:02,  4.21it/s, loss=16.636657, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7793/8562 [31:41<03:03,  4.20it/s, loss=16.636657, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7793/8562 [31:41<03:03,  4.20it/s, loss=16.636456, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7794/8562 [31:41<03:01,  4.22it/s, loss=16.636456, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7794/8562 [31:42<03:01,  4.22it/s, loss=16.636185, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7795/8562 [31:42<03:01,  4.23it/s, loss=16.636185, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7795/8562 [31:42<03:01,  4.23it/s, loss=16.636607, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7796/8562 [31:42<03:00,  4.25it/s, loss=16.636607, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7796/8562 [31:42<03:00,  4.25it/s, loss=16.637006, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7797/8562 [31:42<03:01,  4.20it/s, loss=16.637006, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7797/8562 [31:42<03:01,  4.20it/s, loss=16.636951, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7798/8562 [31:42<03:01,  4.22it/s, loss=16.636951, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7798/8562 [31:42<03:01,  4.22it/s, loss=16.635712, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7799/8562 [31:42<03:00,  4.24it/s, loss=16.635712, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7799/8562 [31:43<03:00,  4.24it/s, loss=16.635231, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7800/8562 [31:43<02:59,  4.25it/s, loss=16.635231, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7800/8562 [31:43<02:59,  4.25it/s, loss=16.635461, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7801/8562 [31:43<02:59,  4.23it/s, loss=16.635461, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7801/8562 [31:43<02:59,  4.23it/s, loss=16.635338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7802/8562 [31:43<02:59,  4.24it/s, loss=16.635338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7802/8562 [31:43<02:59,  4.24it/s, loss=16.635418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7803/8562 [31:43<02:59,  4.22it/s, loss=16.635418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7803/8562 [31:44<02:59,  4.22it/s, loss=16.635482, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7804/8562 [31:44<02:59,  4.22it/s, loss=16.635482, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7804/8562 [31:44<02:59,  4.22it/s, loss=16.634989, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7805/8562 [31:44<02:58,  4.25it/s, loss=16.634989, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7805/8562 [31:44<02:58,  4.25it/s, loss=16.635167, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7806/8562 [31:44<02:58,  4.24it/s, loss=16.635167, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7806/8562 [31:44<02:58,  4.24it/s, loss=16.635576, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7807/8562 [31:44<02:58,  4.22it/s, loss=16.635576, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7807/8562 [31:45<02:58,  4.22it/s, loss=16.635455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7808/8562 [31:45<02:57,  4.24it/s, loss=16.635455, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7808/8562 [31:45<02:57,  4.24it/s, loss=16.634588, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7809/8562 [31:45<02:57,  4.25it/s, loss=16.634588, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7809/8562 [31:45<02:57,  4.25it/s, loss=16.634317, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7810/8562 [31:45<02:56,  4.27it/s, loss=16.634317, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7810/8562 [31:45<02:56,  4.27it/s, loss=16.633824, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7811/8562 [31:45<02:56,  4.25it/s, loss=16.633824, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7811/8562 [31:46<02:56,  4.25it/s, loss=16.633937, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7812/8562 [31:46<02:56,  4.24it/s, loss=16.633937, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████ | 7812/8562 [31:46<02:56,  4.24it/s, loss=16.633901, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7813/8562 [31:46<02:56,  4.24it/s, loss=16.633901, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7813/8562 [31:46<02:56,  4.24it/s, loss=16.634339, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7814/8562 [31:46<02:59,  4.16it/s, loss=16.634339, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7814/8562 [31:46<02:59,  4.16it/s, loss=16.634728, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7815/8562 [31:46<02:58,  4.18it/s, loss=16.634728, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7815/8562 [31:46<02:58,  4.18it/s, loss=16.634840, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7816/8562 [31:46<02:58,  4.19it/s, loss=16.634840, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7816/8562 [31:47<02:58,  4.19it/s, loss=16.634627, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7817/8562 [31:47<02:59,  4.16it/s, loss=16.634627, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7817/8562 [31:47<02:59,  4.16it/s, loss=16.634582, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7818/8562 [31:47<02:57,  4.20it/s, loss=16.634582, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7818/8562 [31:47<02:57,  4.20it/s, loss=16.635191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7819/8562 [31:47<02:57,  4.19it/s, loss=16.635191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7819/8562 [31:47<02:57,  4.19it/s, loss=16.635127, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7820/8562 [31:47<03:02,  4.06it/s, loss=16.635127, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7820/8562 [31:48<03:02,  4.06it/s, loss=16.635406, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7821/8562 [31:48<03:01,  4.09it/s, loss=16.635406, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7821/8562 [31:48<03:01,  4.09it/s, loss=16.635484, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7822/8562 [31:48<02:59,  4.13it/s, loss=16.635484, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7822/8562 [31:48<02:59,  4.13it/s, loss=16.635543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7823/8562 [31:48<02:56,  4.18it/s, loss=16.635543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7823/8562 [31:48<02:56,  4.18it/s, loss=16.635051, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7824/8562 [31:48<02:57,  4.16it/s, loss=16.635051, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7824/8562 [31:49<02:57,  4.16it/s, loss=16.634864, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7825/8562 [31:49<02:55,  4.19it/s, loss=16.634864, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7825/8562 [31:49<02:55,  4.19it/s, loss=16.634341, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7826/8562 [31:49<02:55,  4.20it/s, loss=16.634341, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7826/8562 [31:49<02:55,  4.20it/s, loss=16.634109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7827/8562 [31:49<02:54,  4.21it/s, loss=16.634109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7827/8562 [31:49<02:54,  4.21it/s, loss=16.634298, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7828/8562 [31:49<02:53,  4.24it/s, loss=16.634298, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7828/8562 [31:50<02:53,  4.24it/s, loss=16.634361, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7829/8562 [31:50<02:53,  4.24it/s, loss=16.634361, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7829/8562 [31:50<02:53,  4.24it/s, loss=16.634544, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7830/8562 [31:50<02:53,  4.21it/s, loss=16.634544, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7830/8562 [31:50<02:53,  4.21it/s, loss=16.634475, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7831/8562 [31:50<02:54,  4.19it/s, loss=16.634475, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7831/8562 [31:50<02:54,  4.19it/s, loss=16.634764, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7832/8562 [31:50<02:56,  4.14it/s, loss=16.634764, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7832/8562 [31:51<02:56,  4.14it/s, loss=16.635094, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7833/8562 [31:51<02:54,  4.17it/s, loss=16.635094, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7833/8562 [31:51<02:54,  4.17it/s, loss=16.635459, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7834/8562 [31:51<02:55,  4.15it/s, loss=16.635459, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  91%|█████████▏| 7834/8562 [31:51<02:55,  4.15it/s, loss=16.635834, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7835/8562 [31:51<02:54,  4.17it/s, loss=16.635834, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7835/8562 [31:51<02:54,  4.17it/s, loss=16.635410, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7836/8562 [31:51<02:53,  4.19it/s, loss=16.635410, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7836/8562 [31:52<02:53,  4.19it/s, loss=16.635309, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7837/8562 [31:52<02:51,  4.22it/s, loss=16.635309, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7837/8562 [31:52<02:51,  4.22it/s, loss=16.635418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7838/8562 [31:52<02:50,  4.24it/s, loss=16.635418, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7838/8562 [31:52<02:50,  4.24it/s, loss=16.635329, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7839/8562 [31:52<02:51,  4.22it/s, loss=16.635329, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7839/8562 [31:52<02:51,  4.22it/s, loss=16.635197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7840/8562 [31:52<02:50,  4.24it/s, loss=16.635197, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7840/8562 [31:52<02:50,  4.24it/s, loss=16.634843, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7841/8562 [31:52<02:50,  4.24it/s, loss=16.634843, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7841/8562 [31:53<02:50,  4.24it/s, loss=16.634131, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7842/8562 [31:53<02:49,  4.25it/s, loss=16.634131, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7842/8562 [31:53<02:49,  4.25it/s, loss=16.634054, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7843/8562 [31:53<02:48,  4.27it/s, loss=16.634054, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7843/8562 [31:53<02:48,  4.27it/s, loss=16.634350, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7844/8562 [31:53<02:48,  4.26it/s, loss=16.634350, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7844/8562 [31:53<02:48,  4.26it/s, loss=16.634198, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7845/8562 [31:53<02:49,  4.23it/s, loss=16.634198, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7845/8562 [31:54<02:49,  4.23it/s, loss=16.634317, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7846/8562 [31:54<02:57,  4.04it/s, loss=16.634317, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7846/8562 [31:54<02:57,  4.04it/s, loss=16.634747, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7847/8562 [31:54<02:55,  4.08it/s, loss=16.634747, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7847/8562 [31:54<02:55,  4.08it/s, loss=16.634507, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7848/8562 [31:54<02:53,  4.10it/s, loss=16.634507, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7848/8562 [31:54<02:53,  4.10it/s, loss=16.634695, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7849/8562 [31:54<02:51,  4.16it/s, loss=16.634695, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7849/8562 [31:55<02:51,  4.16it/s, loss=16.635116, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7850/8562 [31:55<02:50,  4.18it/s, loss=16.635116, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7850/8562 [31:55<02:50,  4.18it/s, loss=16.634587, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7851/8562 [31:55<02:49,  4.19it/s, loss=16.634587, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7851/8562 [31:55<02:49,  4.19it/s, loss=16.634541, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7852/8562 [31:55<02:48,  4.22it/s, loss=16.634541, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7852/8562 [31:55<02:48,  4.22it/s, loss=16.634587, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7853/8562 [31:55<02:47,  4.23it/s, loss=16.634587, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7853/8562 [31:56<02:47,  4.23it/s, loss=16.634955, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7854/8562 [31:56<02:48,  4.20it/s, loss=16.634955, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7854/8562 [31:56<02:48,  4.20it/s, loss=16.635095, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7855/8562 [31:56<02:49,  4.17it/s, loss=16.635095, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7855/8562 [31:56<02:49,  4.17it/s, loss=16.635281, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7856/8562 [31:56<02:50,  4.13it/s, loss=16.635281, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7856/8562 [31:56<02:50,  4.13it/s, loss=16.635008, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7857/8562 [31:56<02:49,  4.16it/s, loss=16.635008, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7857/8562 [31:57<02:49,  4.16it/s, loss=16.635220, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7858/8562 [31:57<02:47,  4.21it/s, loss=16.635220, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7858/8562 [31:57<02:47,  4.21it/s, loss=16.634270, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7859/8562 [31:57<02:46,  4.23it/s, loss=16.634270, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7859/8562 [31:57<02:46,  4.23it/s, loss=16.633843, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7860/8562 [31:57<02:45,  4.24it/s, loss=16.633843, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7860/8562 [31:57<02:45,  4.24it/s, loss=16.633557, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7861/8562 [31:57<02:45,  4.24it/s, loss=16.633557, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7861/8562 [31:57<02:45,  4.24it/s, loss=16.633363, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7862/8562 [31:57<02:43,  4.27it/s, loss=16.633363, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7862/8562 [31:58<02:43,  4.27it/s, loss=16.633378, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7863/8562 [31:58<02:43,  4.28it/s, loss=16.633378, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7863/8562 [31:58<02:43,  4.28it/s, loss=16.632837, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7864/8562 [31:58<02:43,  4.26it/s, loss=16.632837, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7864/8562 [31:58<02:43,  4.26it/s, loss=16.632784, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7865/8562 [31:58<02:43,  4.26it/s, loss=16.632784, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7865/8562 [31:58<02:43,  4.26it/s, loss=16.633071, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7866/8562 [31:58<02:43,  4.27it/s, loss=16.633071, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7866/8562 [31:59<02:43,  4.27it/s, loss=16.632821, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7867/8562 [31:59<02:43,  4.26it/s, loss=16.632821, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7867/8562 [31:59<02:43,  4.26it/s, loss=16.633383, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7868/8562 [31:59<02:42,  4.27it/s, loss=16.633383, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7868/8562 [31:59<02:42,  4.27it/s, loss=16.633080, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7869/8562 [31:59<02:42,  4.27it/s, loss=16.633080, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7869/8562 [31:59<02:42,  4.27it/s, loss=16.633282, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7870/8562 [31:59<02:59,  3.85it/s, loss=16.633282, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7870/8562 [32:00<02:59,  3.85it/s, loss=16.633556, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7871/8562 [32:00<03:03,  3.76it/s, loss=16.633556, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7871/8562 [32:00<03:03,  3.76it/s, loss=16.634044, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7872/8562 [32:00<03:07,  3.68it/s, loss=16.634044, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7872/8562 [32:00<03:07,  3.68it/s, loss=16.634228, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7873/8562 [32:00<02:59,  3.85it/s, loss=16.634228, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7873/8562 [32:00<02:59,  3.85it/s, loss=16.634129, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7874/8562 [32:00<02:54,  3.94it/s, loss=16.634129, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7874/8562 [32:01<02:54,  3.94it/s, loss=16.634498, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7875/8562 [32:01<02:50,  4.03it/s, loss=16.634498, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7875/8562 [32:01<02:50,  4.03it/s, loss=16.634897, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7876/8562 [32:01<02:48,  4.06it/s, loss=16.634897, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7876/8562 [32:01<02:48,  4.06it/s, loss=16.634608, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7877/8562 [32:01<02:46,  4.12it/s, loss=16.634608, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7877/8562 [32:01<02:46,  4.12it/s, loss=16.633879, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7878/8562 [32:01<02:45,  4.14it/s, loss=16.633879, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7878/8562 [32:02<02:45,  4.14it/s, loss=16.634148, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7879/8562 [32:02<02:47,  4.08it/s, loss=16.634148, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7879/8562 [32:02<02:47,  4.08it/s, loss=16.634424, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7880/8562 [32:02<02:44,  4.15it/s, loss=16.634424, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7880/8562 [32:02<02:44,  4.15it/s, loss=16.634335, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7881/8562 [32:02<02:43,  4.18it/s, loss=16.634335, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7881/8562 [32:02<02:43,  4.18it/s, loss=16.634753, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7882/8562 [32:02<02:43,  4.16it/s, loss=16.634753, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7882/8562 [32:03<02:43,  4.16it/s, loss=16.634491, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7883/8562 [32:03<02:42,  4.17it/s, loss=16.634491, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7883/8562 [32:03<02:42,  4.17it/s, loss=16.634135, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7884/8562 [32:03<02:41,  4.19it/s, loss=16.634135, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7884/8562 [32:03<02:41,  4.19it/s, loss=16.634318, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7885/8562 [32:03<02:40,  4.22it/s, loss=16.634318, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7885/8562 [32:03<02:40,  4.22it/s, loss=16.634752, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7886/8562 [32:03<02:40,  4.20it/s, loss=16.634752, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7886/8562 [32:04<02:40,  4.20it/s, loss=16.634200, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7887/8562 [32:04<02:39,  4.22it/s, loss=16.634200, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7887/8562 [32:04<02:39,  4.22it/s, loss=16.634141, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7888/8562 [32:04<02:40,  4.20it/s, loss=16.634141, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7888/8562 [32:04<02:40,  4.20it/s, loss=16.633613, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7889/8562 [32:04<02:40,  4.18it/s, loss=16.633613, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7889/8562 [32:04<02:40,  4.18it/s, loss=16.634026, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7890/8562 [32:04<02:42,  4.14it/s, loss=16.634026, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7890/8562 [32:05<02:42,  4.14it/s, loss=16.634395, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7891/8562 [32:05<02:40,  4.18it/s, loss=16.634395, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7891/8562 [32:05<02:40,  4.18it/s, loss=16.634852, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7892/8562 [32:05<02:38,  4.22it/s, loss=16.634852, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7892/8562 [32:05<02:38,  4.22it/s, loss=16.634867, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7893/8562 [32:05<02:38,  4.23it/s, loss=16.634867, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7893/8562 [32:05<02:38,  4.23it/s, loss=16.635117, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7894/8562 [32:05<02:36,  4.26it/s, loss=16.635117, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7894/8562 [32:05<02:36,  4.26it/s, loss=16.635137, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7895/8562 [32:05<02:36,  4.27it/s, loss=16.635137, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7895/8562 [32:06<02:36,  4.27it/s, loss=16.635392, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7896/8562 [32:06<02:36,  4.27it/s, loss=16.635392, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7896/8562 [32:06<02:36,  4.27it/s, loss=16.635558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7897/8562 [32:06<02:48,  3.94it/s, loss=16.635558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7897/8562 [32:06<02:48,  3.94it/s, loss=16.636009, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7898/8562 [32:06<02:45,  4.01it/s, loss=16.636009, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7898/8562 [32:06<02:45,  4.01it/s, loss=16.635816, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7899/8562 [32:06<02:42,  4.09it/s, loss=16.635816, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7899/8562 [32:07<02:42,  4.09it/s, loss=16.635882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7900/8562 [32:07<02:40,  4.13it/s, loss=16.635882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7900/8562 [32:07<02:40,  4.13it/s, loss=16.635272, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7901/8562 [32:07<02:38,  4.18it/s, loss=16.635272, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7901/8562 [32:07<02:38,  4.18it/s, loss=16.635975, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7902/8562 [32:07<02:36,  4.21it/s, loss=16.635975, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7902/8562 [32:07<02:36,  4.21it/s, loss=16.635935, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7903/8562 [32:07<02:36,  4.20it/s, loss=16.635935, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7903/8562 [32:08<02:36,  4.20it/s, loss=16.635657, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7904/8562 [32:08<02:36,  4.20it/s, loss=16.635657, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7904/8562 [32:08<02:36,  4.20it/s, loss=16.635830, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7905/8562 [32:08<02:36,  4.19it/s, loss=16.635830, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7905/8562 [32:08<02:36,  4.19it/s, loss=16.635686, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7906/8562 [32:08<02:35,  4.22it/s, loss=16.635686, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7906/8562 [32:08<02:35,  4.22it/s, loss=16.635907, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7907/8562 [32:08<02:34,  4.24it/s, loss=16.635907, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7907/8562 [32:09<02:34,  4.24it/s, loss=16.635621, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7908/8562 [32:09<02:34,  4.24it/s, loss=16.635621, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7908/8562 [32:09<02:34,  4.24it/s, loss=16.635464, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7909/8562 [32:09<02:33,  4.26it/s, loss=16.635464, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7909/8562 [32:09<02:33,  4.26it/s, loss=16.635174, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7910/8562 [32:09<02:33,  4.24it/s, loss=16.635174, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7910/8562 [32:09<02:33,  4.24it/s, loss=16.635010, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7911/8562 [32:09<02:33,  4.24it/s, loss=16.635010, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7911/8562 [32:10<02:33,  4.24it/s, loss=16.634965, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7912/8562 [32:10<02:36,  4.14it/s, loss=16.634965, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7912/8562 [32:10<02:36,  4.14it/s, loss=16.634783, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7913/8562 [32:10<02:36,  4.14it/s, loss=16.634783, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7913/8562 [32:10<02:36,  4.14it/s, loss=16.634943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7914/8562 [32:10<02:36,  4.14it/s, loss=16.634943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7914/8562 [32:10<02:36,  4.14it/s, loss=16.634559, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7915/8562 [32:10<02:34,  4.18it/s, loss=16.634559, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7915/8562 [32:10<02:34,  4.18it/s, loss=16.634922, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7916/8562 [32:10<02:33,  4.22it/s, loss=16.634922, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7916/8562 [32:11<02:33,  4.22it/s, loss=16.634042, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7917/8562 [32:11<02:32,  4.24it/s, loss=16.634042, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7917/8562 [32:11<02:32,  4.24it/s, loss=16.633631, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7918/8562 [32:11<02:32,  4.22it/s, loss=16.633631, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7918/8562 [32:11<02:32,  4.22it/s, loss=16.633533, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7919/8562 [32:11<02:33,  4.19it/s, loss=16.633533, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  92%|█████████▏| 7919/8562 [32:11<02:33,  4.19it/s, loss=16.633204, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7920/8562 [32:11<02:36,  4.10it/s, loss=16.633204, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7920/8562 [32:12<02:36,  4.10it/s, loss=16.632702, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7921/8562 [32:12<02:36,  4.11it/s, loss=16.632702, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7921/8562 [32:12<02:36,  4.11it/s, loss=16.633392, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7922/8562 [32:12<02:35,  4.13it/s, loss=16.633392, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7922/8562 [32:12<02:35,  4.13it/s, loss=16.633351, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7923/8562 [32:12<02:33,  4.15it/s, loss=16.633351, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7923/8562 [32:12<02:33,  4.15it/s, loss=16.633438, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7924/8562 [32:12<02:32,  4.17it/s, loss=16.633438, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7924/8562 [32:13<02:32,  4.17it/s, loss=16.633235, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7925/8562 [32:13<02:34,  4.12it/s, loss=16.633235, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7925/8562 [32:13<02:34,  4.12it/s, loss=16.633345, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7926/8562 [32:13<02:32,  4.16it/s, loss=16.633345, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7926/8562 [32:13<02:32,  4.16it/s, loss=16.633302, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7927/8562 [32:13<02:30,  4.21it/s, loss=16.633302, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7927/8562 [32:13<02:30,  4.21it/s, loss=16.632793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7928/8562 [32:13<02:30,  4.22it/s, loss=16.632793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7928/8562 [32:14<02:30,  4.22it/s, loss=16.633409, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7929/8562 [32:14<02:29,  4.23it/s, loss=16.633409, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7929/8562 [32:14<02:29,  4.23it/s, loss=16.633450, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7930/8562 [32:14<02:29,  4.22it/s, loss=16.633450, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7930/8562 [32:14<02:29,  4.22it/s, loss=16.633114, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7931/8562 [32:14<02:28,  4.24it/s, loss=16.633114, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7931/8562 [32:14<02:28,  4.24it/s, loss=16.633152, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7932/8562 [32:14<02:29,  4.21it/s, loss=16.633152, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7932/8562 [32:15<02:29,  4.21it/s, loss=16.633338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7933/8562 [32:15<02:28,  4.22it/s, loss=16.633338, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7933/8562 [32:15<02:28,  4.22it/s, loss=16.632943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7934/8562 [32:15<02:29,  4.20it/s, loss=16.632943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7934/8562 [32:15<02:29,  4.20it/s, loss=16.632908, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7935/8562 [32:15<02:28,  4.21it/s, loss=16.632908, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7935/8562 [32:15<02:28,  4.21it/s, loss=16.633413, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7936/8562 [32:15<02:29,  4.19it/s, loss=16.633413, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7936/8562 [32:16<02:29,  4.19it/s, loss=16.633259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7937/8562 [32:16<02:29,  4.18it/s, loss=16.633259, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7937/8562 [32:16<02:29,  4.18it/s, loss=16.632698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7938/8562 [32:16<02:28,  4.20it/s, loss=16.632698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7938/8562 [32:16<02:28,  4.20it/s, loss=16.632671, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7939/8562 [32:16<02:27,  4.22it/s, loss=16.632671, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7939/8562 [32:16<02:27,  4.22it/s, loss=16.632527, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7940/8562 [32:16<02:26,  4.24it/s, loss=16.632527, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7940/8562 [32:16<02:26,  4.24it/s, loss=16.632819, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7941/8562 [32:16<02:26,  4.22it/s, loss=16.632819, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7941/8562 [32:17<02:26,  4.22it/s, loss=16.632825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7942/8562 [32:17<02:26,  4.23it/s, loss=16.632825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7942/8562 [32:17<02:26,  4.23it/s, loss=16.632296, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7943/8562 [32:17<02:26,  4.23it/s, loss=16.632296, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7943/8562 [32:17<02:26,  4.23it/s, loss=16.632841, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7944/8562 [32:17<02:25,  4.25it/s, loss=16.632841, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7944/8562 [32:17<02:25,  4.25it/s, loss=16.632293, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7945/8562 [32:17<02:24,  4.26it/s, loss=16.632293, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7945/8562 [32:18<02:24,  4.26it/s, loss=16.631986, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7946/8562 [32:18<02:24,  4.27it/s, loss=16.631986, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7946/8562 [32:18<02:24,  4.27it/s, loss=16.631431, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7947/8562 [32:18<02:23,  4.27it/s, loss=16.631431, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7947/8562 [32:18<02:23,  4.27it/s, loss=16.631452, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7948/8562 [32:18<02:24,  4.25it/s, loss=16.631452, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7948/8562 [32:18<02:24,  4.25it/s, loss=16.631574, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7949/8562 [32:18<02:25,  4.22it/s, loss=16.631574, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7949/8562 [32:19<02:25,  4.22it/s, loss=16.632131, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7950/8562 [32:19<02:25,  4.22it/s, loss=16.632131, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7950/8562 [32:19<02:25,  4.22it/s, loss=16.632654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7951/8562 [32:19<02:24,  4.23it/s, loss=16.632654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7951/8562 [32:19<02:24,  4.23it/s, loss=16.632844, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7952/8562 [32:19<02:23,  4.24it/s, loss=16.632844, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7952/8562 [32:19<02:23,  4.24it/s, loss=16.633124, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7953/8562 [32:19<02:23,  4.25it/s, loss=16.633124, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7953/8562 [32:20<02:23,  4.25it/s, loss=16.633365, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7954/8562 [32:20<02:23,  4.24it/s, loss=16.633365, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7954/8562 [32:20<02:23,  4.24it/s, loss=16.633538, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7955/8562 [32:20<02:22,  4.25it/s, loss=16.633538, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7955/8562 [32:20<02:22,  4.25it/s, loss=16.632577, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7956/8562 [32:20<02:33,  3.96it/s, loss=16.632577, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7956/8562 [32:20<02:33,  3.96it/s, loss=16.632755, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7957/8562 [32:20<02:45,  3.66it/s, loss=16.632755, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7957/8562 [32:21<02:45,  3.66it/s, loss=16.632330, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7958/8562 [32:21<02:48,  3.58it/s, loss=16.632330, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7958/8562 [32:21<02:48,  3.58it/s, loss=16.632328, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7959/8562 [32:21<02:46,  3.61it/s, loss=16.632328, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7959/8562 [32:21<02:46,  3.61it/s, loss=16.632770, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7960/8562 [32:21<02:51,  3.50it/s, loss=16.632770, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7960/8562 [32:22<02:51,  3.50it/s, loss=16.631932, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7961/8562 [32:22<03:10,  3.15it/s, loss=16.631932, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7961/8562 [32:22<03:10,  3.15it/s, loss=16.631677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7962/8562 [32:22<03:16,  3.05it/s, loss=16.631677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7962/8562 [32:22<03:16,  3.05it/s, loss=16.631997, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7963/8562 [32:22<03:07,  3.20it/s, loss=16.631997, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7963/8562 [32:23<03:07,  3.20it/s, loss=16.631716, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7964/8562 [32:23<03:03,  3.26it/s, loss=16.631716, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7964/8562 [32:23<03:03,  3.26it/s, loss=16.631949, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7965/8562 [32:23<03:14,  3.07it/s, loss=16.631949, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7965/8562 [32:23<03:14,  3.07it/s, loss=16.631991, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7966/8562 [32:23<03:17,  3.02it/s, loss=16.631991, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7966/8562 [32:24<03:17,  3.02it/s, loss=16.631906, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7967/8562 [32:24<03:23,  2.93it/s, loss=16.631906, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7967/8562 [32:24<03:23,  2.93it/s, loss=16.631929, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7968/8562 [32:24<03:20,  2.97it/s, loss=16.631929, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7968/8562 [32:24<03:20,  2.97it/s, loss=16.631271, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7969/8562 [32:24<03:24,  2.90it/s, loss=16.631271, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7969/8562 [32:25<03:24,  2.90it/s, loss=16.631260, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7970/8562 [32:25<03:26,  2.87it/s, loss=16.631260, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7970/8562 [32:25<03:26,  2.87it/s, loss=16.631768, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7971/8562 [32:25<03:21,  2.94it/s, loss=16.631768, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7971/8562 [32:25<03:21,  2.94it/s, loss=16.631509, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7972/8562 [32:25<03:12,  3.06it/s, loss=16.631509, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7972/8562 [32:26<03:12,  3.06it/s, loss=16.631723, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7973/8562 [32:26<03:26,  2.86it/s, loss=16.631723, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7973/8562 [32:26<03:26,  2.86it/s, loss=16.631339, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7974/8562 [32:26<03:14,  3.03it/s, loss=16.631339, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7974/8562 [32:26<03:14,  3.03it/s, loss=16.630295, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7975/8562 [32:26<03:15,  3.00it/s, loss=16.630295, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7975/8562 [32:27<03:15,  3.00it/s, loss=16.630051, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7976/8562 [32:27<03:10,  3.08it/s, loss=16.630051, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7976/8562 [32:27<03:10,  3.08it/s, loss=16.629946, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7977/8562 [32:27<03:04,  3.18it/s, loss=16.629946, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7977/8562 [32:27<03:04,  3.18it/s, loss=16.629398, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7978/8562 [32:27<03:03,  3.18it/s, loss=16.629398, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7978/8562 [32:28<03:03,  3.18it/s, loss=16.629173, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7979/8562 [32:28<03:08,  3.10it/s, loss=16.629173, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7979/8562 [32:28<03:08,  3.10it/s, loss=16.629440, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7980/8562 [32:28<02:53,  3.36it/s, loss=16.629440, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7980/8562 [32:28<02:53,  3.36it/s, loss=16.628910, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7981/8562 [32:28<02:42,  3.56it/s, loss=16.628910, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7981/8562 [32:28<02:42,  3.56it/s, loss=16.628788, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7982/8562 [32:28<02:36,  3.70it/s, loss=16.628788, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7982/8562 [32:29<02:36,  3.70it/s, loss=16.628494, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7983/8562 [32:29<02:32,  3.79it/s, loss=16.628494, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7983/8562 [32:29<02:32,  3.79it/s, loss=16.628325, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7984/8562 [32:29<02:28,  3.89it/s, loss=16.628325, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7984/8562 [32:29<02:28,  3.89it/s, loss=16.628841, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7985/8562 [32:29<02:24,  3.99it/s, loss=16.628841, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7985/8562 [32:29<02:24,  3.99it/s, loss=16.628855, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7986/8562 [32:29<02:21,  4.06it/s, loss=16.628855, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7986/8562 [32:29<02:21,  4.06it/s, loss=16.628689, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7987/8562 [32:29<02:20,  4.11it/s, loss=16.628689, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7987/8562 [32:30<02:20,  4.11it/s, loss=16.628836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7988/8562 [32:30<02:19,  4.12it/s, loss=16.628836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7988/8562 [32:30<02:19,  4.12it/s, loss=16.628551, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7989/8562 [32:30<02:17,  4.16it/s, loss=16.628551, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7989/8562 [32:30<02:17,  4.16it/s, loss=16.628477, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7990/8562 [32:30<02:18,  4.12it/s, loss=16.628477, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7990/8562 [32:30<02:18,  4.12it/s, loss=16.628470, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7991/8562 [32:30<02:17,  4.15it/s, loss=16.628470, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7991/8562 [32:31<02:17,  4.15it/s, loss=16.628098, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7992/8562 [32:31<02:18,  4.12it/s, loss=16.628098, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7992/8562 [32:31<02:18,  4.12it/s, loss=16.628215, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7993/8562 [32:31<02:17,  4.15it/s, loss=16.628215, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7993/8562 [32:31<02:17,  4.15it/s, loss=16.628088, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7994/8562 [32:31<02:15,  4.19it/s, loss=16.628088, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7994/8562 [32:31<02:15,  4.19it/s, loss=16.627682, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7995/8562 [32:31<02:16,  4.16it/s, loss=16.627682, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7995/8562 [32:32<02:16,  4.16it/s, loss=16.628135, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7996/8562 [32:32<02:14,  4.20it/s, loss=16.628135, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7996/8562 [32:32<02:14,  4.20it/s, loss=16.628177, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7997/8562 [32:32<02:13,  4.22it/s, loss=16.628177, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7997/8562 [32:32<02:13,  4.22it/s, loss=16.628188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7998/8562 [32:32<02:13,  4.23it/s, loss=16.628188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7998/8562 [32:32<02:13,  4.23it/s, loss=16.627696, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7999/8562 [32:32<02:14,  4.20it/s, loss=16.627696, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 7999/8562 [32:33<02:14,  4.20it/s, loss=16.628058, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8000/8562 [32:33<02:14,  4.19it/s, loss=16.628058, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8000/8562 [32:33<02:14,  4.19it/s, loss=16.628322, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8001/8562 [32:33<02:13,  4.21it/s, loss=16.628322, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8001/8562 [32:33<02:13,  4.21it/s, loss=16.628118, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8002/8562 [32:33<02:12,  4.22it/s, loss=16.628118, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8002/8562 [32:33<02:12,  4.22it/s, loss=16.627958, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8003/8562 [32:33<02:11,  4.24it/s, loss=16.627958, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8003/8562 [32:34<02:11,  4.24it/s, loss=16.628090, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8004/8562 [32:34<02:12,  4.21it/s, loss=16.628090, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8004/8562 [32:34<02:12,  4.21it/s, loss=16.627797, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8005/8562 [32:34<02:13,  4.16it/s, loss=16.627797, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  93%|█████████▎| 8005/8562 [32:34<02:13,  4.16it/s, loss=16.627921, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8006/8562 [32:34<02:13,  4.17it/s, loss=16.627921, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8006/8562 [32:34<02:13,  4.17it/s, loss=16.628127, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8007/8562 [32:34<02:11,  4.20it/s, loss=16.628127, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8007/8562 [32:34<02:11,  4.20it/s, loss=16.628658, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8008/8562 [32:34<02:11,  4.22it/s, loss=16.628658, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8008/8562 [32:35<02:11,  4.22it/s, loss=16.628683, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8009/8562 [32:35<02:10,  4.23it/s, loss=16.628683, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8009/8562 [32:35<02:10,  4.23it/s, loss=16.628270, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8010/8562 [32:35<02:09,  4.26it/s, loss=16.628270, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8010/8562 [32:35<02:09,  4.26it/s, loss=16.628233, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8011/8562 [32:35<02:09,  4.26it/s, loss=16.628233, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8011/8562 [32:35<02:09,  4.26it/s, loss=16.627995, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8012/8562 [32:35<02:10,  4.22it/s, loss=16.627995, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8012/8562 [32:36<02:10,  4.22it/s, loss=16.628471, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8013/8562 [32:36<02:10,  4.22it/s, loss=16.628471, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8013/8562 [32:36<02:10,  4.22it/s, loss=16.628323, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8014/8562 [32:36<02:09,  4.23it/s, loss=16.628323, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8014/8562 [32:36<02:09,  4.23it/s, loss=16.628543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8015/8562 [32:36<02:09,  4.22it/s, loss=16.628543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8015/8562 [32:36<02:09,  4.22it/s, loss=16.628552, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8016/8562 [32:36<02:09,  4.21it/s, loss=16.628552, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8016/8562 [32:37<02:09,  4.21it/s, loss=16.628282, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8017/8562 [32:37<02:09,  4.21it/s, loss=16.628282, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8017/8562 [32:37<02:09,  4.21it/s, loss=16.628713, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8018/8562 [32:37<02:09,  4.20it/s, loss=16.628713, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8018/8562 [32:37<02:09,  4.20it/s, loss=16.628304, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8019/8562 [32:37<02:08,  4.24it/s, loss=16.628304, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8019/8562 [32:37<02:08,  4.24it/s, loss=16.627609, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8020/8562 [32:37<02:08,  4.23it/s, loss=16.627609, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8020/8562 [32:38<02:08,  4.23it/s, loss=16.627429, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8021/8562 [32:38<02:09,  4.17it/s, loss=16.627429, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8021/8562 [32:38<02:09,  4.17it/s, loss=16.627447, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8022/8562 [32:38<02:08,  4.20it/s, loss=16.627447, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8022/8562 [32:38<02:08,  4.20it/s, loss=16.627177, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8023/8562 [32:38<02:08,  4.18it/s, loss=16.627177, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8023/8562 [32:38<02:08,  4.18it/s, loss=16.627103, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8024/8562 [32:38<02:07,  4.22it/s, loss=16.627103, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8024/8562 [32:39<02:07,  4.22it/s, loss=16.627155, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8025/8562 [32:39<02:07,  4.22it/s, loss=16.627155, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8025/8562 [32:39<02:07,  4.22it/s, loss=16.627030, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8026/8562 [32:39<02:06,  4.25it/s, loss=16.627030, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▎| 8026/8562 [32:39<02:06,  4.25it/s, loss=16.627653, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8027/8562 [32:39<02:05,  4.25it/s, loss=16.627653, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8027/8562 [32:39<02:05,  4.25it/s, loss=16.626962, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8028/8562 [32:39<02:05,  4.24it/s, loss=16.626962, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8028/8562 [32:39<02:05,  4.24it/s, loss=16.626915, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8029/8562 [32:39<02:06,  4.21it/s, loss=16.626915, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8029/8562 [32:40<02:06,  4.21it/s, loss=16.626794, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8030/8562 [32:40<02:06,  4.22it/s, loss=16.626794, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8030/8562 [32:40<02:06,  4.22it/s, loss=16.627145, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8031/8562 [32:40<02:05,  4.23it/s, loss=16.627145, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8031/8562 [32:40<02:05,  4.23it/s, loss=16.626018, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8032/8562 [32:40<02:05,  4.24it/s, loss=16.626018, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8032/8562 [32:40<02:05,  4.24it/s, loss=16.625803, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8033/8562 [32:40<02:04,  4.25it/s, loss=16.625803, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8033/8562 [32:41<02:04,  4.25it/s, loss=16.625625, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8034/8562 [32:41<02:04,  4.25it/s, loss=16.625625, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8034/8562 [32:41<02:04,  4.25it/s, loss=16.626374, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8035/8562 [32:41<02:04,  4.24it/s, loss=16.626374, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8035/8562 [32:41<02:04,  4.24it/s, loss=16.626013, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8036/8562 [32:41<02:05,  4.20it/s, loss=16.626013, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8036/8562 [32:41<02:05,  4.20it/s, loss=16.625501, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8037/8562 [32:41<02:04,  4.22it/s, loss=16.625501, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8037/8562 [32:42<02:04,  4.22it/s, loss=16.624419, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8038/8562 [32:42<02:03,  4.24it/s, loss=16.624419, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8038/8562 [32:42<02:03,  4.24it/s, loss=16.624759, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8039/8562 [32:42<02:02,  4.25it/s, loss=16.624759, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8039/8562 [32:42<02:02,  4.25it/s, loss=16.624571, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8040/8562 [32:42<02:03,  4.22it/s, loss=16.624571, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8040/8562 [32:42<02:03,  4.22it/s, loss=16.624247, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8041/8562 [32:42<02:03,  4.24it/s, loss=16.624247, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8041/8562 [32:43<02:03,  4.24it/s, loss=16.624183, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8042/8562 [32:43<02:02,  4.24it/s, loss=16.624183, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8042/8562 [32:43<02:02,  4.24it/s, loss=16.623921, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8043/8562 [32:43<02:02,  4.25it/s, loss=16.623921, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8043/8562 [32:43<02:02,  4.25it/s, loss=16.623694, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8044/8562 [32:43<02:05,  4.12it/s, loss=16.623694, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8044/8562 [32:43<02:05,  4.12it/s, loss=16.623194, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8045/8562 [32:43<02:05,  4.13it/s, loss=16.623194, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8045/8562 [32:44<02:05,  4.13it/s, loss=16.623281, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8046/8562 [32:44<02:05,  4.11it/s, loss=16.623281, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8046/8562 [32:44<02:05,  4.11it/s, loss=16.623280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8047/8562 [32:44<02:03,  4.15it/s, loss=16.623280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8047/8562 [32:44<02:03,  4.15it/s, loss=16.623084, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8048/8562 [32:44<02:03,  4.16it/s, loss=16.623084, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8048/8562 [32:44<02:03,  4.16it/s, loss=16.622424, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8049/8562 [32:44<02:02,  4.19it/s, loss=16.622424, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8049/8562 [32:44<02:02,  4.19it/s, loss=16.622269, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8050/8562 [32:44<02:01,  4.21it/s, loss=16.622269, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8050/8562 [32:45<02:01,  4.21it/s, loss=16.622244, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8051/8562 [32:45<02:05,  4.08it/s, loss=16.622244, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8051/8562 [32:45<02:05,  4.08it/s, loss=16.621903, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8052/8562 [32:45<02:03,  4.14it/s, loss=16.621903, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8052/8562 [32:45<02:03,  4.14it/s, loss=16.621584, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8053/8562 [32:45<02:02,  4.17it/s, loss=16.621584, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8053/8562 [32:45<02:02,  4.17it/s, loss=16.621796, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8054/8562 [32:45<02:01,  4.18it/s, loss=16.621796, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8054/8562 [32:46<02:01,  4.18it/s, loss=16.621595, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8055/8562 [32:46<02:01,  4.17it/s, loss=16.621595, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8055/8562 [32:46<02:01,  4.17it/s, loss=16.621130, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8056/8562 [32:46<02:00,  4.18it/s, loss=16.621130, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8056/8562 [32:46<02:00,  4.18it/s, loss=16.621256, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8057/8562 [32:46<01:59,  4.21it/s, loss=16.621256, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8057/8562 [32:46<01:59,  4.21it/s, loss=16.621599, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8058/8562 [32:46<01:59,  4.23it/s, loss=16.621599, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8058/8562 [32:47<01:59,  4.23it/s, loss=16.621988, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8059/8562 [32:47<01:59,  4.20it/s, loss=16.621988, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8059/8562 [32:47<01:59,  4.20it/s, loss=16.622077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8060/8562 [32:47<02:01,  4.12it/s, loss=16.622077, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8060/8562 [32:47<02:01,  4.12it/s, loss=16.621733, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8061/8562 [32:47<02:00,  4.16it/s, loss=16.621733, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8061/8562 [32:47<02:00,  4.16it/s, loss=16.621795, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8062/8562 [32:47<01:59,  4.18it/s, loss=16.621795, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8062/8562 [32:48<01:59,  4.18it/s, loss=16.622116, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8063/8562 [32:48<01:58,  4.19it/s, loss=16.622116, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8063/8562 [32:48<01:58,  4.19it/s, loss=16.622403, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8064/8562 [32:48<01:59,  4.15it/s, loss=16.622403, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8064/8562 [32:48<01:59,  4.15it/s, loss=16.622815, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8065/8562 [32:48<01:57,  4.21it/s, loss=16.622815, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8065/8562 [32:48<01:57,  4.21it/s, loss=16.622676, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8066/8562 [32:48<01:56,  4.24it/s, loss=16.622676, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8066/8562 [32:49<01:56,  4.24it/s, loss=16.622826, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8067/8562 [32:49<01:57,  4.22it/s, loss=16.622826, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8067/8562 [32:49<01:57,  4.22it/s, loss=16.622920, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8068/8562 [32:49<01:56,  4.23it/s, loss=16.622920, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8068/8562 [32:49<01:56,  4.23it/s, loss=16.623504, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8069/8562 [32:49<01:55,  4.28it/s, loss=16.623504, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8069/8562 [32:49<01:55,  4.28it/s, loss=16.623784, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8070/8562 [32:49<01:54,  4.29it/s, loss=16.623784, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8070/8562 [32:49<01:54,  4.29it/s, loss=16.622779, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8071/8562 [32:49<01:53,  4.31it/s, loss=16.622779, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8071/8562 [32:50<01:53,  4.31it/s, loss=16.622660, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8072/8562 [32:50<01:54,  4.29it/s, loss=16.622660, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8072/8562 [32:50<01:54,  4.29it/s, loss=16.623091, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8073/8562 [32:50<01:54,  4.27it/s, loss=16.623091, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8073/8562 [32:50<01:54,  4.27it/s, loss=16.623262, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8074/8562 [32:50<01:56,  4.20it/s, loss=16.623262, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8074/8562 [32:50<01:56,  4.20it/s, loss=16.623593, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8075/8562 [32:50<01:57,  4.14it/s, loss=16.623593, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8075/8562 [32:51<01:57,  4.14it/s, loss=16.623780, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8076/8562 [32:51<01:57,  4.15it/s, loss=16.623780, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8076/8562 [32:51<01:57,  4.15it/s, loss=16.623672, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8077/8562 [32:51<01:56,  4.15it/s, loss=16.623672, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8077/8562 [32:51<01:56,  4.15it/s, loss=16.624184, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8078/8562 [32:51<01:55,  4.18it/s, loss=16.624184, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8078/8562 [32:51<01:55,  4.18it/s, loss=16.624592, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8079/8562 [32:51<01:55,  4.20it/s, loss=16.624592, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8079/8562 [32:52<01:55,  4.20it/s, loss=16.624654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8080/8562 [32:52<01:54,  4.21it/s, loss=16.624654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8080/8562 [32:52<01:54,  4.21it/s, loss=16.624891, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8081/8562 [32:52<01:53,  4.22it/s, loss=16.624891, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8081/8562 [32:52<01:53,  4.22it/s, loss=16.624841, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8082/8562 [32:52<01:53,  4.21it/s, loss=16.624841, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8082/8562 [32:52<01:53,  4.21it/s, loss=16.625017, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8083/8562 [32:52<01:53,  4.23it/s, loss=16.625017, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8083/8562 [32:53<01:53,  4.23it/s, loss=16.624307, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8084/8562 [32:53<01:52,  4.24it/s, loss=16.624307, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8084/8562 [32:53<01:52,  4.24it/s, loss=16.624653, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8085/8562 [32:53<01:53,  4.22it/s, loss=16.624653, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8085/8562 [32:53<01:53,  4.22it/s, loss=16.624882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8086/8562 [32:53<01:52,  4.22it/s, loss=16.624882, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8086/8562 [32:53<01:52,  4.22it/s, loss=16.625041, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8087/8562 [32:53<01:55,  4.11it/s, loss=16.625041, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8087/8562 [32:54<01:55,  4.11it/s, loss=16.625489, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8088/8562 [32:54<01:54,  4.14it/s, loss=16.625489, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8088/8562 [32:54<01:54,  4.14it/s, loss=16.624840, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8089/8562 [32:54<01:53,  4.17it/s, loss=16.624840, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8089/8562 [32:54<01:53,  4.17it/s, loss=16.625156, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8090/8562 [32:54<01:51,  4.21it/s, loss=16.625156, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8090/8562 [32:54<01:51,  4.21it/s, loss=16.624937, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8091/8562 [32:54<01:51,  4.24it/s, loss=16.624937, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  94%|█████████▍| 8091/8562 [32:54<01:51,  4.24it/s, loss=16.624506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8092/8562 [32:54<01:50,  4.24it/s, loss=16.624506, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8092/8562 [32:55<01:50,  4.24it/s, loss=16.623790, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8093/8562 [32:55<01:51,  4.21it/s, loss=16.623790, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8093/8562 [32:55<01:51,  4.21it/s, loss=16.623437, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8094/8562 [32:55<01:51,  4.21it/s, loss=16.623437, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8094/8562 [32:55<01:51,  4.21it/s, loss=16.623229, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8095/8562 [32:55<01:51,  4.20it/s, loss=16.623229, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8095/8562 [32:55<01:51,  4.20it/s, loss=16.622895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8096/8562 [32:55<01:53,  4.10it/s, loss=16.622895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8096/8562 [32:56<01:53,  4.10it/s, loss=16.623545, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8097/8562 [32:56<01:52,  4.15it/s, loss=16.623545, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8097/8562 [32:56<01:52,  4.15it/s, loss=16.623288, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8098/8562 [32:56<01:51,  4.17it/s, loss=16.623288, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8098/8562 [32:56<01:51,  4.17it/s, loss=16.623109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8099/8562 [32:56<01:51,  4.15it/s, loss=16.623109, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8099/8562 [32:56<01:51,  4.15it/s, loss=16.623318, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8100/8562 [32:56<01:51,  4.15it/s, loss=16.623318, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8100/8562 [32:57<01:51,  4.15it/s, loss=16.623698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8101/8562 [32:57<01:50,  4.18it/s, loss=16.623698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8101/8562 [32:57<01:50,  4.18it/s, loss=16.624179, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8102/8562 [32:57<01:49,  4.20it/s, loss=16.624179, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8102/8562 [32:57<01:49,  4.20it/s, loss=16.623676, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8103/8562 [32:57<01:48,  4.23it/s, loss=16.623676, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8103/8562 [32:57<01:48,  4.23it/s, loss=16.623767, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8104/8562 [32:57<01:50,  4.16it/s, loss=16.623767, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8104/8562 [32:58<01:50,  4.16it/s, loss=16.623577, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8105/8562 [32:58<01:49,  4.16it/s, loss=16.623577, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8105/8562 [32:58<01:49,  4.16it/s, loss=16.623245, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8106/8562 [32:58<01:48,  4.19it/s, loss=16.623245, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8106/8562 [32:58<01:48,  4.19it/s, loss=16.622865, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8107/8562 [32:58<01:48,  4.20it/s, loss=16.622865, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8107/8562 [32:58<01:48,  4.20it/s, loss=16.622984, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8108/8562 [32:58<01:48,  4.20it/s, loss=16.622984, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8108/8562 [32:59<01:48,  4.20it/s, loss=16.623406, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8109/8562 [32:59<01:47,  4.21it/s, loss=16.623406, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8109/8562 [32:59<01:47,  4.21it/s, loss=16.623668, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8110/8562 [32:59<01:46,  4.23it/s, loss=16.623668, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8110/8562 [32:59<01:46,  4.23it/s, loss=16.624069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8111/8562 [32:59<01:48,  4.17it/s, loss=16.624069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8111/8562 [32:59<01:48,  4.17it/s, loss=16.624751, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8112/8562 [32:59<01:47,  4.20it/s, loss=16.624751, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8112/8562 [32:59<01:47,  4.20it/s, loss=16.624954, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8113/8562 [32:59<01:47,  4.17it/s, loss=16.624954, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8113/8562 [33:00<01:47,  4.17it/s, loss=16.624602, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8114/8562 [33:00<01:46,  4.21it/s, loss=16.624602, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8114/8562 [33:00<01:46,  4.21it/s, loss=16.624654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8115/8562 [33:00<01:45,  4.23it/s, loss=16.624654, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8115/8562 [33:00<01:45,  4.23it/s, loss=16.624994, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8116/8562 [33:00<01:44,  4.25it/s, loss=16.624994, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8116/8562 [33:00<01:44,  4.25it/s, loss=16.625039, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8117/8562 [33:00<01:44,  4.25it/s, loss=16.625039, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8117/8562 [33:01<01:44,  4.25it/s, loss=16.625033, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8118/8562 [33:01<01:46,  4.16it/s, loss=16.625033, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8118/8562 [33:01<01:46,  4.16it/s, loss=16.625553, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8119/8562 [33:01<01:47,  4.11it/s, loss=16.625553, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8119/8562 [33:01<01:47,  4.11it/s, loss=16.625520, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8120/8562 [33:01<01:46,  4.16it/s, loss=16.625520, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8120/8562 [33:01<01:46,  4.16it/s, loss=16.625949, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8121/8562 [33:01<01:44,  4.21it/s, loss=16.625949, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8121/8562 [33:02<01:44,  4.21it/s, loss=16.625722, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8122/8562 [33:02<01:45,  4.17it/s, loss=16.625722, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8122/8562 [33:02<01:45,  4.17it/s, loss=16.626184, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8123/8562 [33:02<01:44,  4.20it/s, loss=16.626184, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8123/8562 [33:02<01:44,  4.20it/s, loss=16.626584, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8124/8562 [33:02<01:43,  4.24it/s, loss=16.626584, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8124/8562 [33:02<01:43,  4.24it/s, loss=16.626216, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8125/8562 [33:02<01:42,  4.26it/s, loss=16.626216, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8125/8562 [33:03<01:42,  4.26it/s, loss=16.626544, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8126/8562 [33:03<01:42,  4.25it/s, loss=16.626544, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8126/8562 [33:03<01:42,  4.25it/s, loss=16.626421, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8127/8562 [33:03<01:41,  4.27it/s, loss=16.626421, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8127/8562 [33:03<01:41,  4.27it/s, loss=16.625617, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8128/8562 [33:03<01:41,  4.26it/s, loss=16.625617, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8128/8562 [33:03<01:41,  4.26it/s, loss=16.625957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8129/8562 [33:03<01:42,  4.24it/s, loss=16.625957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8129/8562 [33:04<01:42,  4.24it/s, loss=16.625938, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8130/8562 [33:04<01:43,  4.16it/s, loss=16.625938, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8130/8562 [33:04<01:43,  4.16it/s, loss=16.625962, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8131/8562 [33:04<01:43,  4.17it/s, loss=16.625962, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8131/8562 [33:04<01:43,  4.17it/s, loss=16.626382, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8132/8562 [33:04<01:43,  4.16it/s, loss=16.626382, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8132/8562 [33:04<01:43,  4.16it/s, loss=16.627089, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8133/8562 [33:04<01:43,  4.16it/s, loss=16.627089, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▍| 8133/8562 [33:04<01:43,  4.16it/s, loss=16.626827, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8134/8562 [33:04<01:41,  4.20it/s, loss=16.626827, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8134/8562 [33:05<01:41,  4.20it/s, loss=16.627378, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8135/8562 [33:05<01:41,  4.22it/s, loss=16.627378, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8135/8562 [33:05<01:41,  4.22it/s, loss=16.627731, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8136/8562 [33:05<01:40,  4.23it/s, loss=16.627731, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8136/8562 [33:05<01:40,  4.23it/s, loss=16.627825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8137/8562 [33:05<01:39,  4.25it/s, loss=16.627825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8137/8562 [33:05<01:39,  4.25it/s, loss=16.627630, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8138/8562 [33:05<01:39,  4.25it/s, loss=16.627630, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8138/8562 [33:06<01:39,  4.25it/s, loss=16.627810, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8139/8562 [33:06<01:39,  4.24it/s, loss=16.627810, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8139/8562 [33:06<01:39,  4.24it/s, loss=16.627900, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8140/8562 [33:06<01:39,  4.23it/s, loss=16.627900, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8140/8562 [33:06<01:39,  4.23it/s, loss=16.627951, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8141/8562 [33:06<01:38,  4.26it/s, loss=16.627951, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8141/8562 [33:06<01:38,  4.26it/s, loss=16.628444, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8142/8562 [33:06<01:40,  4.16it/s, loss=16.628444, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8142/8562 [33:07<01:40,  4.16it/s, loss=16.627856, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8143/8562 [33:07<01:40,  4.19it/s, loss=16.627856, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8143/8562 [33:07<01:40,  4.19it/s, loss=16.627720, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8144/8562 [33:07<01:39,  4.20it/s, loss=16.627720, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8144/8562 [33:07<01:39,  4.20it/s, loss=16.628222, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8145/8562 [33:07<01:38,  4.22it/s, loss=16.628222, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8145/8562 [33:07<01:38,  4.22it/s, loss=16.628491, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8146/8562 [33:07<01:38,  4.24it/s, loss=16.628491, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8146/8562 [33:08<01:38,  4.24it/s, loss=16.628967, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8147/8562 [33:08<01:37,  4.24it/s, loss=16.628967, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8147/8562 [33:08<01:37,  4.24it/s, loss=16.628600, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8148/8562 [33:08<01:37,  4.26it/s, loss=16.628600, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8148/8562 [33:08<01:37,  4.26it/s, loss=16.628678, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8149/8562 [33:08<01:36,  4.27it/s, loss=16.628678, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8149/8562 [33:08<01:36,  4.27it/s, loss=16.629228, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8150/8562 [33:08<01:36,  4.29it/s, loss=16.629228, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8150/8562 [33:08<01:36,  4.29it/s, loss=16.629335, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8151/8562 [33:08<01:36,  4.26it/s, loss=16.629335, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8151/8562 [33:09<01:36,  4.26it/s, loss=16.628775, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8152/8562 [33:09<01:36,  4.26it/s, loss=16.628775, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8152/8562 [33:09<01:36,  4.26it/s, loss=16.629218, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8153/8562 [33:09<01:35,  4.27it/s, loss=16.629218, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8153/8562 [33:09<01:35,  4.27it/s, loss=16.629107, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8154/8562 [33:09<01:35,  4.26it/s, loss=16.629107, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8154/8562 [33:09<01:35,  4.26it/s, loss=16.629287, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8155/8562 [33:09<01:35,  4.26it/s, loss=16.629287, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8155/8562 [33:10<01:35,  4.26it/s, loss=16.628859, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8156/8562 [33:10<01:34,  4.28it/s, loss=16.628859, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8156/8562 [33:10<01:34,  4.28it/s, loss=16.628817, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8157/8562 [33:10<01:34,  4.30it/s, loss=16.628817, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8157/8562 [33:10<01:34,  4.30it/s, loss=16.628553, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8158/8562 [33:10<01:33,  4.31it/s, loss=16.628553, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8158/8562 [33:10<01:33,  4.31it/s, loss=16.628698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8159/8562 [33:10<01:35,  4.24it/s, loss=16.628698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8159/8562 [33:11<01:35,  4.24it/s, loss=16.628977, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8160/8562 [33:11<01:34,  4.26it/s, loss=16.628977, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8160/8562 [33:11<01:34,  4.26it/s, loss=16.628415, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8161/8562 [33:11<01:34,  4.26it/s, loss=16.628415, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8161/8562 [33:11<01:34,  4.26it/s, loss=16.628675, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8162/8562 [33:11<01:33,  4.27it/s, loss=16.628675, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8162/8562 [33:11<01:33,  4.27it/s, loss=16.629189, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8163/8562 [33:11<01:33,  4.27it/s, loss=16.629189, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8163/8562 [33:12<01:33,  4.27it/s, loss=16.629176, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8164/8562 [33:12<01:33,  4.26it/s, loss=16.629176, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8164/8562 [33:12<01:33,  4.26it/s, loss=16.629478, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8165/8562 [33:12<01:32,  4.28it/s, loss=16.629478, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8165/8562 [33:12<01:32,  4.28it/s, loss=16.629728, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8166/8562 [33:12<01:32,  4.29it/s, loss=16.629728, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8166/8562 [33:12<01:32,  4.29it/s, loss=16.629696, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8167/8562 [33:12<01:33,  4.23it/s, loss=16.629696, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8167/8562 [33:12<01:33,  4.23it/s, loss=16.630106, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8168/8562 [33:12<01:34,  4.18it/s, loss=16.630106, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8168/8562 [33:13<01:34,  4.18it/s, loss=16.630098, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8169/8562 [33:13<01:33,  4.22it/s, loss=16.630098, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8169/8562 [33:13<01:33,  4.22it/s, loss=16.630353, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8170/8562 [33:13<01:32,  4.26it/s, loss=16.630353, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8170/8562 [33:13<01:32,  4.26it/s, loss=16.630838, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8171/8562 [33:13<01:31,  4.28it/s, loss=16.630838, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8171/8562 [33:13<01:31,  4.28it/s, loss=16.630649, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8172/8562 [33:13<01:30,  4.29it/s, loss=16.630649, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8172/8562 [33:14<01:30,  4.29it/s, loss=16.630762, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8173/8562 [33:14<01:30,  4.29it/s, loss=16.630762, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8173/8562 [33:14<01:30,  4.29it/s, loss=16.631131, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8174/8562 [33:14<01:30,  4.31it/s, loss=16.631131, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8174/8562 [33:14<01:30,  4.31it/s, loss=16.631488, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8175/8562 [33:14<01:29,  4.31it/s, loss=16.631488, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8175/8562 [33:14<01:29,  4.31it/s, loss=16.631174, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8176/8562 [33:14<01:30,  4.27it/s, loss=16.631174, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  95%|█████████▌| 8176/8562 [33:15<01:30,  4.27it/s, loss=16.631286, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8177/8562 [33:15<01:29,  4.28it/s, loss=16.631286, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8177/8562 [33:15<01:29,  4.28it/s, loss=16.631169, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8178/8562 [33:15<01:29,  4.29it/s, loss=16.631169, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8178/8562 [33:15<01:29,  4.29it/s, loss=16.630793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8179/8562 [33:15<01:29,  4.29it/s, loss=16.630793, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8179/8562 [33:15<01:29,  4.29it/s, loss=16.630601, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8180/8562 [33:15<01:29,  4.27it/s, loss=16.630601, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8180/8562 [33:16<01:29,  4.27it/s, loss=16.630885, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8181/8562 [33:16<01:29,  4.28it/s, loss=16.630885, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8181/8562 [33:16<01:29,  4.28it/s, loss=16.631282, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8182/8562 [33:16<01:28,  4.29it/s, loss=16.631282, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8182/8562 [33:16<01:28,  4.29it/s, loss=16.631351, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8183/8562 [33:16<01:28,  4.29it/s, loss=16.631351, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8183/8562 [33:16<01:28,  4.29it/s, loss=16.631055, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8184/8562 [33:16<01:28,  4.28it/s, loss=16.631055, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8184/8562 [33:16<01:28,  4.28it/s, loss=16.631673, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8185/8562 [33:16<01:27,  4.31it/s, loss=16.631673, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8185/8562 [33:17<01:27,  4.31it/s, loss=16.631571, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8186/8562 [33:17<01:27,  4.29it/s, loss=16.631571, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8186/8562 [33:17<01:27,  4.29it/s, loss=16.631874, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8187/8562 [33:17<01:27,  4.29it/s, loss=16.631874, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8187/8562 [33:17<01:27,  4.29it/s, loss=16.632100, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8188/8562 [33:17<01:27,  4.28it/s, loss=16.632100, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8188/8562 [33:17<01:27,  4.28it/s, loss=16.632127, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8189/8562 [33:17<01:26,  4.29it/s, loss=16.632127, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8189/8562 [33:18<01:26,  4.29it/s, loss=16.632363, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8190/8562 [33:18<01:27,  4.23it/s, loss=16.632363, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8190/8562 [33:18<01:27,  4.23it/s, loss=16.632624, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8191/8562 [33:18<01:27,  4.22it/s, loss=16.632624, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8191/8562 [33:18<01:27,  4.22it/s, loss=16.631930, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8192/8562 [33:18<01:27,  4.25it/s, loss=16.631930, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8192/8562 [33:18<01:27,  4.25it/s, loss=16.631744, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8193/8562 [33:18<01:26,  4.26it/s, loss=16.631744, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8193/8562 [33:19<01:26,  4.26it/s, loss=16.631761, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8194/8562 [33:19<01:26,  4.25it/s, loss=16.631761, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8194/8562 [33:19<01:26,  4.25it/s, loss=16.631802, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8195/8562 [33:19<01:26,  4.25it/s, loss=16.631802, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8195/8562 [33:19<01:26,  4.25it/s, loss=16.631988, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8196/8562 [33:19<01:25,  4.27it/s, loss=16.631988, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8196/8562 [33:19<01:25,  4.27it/s, loss=16.632291, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8197/8562 [33:19<01:25,  4.27it/s, loss=16.632291, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8197/8562 [33:19<01:25,  4.27it/s, loss=16.632427, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8198/8562 [33:19<01:24,  4.29it/s, loss=16.632427, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8198/8562 [33:20<01:24,  4.29it/s, loss=16.632594, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8199/8562 [33:20<01:24,  4.31it/s, loss=16.632594, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8199/8562 [33:20<01:24,  4.31it/s, loss=16.633098, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8200/8562 [33:20<01:23,  4.32it/s, loss=16.633098, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8200/8562 [33:20<01:23,  4.32it/s, loss=16.633394, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8201/8562 [33:20<01:24,  4.30it/s, loss=16.633394, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8201/8562 [33:20<01:24,  4.30it/s, loss=16.633639, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8202/8562 [33:20<01:23,  4.30it/s, loss=16.633639, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8202/8562 [33:21<01:23,  4.30it/s, loss=16.633332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8203/8562 [33:21<01:23,  4.31it/s, loss=16.633332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8203/8562 [33:21<01:23,  4.31it/s, loss=16.633688, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8204/8562 [33:21<01:24,  4.26it/s, loss=16.633688, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8204/8562 [33:21<01:24,  4.26it/s, loss=16.633334, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8205/8562 [33:21<01:23,  4.29it/s, loss=16.633334, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8205/8562 [33:21<01:23,  4.29it/s, loss=16.633070, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8206/8562 [33:21<01:22,  4.30it/s, loss=16.633070, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8206/8562 [33:22<01:22,  4.30it/s, loss=16.633340, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8207/8562 [33:22<01:22,  4.32it/s, loss=16.633340, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8207/8562 [33:22<01:22,  4.32it/s, loss=16.633577, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8208/8562 [33:22<01:22,  4.28it/s, loss=16.633577, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8208/8562 [33:22<01:22,  4.28it/s, loss=16.633578, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8209/8562 [33:22<01:22,  4.29it/s, loss=16.633578, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8209/8562 [33:22<01:22,  4.29it/s, loss=16.633330, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8210/8562 [33:22<01:22,  4.29it/s, loss=16.633330, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8210/8562 [33:23<01:22,  4.29it/s, loss=16.633836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8211/8562 [33:23<01:22,  4.26it/s, loss=16.633836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8211/8562 [33:23<01:22,  4.26it/s, loss=16.632958, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8212/8562 [33:23<01:23,  4.17it/s, loss=16.632958, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8212/8562 [33:23<01:23,  4.17it/s, loss=16.633396, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8213/8562 [33:23<01:24,  4.12it/s, loss=16.633396, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8213/8562 [33:23<01:24,  4.12it/s, loss=16.633772, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8214/8562 [33:23<01:23,  4.17it/s, loss=16.633772, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8214/8562 [33:23<01:23,  4.17it/s, loss=16.634014, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8215/8562 [33:23<01:22,  4.20it/s, loss=16.634014, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8215/8562 [33:24<01:22,  4.20it/s, loss=16.634612, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8216/8562 [33:24<01:21,  4.23it/s, loss=16.634612, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8216/8562 [33:24<01:21,  4.23it/s, loss=16.634928, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8217/8562 [33:24<01:21,  4.25it/s, loss=16.634928, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8217/8562 [33:24<01:21,  4.25it/s, loss=16.634452, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8218/8562 [33:24<01:20,  4.26it/s, loss=16.634452, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8218/8562 [33:24<01:20,  4.26it/s, loss=16.634245, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8219/8562 [33:24<01:21,  4.21it/s, loss=16.634245, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8219/8562 [33:25<01:21,  4.21it/s, loss=16.633501, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8220/8562 [33:25<01:20,  4.24it/s, loss=16.633501, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8220/8562 [33:25<01:20,  4.24it/s, loss=16.633191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8221/8562 [33:25<01:21,  4.19it/s, loss=16.633191, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8221/8562 [33:25<01:21,  4.19it/s, loss=16.632984, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8222/8562 [33:25<01:26,  3.91it/s, loss=16.632984, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8222/8562 [33:25<01:26,  3.91it/s, loss=16.633023, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8223/8562 [33:25<01:29,  3.80it/s, loss=16.633023, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8223/8562 [33:26<01:29,  3.80it/s, loss=16.633043, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8224/8562 [33:26<01:32,  3.67it/s, loss=16.633043, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8224/8562 [33:26<01:32,  3.67it/s, loss=16.633260, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8225/8562 [33:26<01:31,  3.67it/s, loss=16.633260, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8225/8562 [33:26<01:31,  3.67it/s, loss=16.632956, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8226/8562 [33:26<01:41,  3.30it/s, loss=16.632956, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8226/8562 [33:27<01:41,  3.30it/s, loss=16.632878, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8227/8562 [33:27<01:38,  3.40it/s, loss=16.632878, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8227/8562 [33:27<01:38,  3.40it/s, loss=16.632569, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8228/8562 [33:27<01:38,  3.39it/s, loss=16.632569, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8228/8562 [33:27<01:38,  3.39it/s, loss=16.632818, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8229/8562 [33:27<01:38,  3.40it/s, loss=16.632818, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8229/8562 [33:28<01:38,  3.40it/s, loss=16.633262, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8230/8562 [33:28<01:39,  3.35it/s, loss=16.633262, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8230/8562 [33:28<01:39,  3.35it/s, loss=16.633033, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8231/8562 [33:28<01:54,  2.90it/s, loss=16.633033, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8231/8562 [33:28<01:54,  2.90it/s, loss=16.633048, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8232/8562 [33:28<01:55,  2.87it/s, loss=16.633048, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8232/8562 [33:29<01:55,  2.87it/s, loss=16.632740, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8233/8562 [33:29<01:56,  2.84it/s, loss=16.632740, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8233/8562 [33:29<01:56,  2.84it/s, loss=16.632445, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8234/8562 [33:29<01:55,  2.84it/s, loss=16.632445, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8234/8562 [33:29<01:55,  2.84it/s, loss=16.632632, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8235/8562 [33:29<01:52,  2.90it/s, loss=16.632632, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8235/8562 [33:30<01:52,  2.90it/s, loss=16.632417, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8236/8562 [33:30<01:45,  3.10it/s, loss=16.632417, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8236/8562 [33:30<01:45,  3.10it/s, loss=16.631968, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8237/8562 [33:30<01:39,  3.26it/s, loss=16.631968, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8237/8562 [33:30<01:39,  3.26it/s, loss=16.632071, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8238/8562 [33:30<01:39,  3.27it/s, loss=16.632071, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8238/8562 [33:31<01:39,  3.27it/s, loss=16.631873, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8239/8562 [33:31<01:42,  3.16it/s, loss=16.631873, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8239/8562 [33:31<01:42,  3.16it/s, loss=16.631962, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8240/8562 [33:31<01:39,  3.24it/s, loss=16.631962, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▌| 8240/8562 [33:31<01:39,  3.24it/s, loss=16.631771, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8241/8562 [33:31<01:37,  3.29it/s, loss=16.631771, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8241/8562 [33:32<01:37,  3.29it/s, loss=16.631800, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8242/8562 [33:32<01:36,  3.32it/s, loss=16.631800, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8242/8562 [33:32<01:36,  3.32it/s, loss=16.631322, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8243/8562 [33:32<01:46,  2.98it/s, loss=16.631322, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8243/8562 [33:32<01:46,  2.98it/s, loss=16.631561, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8244/8562 [33:32<01:47,  2.95it/s, loss=16.631561, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8244/8562 [33:33<01:47,  2.95it/s, loss=16.631724, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8245/8562 [33:33<01:44,  3.04it/s, loss=16.631724, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8245/8562 [33:33<01:44,  3.04it/s, loss=16.632082, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8246/8562 [33:33<01:41,  3.11it/s, loss=16.632082, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8246/8562 [33:33<01:41,  3.11it/s, loss=16.632166, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8247/8562 [33:33<01:39,  3.15it/s, loss=16.632166, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8247/8562 [33:33<01:39,  3.15it/s, loss=16.631733, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8248/8562 [33:33<01:32,  3.40it/s, loss=16.631733, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8248/8562 [33:34<01:32,  3.40it/s, loss=16.632027, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8249/8562 [33:34<01:26,  3.60it/s, loss=16.632027, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8249/8562 [33:34<01:26,  3.60it/s, loss=16.632377, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8250/8562 [33:34<01:22,  3.77it/s, loss=16.632377, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8250/8562 [33:34<01:22,  3.77it/s, loss=16.632569, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8251/8562 [33:34<01:19,  3.90it/s, loss=16.632569, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8251/8562 [33:34<01:19,  3.90it/s, loss=16.632330, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8252/8562 [33:34<01:17,  3.99it/s, loss=16.632330, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8252/8562 [33:35<01:17,  3.99it/s, loss=16.631616, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8253/8562 [33:35<01:15,  4.07it/s, loss=16.631616, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8253/8562 [33:35<01:15,  4.07it/s, loss=16.630701, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8254/8562 [33:35<01:14,  4.15it/s, loss=16.630701, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8254/8562 [33:35<01:14,  4.15it/s, loss=16.631350, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8255/8562 [33:35<01:13,  4.17it/s, loss=16.631350, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8255/8562 [33:35<01:13,  4.17it/s, loss=16.631060, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8256/8562 [33:35<01:13,  4.19it/s, loss=16.631060, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8256/8562 [33:36<01:13,  4.19it/s, loss=16.631188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8257/8562 [33:36<01:12,  4.19it/s, loss=16.631188, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8257/8562 [33:36<01:12,  4.19it/s, loss=16.631719, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8258/8562 [33:36<01:12,  4.19it/s, loss=16.631719, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8258/8562 [33:36<01:12,  4.19it/s, loss=16.631360, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8259/8562 [33:36<01:12,  4.21it/s, loss=16.631360, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8259/8562 [33:36<01:12,  4.21it/s, loss=16.631825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8260/8562 [33:36<01:12,  4.18it/s, loss=16.631825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8260/8562 [33:37<01:12,  4.18it/s, loss=16.631642, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8261/8562 [33:37<01:12,  4.17it/s, loss=16.631642, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8261/8562 [33:37<01:12,  4.17it/s, loss=16.631543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8262/8562 [33:37<01:11,  4.20it/s, loss=16.631543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  96%|█████████▋| 8262/8562 [33:37<01:11,  4.20it/s, loss=16.631780, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8263/8562 [33:37<01:10,  4.24it/s, loss=16.631780, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8263/8562 [33:37<01:10,  4.24it/s, loss=16.631272, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8264/8562 [33:37<01:10,  4.24it/s, loss=16.631272, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8264/8562 [33:37<01:10,  4.24it/s, loss=16.631281, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8265/8562 [33:37<01:09,  4.25it/s, loss=16.631281, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8265/8562 [33:38<01:09,  4.25it/s, loss=16.631372, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8266/8562 [33:38<01:09,  4.28it/s, loss=16.631372, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8266/8562 [33:38<01:09,  4.28it/s, loss=16.631539, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8267/8562 [33:38<01:08,  4.30it/s, loss=16.631539, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8267/8562 [33:38<01:08,  4.30it/s, loss=16.631844, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8268/8562 [33:38<01:08,  4.29it/s, loss=16.631844, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8268/8562 [33:38<01:08,  4.29it/s, loss=16.631774, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8269/8562 [33:38<01:08,  4.29it/s, loss=16.631774, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8269/8562 [33:39<01:08,  4.29it/s, loss=16.631027, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8270/8562 [33:39<01:08,  4.26it/s, loss=16.631027, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8270/8562 [33:39<01:08,  4.26it/s, loss=16.630203, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8271/8562 [33:39<01:07,  4.28it/s, loss=16.630203, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8271/8562 [33:39<01:07,  4.28it/s, loss=16.629909, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8272/8562 [33:39<01:07,  4.29it/s, loss=16.629909, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8272/8562 [33:39<01:07,  4.29it/s, loss=16.630028, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8273/8562 [33:39<01:07,  4.28it/s, loss=16.630028, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8273/8562 [33:40<01:07,  4.28it/s, loss=16.630442, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8274/8562 [33:40<01:07,  4.26it/s, loss=16.630442, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8274/8562 [33:40<01:07,  4.26it/s, loss=16.630754, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8275/8562 [33:40<01:06,  4.30it/s, loss=16.630754, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8275/8562 [33:40<01:06,  4.30it/s, loss=16.630934, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8276/8562 [33:40<01:06,  4.30it/s, loss=16.630934, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8276/8562 [33:40<01:06,  4.30it/s, loss=16.630594, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8277/8562 [33:40<01:06,  4.29it/s, loss=16.630594, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8277/8562 [33:40<01:06,  4.29it/s, loss=16.630840, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8278/8562 [33:40<01:06,  4.30it/s, loss=16.630840, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8278/8562 [33:41<01:06,  4.30it/s, loss=16.630740, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8279/8562 [33:41<01:05,  4.30it/s, loss=16.630740, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8279/8562 [33:41<01:05,  4.30it/s, loss=16.631170, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8280/8562 [33:41<01:05,  4.30it/s, loss=16.631170, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8280/8562 [33:41<01:05,  4.30it/s, loss=16.631041, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8281/8562 [33:41<01:05,  4.31it/s, loss=16.631041, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8281/8562 [33:41<01:05,  4.31it/s, loss=16.631181, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8282/8562 [33:41<01:04,  4.31it/s, loss=16.631181, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8282/8562 [33:42<01:04,  4.31it/s, loss=16.631044, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8283/8562 [33:42<01:06,  4.21it/s, loss=16.631044, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8283/8562 [33:42<01:06,  4.21it/s, loss=16.631254, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8284/8562 [33:42<01:05,  4.25it/s, loss=16.631254, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8284/8562 [33:42<01:05,  4.25it/s, loss=16.631181, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8285/8562 [33:42<01:05,  4.25it/s, loss=16.631181, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8285/8562 [33:42<01:05,  4.25it/s, loss=16.630618, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8286/8562 [33:42<01:04,  4.28it/s, loss=16.630618, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8286/8562 [33:43<01:04,  4.28it/s, loss=16.630824, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8287/8562 [33:43<01:04,  4.27it/s, loss=16.630824, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8287/8562 [33:43<01:04,  4.27it/s, loss=16.630677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8288/8562 [33:43<01:03,  4.30it/s, loss=16.630677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8288/8562 [33:43<01:03,  4.30it/s, loss=16.630798, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8289/8562 [33:43<01:03,  4.30it/s, loss=16.630798, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8289/8562 [33:43<01:03,  4.30it/s, loss=16.630664, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8290/8562 [33:43<01:03,  4.31it/s, loss=16.630664, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8290/8562 [33:44<01:03,  4.31it/s, loss=16.629652, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8291/8562 [33:44<01:03,  4.25it/s, loss=16.629652, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8291/8562 [33:44<01:03,  4.25it/s, loss=16.629631, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8292/8562 [33:44<01:05,  4.11it/s, loss=16.629631, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8292/8562 [33:44<01:05,  4.11it/s, loss=16.629825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8293/8562 [33:44<01:04,  4.16it/s, loss=16.629825, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8293/8562 [33:44<01:04,  4.16it/s, loss=16.630131, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8294/8562 [33:44<01:03,  4.20it/s, loss=16.630131, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8294/8562 [33:44<01:03,  4.20it/s, loss=16.629967, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8295/8562 [33:44<01:03,  4.21it/s, loss=16.629967, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8295/8562 [33:45<01:03,  4.21it/s, loss=16.630560, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8296/8562 [33:45<01:04,  4.13it/s, loss=16.630560, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8296/8562 [33:45<01:04,  4.13it/s, loss=16.631099, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8297/8562 [33:45<01:03,  4.18it/s, loss=16.631099, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8297/8562 [33:45<01:03,  4.18it/s, loss=16.630961, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8298/8562 [33:45<01:02,  4.21it/s, loss=16.630961, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8298/8562 [33:45<01:02,  4.21it/s, loss=16.631530, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8299/8562 [33:45<01:02,  4.24it/s, loss=16.631530, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8299/8562 [33:46<01:02,  4.24it/s, loss=16.631450, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8300/8562 [33:46<01:02,  4.17it/s, loss=16.631450, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8300/8562 [33:46<01:02,  4.17it/s, loss=16.631057, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8301/8562 [33:46<01:02,  4.21it/s, loss=16.631057, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8301/8562 [33:46<01:02,  4.21it/s, loss=16.631048, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8302/8562 [33:46<01:01,  4.21it/s, loss=16.631048, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8302/8562 [33:46<01:01,  4.21it/s, loss=16.630879, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8303/8562 [33:46<01:01,  4.23it/s, loss=16.630879, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8303/8562 [33:47<01:01,  4.23it/s, loss=16.630496, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8304/8562 [33:47<01:00,  4.26it/s, loss=16.630496, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8304/8562 [33:47<01:00,  4.26it/s, loss=16.630105, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8305/8562 [33:47<01:00,  4.26it/s, loss=16.630105, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8305/8562 [33:47<01:00,  4.26it/s, loss=16.630120, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8306/8562 [33:47<00:59,  4.28it/s, loss=16.630120, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8306/8562 [33:47<00:59,  4.28it/s, loss=16.630558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8307/8562 [33:47<00:59,  4.29it/s, loss=16.630558, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8307/8562 [33:48<00:59,  4.29it/s, loss=16.630335, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8308/8562 [33:48<00:59,  4.28it/s, loss=16.630335, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8308/8562 [33:48<00:59,  4.28it/s, loss=16.630701, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8309/8562 [33:48<00:59,  4.28it/s, loss=16.630701, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8309/8562 [33:48<00:59,  4.28it/s, loss=16.630380, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8310/8562 [33:48<00:59,  4.27it/s, loss=16.630380, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8310/8562 [33:48<00:59,  4.27it/s, loss=16.630486, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8311/8562 [33:48<00:59,  4.25it/s, loss=16.630486, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8311/8562 [33:48<00:59,  4.25it/s, loss=16.630680, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8312/8562 [33:49<00:59,  4.22it/s, loss=16.630680, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8312/8562 [33:49<00:59,  4.22it/s, loss=16.630641, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8313/8562 [33:49<00:58,  4.24it/s, loss=16.630641, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8313/8562 [33:49<00:58,  4.24it/s, loss=16.630345, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8314/8562 [33:49<00:58,  4.26it/s, loss=16.630345, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8314/8562 [33:49<00:58,  4.26it/s, loss=16.630615, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8315/8562 [33:49<00:58,  4.22it/s, loss=16.630615, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8315/8562 [33:49<00:58,  4.22it/s, loss=16.629957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8316/8562 [33:49<00:58,  4.21it/s, loss=16.629957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8316/8562 [33:50<00:58,  4.21it/s, loss=16.629502, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8317/8562 [33:50<00:57,  4.25it/s, loss=16.629502, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8317/8562 [33:50<00:57,  4.25it/s, loss=16.629583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8318/8562 [33:50<00:57,  4.28it/s, loss=16.629583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8318/8562 [33:50<00:57,  4.28it/s, loss=16.629382, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8319/8562 [33:50<00:56,  4.29it/s, loss=16.629382, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8319/8562 [33:50<00:56,  4.29it/s, loss=16.629775, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8320/8562 [33:50<00:56,  4.26it/s, loss=16.629775, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8320/8562 [33:51<00:56,  4.26it/s, loss=16.629664, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8321/8562 [33:51<00:56,  4.28it/s, loss=16.629664, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8321/8562 [33:51<00:56,  4.28it/s, loss=16.630230, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8322/8562 [33:51<00:55,  4.30it/s, loss=16.630230, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8322/8562 [33:51<00:55,  4.30it/s, loss=16.630667, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8323/8562 [33:51<00:56,  4.22it/s, loss=16.630667, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8323/8562 [33:51<00:56,  4.22it/s, loss=16.630601, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8324/8562 [33:51<00:55,  4.25it/s, loss=16.630601, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8324/8562 [33:52<00:55,  4.25it/s, loss=16.630359, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8325/8562 [33:52<00:55,  4.26it/s, loss=16.630359, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8325/8562 [33:52<00:55,  4.26it/s, loss=16.629512, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8326/8562 [33:52<00:55,  4.24it/s, loss=16.629512, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8326/8562 [33:52<00:55,  4.24it/s, loss=16.629891, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8327/8562 [33:52<00:55,  4.27it/s, loss=16.629891, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8327/8562 [33:52<00:55,  4.27it/s, loss=16.629640, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8328/8562 [33:52<00:54,  4.28it/s, loss=16.629640, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8328/8562 [33:52<00:54,  4.28it/s, loss=16.629959, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8329/8562 [33:53<00:55,  4.18it/s, loss=16.629959, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8329/8562 [33:53<00:55,  4.18it/s, loss=16.630036, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8330/8562 [33:53<00:55,  4.20it/s, loss=16.630036, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8330/8562 [33:53<00:55,  4.20it/s, loss=16.630589, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8331/8562 [33:53<00:54,  4.24it/s, loss=16.630589, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8331/8562 [33:53<00:54,  4.24it/s, loss=16.630224, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8332/8562 [33:53<00:53,  4.27it/s, loss=16.630224, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8332/8562 [33:53<00:53,  4.27it/s, loss=16.630081, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8333/8562 [33:53<00:53,  4.27it/s, loss=16.630081, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8333/8562 [33:54<00:53,  4.27it/s, loss=16.630432, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8334/8562 [33:54<00:53,  4.24it/s, loss=16.630432, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8334/8562 [33:54<00:53,  4.24it/s, loss=16.630117, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8335/8562 [33:54<00:53,  4.28it/s, loss=16.630117, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8335/8562 [33:54<00:53,  4.28it/s, loss=16.630255, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8336/8562 [33:54<00:52,  4.28it/s, loss=16.630255, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8336/8562 [33:54<00:52,  4.28it/s, loss=16.630580, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8337/8562 [33:54<00:52,  4.29it/s, loss=16.630580, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8337/8562 [33:55<00:52,  4.29it/s, loss=16.630842, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8338/8562 [33:55<00:52,  4.28it/s, loss=16.630842, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8338/8562 [33:55<00:52,  4.28it/s, loss=16.631081, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8339/8562 [33:55<00:52,  4.27it/s, loss=16.631081, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8339/8562 [33:55<00:52,  4.27it/s, loss=16.630757, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8340/8562 [33:55<00:51,  4.27it/s, loss=16.630757, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8340/8562 [33:55<00:51,  4.27it/s, loss=16.630807, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8341/8562 [33:55<00:51,  4.28it/s, loss=16.630807, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8341/8562 [33:56<00:51,  4.28it/s, loss=16.630018, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8342/8562 [33:56<00:51,  4.28it/s, loss=16.630018, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8342/8562 [33:56<00:51,  4.28it/s, loss=16.629715, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8343/8562 [33:56<00:50,  4.30it/s, loss=16.629715, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8343/8562 [33:56<00:50,  4.30it/s, loss=16.629661, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8344/8562 [33:56<00:51,  4.23it/s, loss=16.629661, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8344/8562 [33:56<00:51,  4.23it/s, loss=16.629299, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8345/8562 [33:56<00:51,  4.24it/s, loss=16.629299, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8345/8562 [33:56<00:51,  4.24it/s, loss=16.629236, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8346/8562 [33:56<00:50,  4.26it/s, loss=16.629236, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8346/8562 [33:57<00:50,  4.26it/s, loss=16.629172, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8347/8562 [33:57<00:50,  4.28it/s, loss=16.629172, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  97%|█████████▋| 8347/8562 [33:57<00:50,  4.28it/s, loss=16.629208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8348/8562 [33:57<00:50,  4.26it/s, loss=16.629208, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8348/8562 [33:57<00:50,  4.26it/s, loss=16.629180, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8349/8562 [33:57<00:49,  4.27it/s, loss=16.629180, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8349/8562 [33:57<00:49,  4.27it/s, loss=16.628908, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8350/8562 [33:57<00:49,  4.26it/s, loss=16.628908, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8350/8562 [33:58<00:49,  4.26it/s, loss=16.628931, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8351/8562 [33:58<00:50,  4.16it/s, loss=16.628931, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8351/8562 [33:58<00:50,  4.16it/s, loss=16.629002, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8352/8562 [33:58<00:50,  4.15it/s, loss=16.629002, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8352/8562 [33:58<00:50,  4.15it/s, loss=16.629373, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8353/8562 [33:58<00:50,  4.11it/s, loss=16.629373, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8353/8562 [33:58<00:50,  4.11it/s, loss=16.629688, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8354/8562 [33:58<00:50,  4.11it/s, loss=16.629688, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8354/8562 [33:59<00:50,  4.11it/s, loss=16.629592, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8355/8562 [33:59<00:49,  4.14it/s, loss=16.629592, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8355/8562 [33:59<00:49,  4.14it/s, loss=16.629994, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8356/8562 [33:59<00:49,  4.15it/s, loss=16.629994, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8356/8562 [33:59<00:49,  4.15it/s, loss=16.630580, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8357/8562 [33:59<00:49,  4.16it/s, loss=16.630580, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8357/8562 [33:59<00:49,  4.16it/s, loss=16.630488, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8358/8562 [33:59<00:48,  4.18it/s, loss=16.630488, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8358/8562 [34:00<00:48,  4.18it/s, loss=16.630121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8359/8562 [34:00<00:48,  4.15it/s, loss=16.630121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8359/8562 [34:00<00:48,  4.15it/s, loss=16.630033, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8360/8562 [34:00<00:48,  4.16it/s, loss=16.630033, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8360/8562 [34:00<00:48,  4.16it/s, loss=16.629872, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8361/8562 [34:00<00:49,  4.09it/s, loss=16.629872, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8361/8562 [34:00<00:49,  4.09it/s, loss=16.630040, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8362/8562 [34:00<00:48,  4.14it/s, loss=16.630040, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8362/8562 [34:01<00:48,  4.14it/s, loss=16.629444, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8363/8562 [34:01<00:47,  4.17it/s, loss=16.629444, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8363/8562 [34:01<00:47,  4.17it/s, loss=16.629067, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8364/8562 [34:01<00:48,  4.12it/s, loss=16.629067, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8364/8562 [34:01<00:48,  4.12it/s, loss=16.628932, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8365/8562 [34:01<00:47,  4.16it/s, loss=16.628932, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8365/8562 [34:01<00:47,  4.16it/s, loss=16.628843, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8366/8562 [34:01<00:46,  4.19it/s, loss=16.628843, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8366/8562 [34:02<00:46,  4.19it/s, loss=16.628699, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8367/8562 [34:02<00:46,  4.20it/s, loss=16.628699, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8367/8562 [34:02<00:46,  4.20it/s, loss=16.628220, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8368/8562 [34:02<00:46,  4.22it/s, loss=16.628220, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8368/8562 [34:02<00:46,  4.22it/s, loss=16.628146, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8369/8562 [34:02<00:45,  4.23it/s, loss=16.628146, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8369/8562 [34:02<00:45,  4.23it/s, loss=16.628090, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8370/8562 [34:02<00:45,  4.22it/s, loss=16.628090, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8370/8562 [34:02<00:45,  4.22it/s, loss=16.628025, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8371/8562 [34:02<00:44,  4.25it/s, loss=16.628025, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8371/8562 [34:03<00:44,  4.25it/s, loss=16.627664, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8372/8562 [34:03<00:44,  4.23it/s, loss=16.627664, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8372/8562 [34:03<00:44,  4.23it/s, loss=16.627616, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8373/8562 [34:03<00:45,  4.19it/s, loss=16.627616, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8373/8562 [34:03<00:45,  4.19it/s, loss=16.627495, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8374/8562 [34:03<00:44,  4.21it/s, loss=16.627495, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8374/8562 [34:03<00:44,  4.21it/s, loss=16.627071, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8375/8562 [34:03<00:44,  4.22it/s, loss=16.627071, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8375/8562 [34:04<00:44,  4.22it/s, loss=16.627210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8376/8562 [34:04<00:43,  4.23it/s, loss=16.627210, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8376/8562 [34:04<00:43,  4.23it/s, loss=16.627069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8377/8562 [34:04<00:43,  4.25it/s, loss=16.627069, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8377/8562 [34:04<00:43,  4.25it/s, loss=16.626902, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8378/8562 [34:04<00:43,  4.25it/s, loss=16.626902, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8378/8562 [34:04<00:43,  4.25it/s, loss=16.627018, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8379/8562 [34:04<00:42,  4.27it/s, loss=16.627018, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8379/8562 [34:05<00:42,  4.27it/s, loss=16.627478, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8380/8562 [34:05<00:42,  4.23it/s, loss=16.627478, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8380/8562 [34:05<00:42,  4.23it/s, loss=16.627836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8381/8562 [34:05<00:42,  4.24it/s, loss=16.627836, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8381/8562 [34:05<00:42,  4.24it/s, loss=16.626939, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8382/8562 [34:05<00:42,  4.22it/s, loss=16.626939, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8382/8562 [34:05<00:42,  4.22it/s, loss=16.626895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8383/8562 [34:05<00:42,  4.19it/s, loss=16.626895, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8383/8562 [34:06<00:42,  4.19it/s, loss=16.626750, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8384/8562 [34:06<00:42,  4.20it/s, loss=16.626750, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8384/8562 [34:06<00:42,  4.20it/s, loss=16.626084, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8385/8562 [34:06<00:42,  4.17it/s, loss=16.626084, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8385/8562 [34:06<00:42,  4.17it/s, loss=16.626031, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8386/8562 [34:06<00:41,  4.20it/s, loss=16.626031, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8386/8562 [34:06<00:41,  4.20it/s, loss=16.625957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8387/8562 [34:06<00:41,  4.23it/s, loss=16.625957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8387/8562 [34:06<00:41,  4.23it/s, loss=16.625732, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8388/8562 [34:06<00:40,  4.26it/s, loss=16.625732, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8388/8562 [34:07<00:40,  4.26it/s, loss=16.625857, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8389/8562 [34:07<00:41,  4.16it/s, loss=16.625857, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8389/8562 [34:07<00:41,  4.16it/s, loss=16.625732, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8390/8562 [34:07<00:41,  4.15it/s, loss=16.625732, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8390/8562 [34:07<00:41,  4.15it/s, loss=16.625633, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8391/8562 [34:07<00:41,  4.07it/s, loss=16.625633, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8391/8562 [34:07<00:41,  4.07it/s, loss=16.625971, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8392/8562 [34:07<00:41,  4.13it/s, loss=16.625971, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8392/8562 [34:08<00:41,  4.13it/s, loss=16.626249, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8393/8562 [34:08<00:40,  4.19it/s, loss=16.626249, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8393/8562 [34:08<00:40,  4.19it/s, loss=16.626284, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8394/8562 [34:08<00:39,  4.23it/s, loss=16.626284, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8394/8562 [34:08<00:39,  4.23it/s, loss=16.626242, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8395/8562 [34:08<00:39,  4.22it/s, loss=16.626242, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8395/8562 [34:08<00:39,  4.22it/s, loss=16.626523, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8396/8562 [34:08<00:40,  4.12it/s, loss=16.626523, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8396/8562 [34:09<00:40,  4.12it/s, loss=16.626189, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8397/8562 [34:09<00:39,  4.15it/s, loss=16.626189, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8397/8562 [34:09<00:39,  4.15it/s, loss=16.626505, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8398/8562 [34:09<00:39,  4.18it/s, loss=16.626505, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8398/8562 [34:09<00:39,  4.18it/s, loss=16.626311, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8399/8562 [34:09<00:38,  4.20it/s, loss=16.626311, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8399/8562 [34:09<00:38,  4.20it/s, loss=16.626158, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8400/8562 [34:09<00:38,  4.19it/s, loss=16.626158, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8400/8562 [34:10<00:38,  4.19it/s, loss=16.625608, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8401/8562 [34:10<00:38,  4.20it/s, loss=16.625608, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8401/8562 [34:10<00:38,  4.20it/s, loss=16.625168, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8402/8562 [34:10<00:37,  4.23it/s, loss=16.625168, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8402/8562 [34:10<00:37,  4.23it/s, loss=16.624890, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8403/8562 [34:10<00:37,  4.22it/s, loss=16.624890, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8403/8562 [34:10<00:37,  4.22it/s, loss=16.624781, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8404/8562 [34:10<00:37,  4.23it/s, loss=16.624781, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8404/8562 [34:11<00:37,  4.23it/s, loss=16.625176, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8405/8562 [34:11<00:36,  4.25it/s, loss=16.625176, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8405/8562 [34:11<00:36,  4.25it/s, loss=16.625501, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8406/8562 [34:11<00:36,  4.22it/s, loss=16.625501, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8406/8562 [34:11<00:36,  4.22it/s, loss=16.625943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8407/8562 [34:11<00:36,  4.19it/s, loss=16.625943, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8407/8562 [34:11<00:36,  4.19it/s, loss=16.625909, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8408/8562 [34:11<00:36,  4.21it/s, loss=16.625909, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8408/8562 [34:12<00:36,  4.21it/s, loss=16.625690, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8409/8562 [34:12<00:36,  4.22it/s, loss=16.625690, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8409/8562 [34:12<00:36,  4.22it/s, loss=16.625647, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8410/8562 [34:12<00:35,  4.23it/s, loss=16.625647, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8410/8562 [34:12<00:35,  4.23it/s, loss=16.624974, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8411/8562 [34:12<00:35,  4.22it/s, loss=16.624974, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8411/8562 [34:12<00:35,  4.22it/s, loss=16.625311, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8412/8562 [34:12<00:35,  4.18it/s, loss=16.625311, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8412/8562 [34:12<00:35,  4.18it/s, loss=16.625642, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8413/8562 [34:12<00:35,  4.22it/s, loss=16.625642, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8413/8562 [34:13<00:35,  4.22it/s, loss=16.625543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8414/8562 [34:13<00:35,  4.18it/s, loss=16.625543, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8414/8562 [34:13<00:35,  4.18it/s, loss=16.625403, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8415/8562 [34:13<00:35,  4.19it/s, loss=16.625403, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8415/8562 [34:13<00:35,  4.19it/s, loss=16.625108, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8416/8562 [34:13<00:34,  4.21it/s, loss=16.625108, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8416/8562 [34:13<00:34,  4.21it/s, loss=16.625037, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8417/8562 [34:13<00:35,  4.07it/s, loss=16.625037, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8417/8562 [34:14<00:35,  4.07it/s, loss=16.624674, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8418/8562 [34:14<00:34,  4.12it/s, loss=16.624674, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8418/8562 [34:14<00:34,  4.12it/s, loss=16.624278, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8419/8562 [34:14<00:34,  4.16it/s, loss=16.624278, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8419/8562 [34:14<00:34,  4.16it/s, loss=16.624561, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8420/8562 [34:14<00:33,  4.18it/s, loss=16.624561, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8420/8562 [34:14<00:33,  4.18it/s, loss=16.624161, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8421/8562 [34:14<00:34,  4.09it/s, loss=16.624161, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8421/8562 [34:15<00:34,  4.09it/s, loss=16.624198, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8422/8562 [34:15<00:34,  4.10it/s, loss=16.624198, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8422/8562 [34:15<00:34,  4.10it/s, loss=16.624204, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8423/8562 [34:15<00:33,  4.15it/s, loss=16.624204, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8423/8562 [34:15<00:33,  4.15it/s, loss=16.624205, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8424/8562 [34:15<00:32,  4.19it/s, loss=16.624205, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8424/8562 [34:15<00:32,  4.19it/s, loss=16.624735, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8425/8562 [34:15<00:32,  4.21it/s, loss=16.624735, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8425/8562 [34:16<00:32,  4.21it/s, loss=16.624866, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8426/8562 [34:16<00:32,  4.25it/s, loss=16.624866, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8426/8562 [34:16<00:32,  4.25it/s, loss=16.625440, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8427/8562 [34:16<00:31,  4.25it/s, loss=16.625440, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8427/8562 [34:16<00:31,  4.25it/s, loss=16.625268, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8428/8562 [34:16<00:31,  4.25it/s, loss=16.625268, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8428/8562 [34:16<00:31,  4.25it/s, loss=16.625080, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8429/8562 [34:16<00:31,  4.26it/s, loss=16.625080, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8429/8562 [34:17<00:31,  4.26it/s, loss=16.625323, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8430/8562 [34:17<00:30,  4.28it/s, loss=16.625323, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8430/8562 [34:17<00:30,  4.28it/s, loss=16.624958, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8431/8562 [34:17<00:31,  4.22it/s, loss=16.624958, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8431/8562 [34:17<00:31,  4.22it/s, loss=16.625266, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8432/8562 [34:17<00:30,  4.23it/s, loss=16.625266, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8432/8562 [34:17<00:30,  4.23it/s, loss=16.625680, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8433/8562 [34:17<00:30,  4.23it/s, loss=16.625680, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  98%|█████████▊| 8433/8562 [34:17<00:30,  4.23it/s, loss=16.625107, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8434/8562 [34:17<00:30,  4.25it/s, loss=16.625107, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8434/8562 [34:18<00:30,  4.25it/s, loss=16.624978, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8435/8562 [34:18<00:29,  4.25it/s, loss=16.624978, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8435/8562 [34:18<00:29,  4.25it/s, loss=16.625175, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8436/8562 [34:18<00:29,  4.25it/s, loss=16.625175, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8436/8562 [34:18<00:29,  4.25it/s, loss=16.625507, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8437/8562 [34:18<00:29,  4.25it/s, loss=16.625507, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8437/8562 [34:18<00:29,  4.25it/s, loss=16.625275, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8438/8562 [34:18<00:29,  4.26it/s, loss=16.625275, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8438/8562 [34:19<00:29,  4.26it/s, loss=16.625578, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8439/8562 [34:19<00:28,  4.26it/s, loss=16.625578, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8439/8562 [34:19<00:28,  4.26it/s, loss=16.625643, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8440/8562 [34:19<00:28,  4.26it/s, loss=16.625643, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8440/8562 [34:19<00:28,  4.26it/s, loss=16.625767, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8441/8562 [34:19<00:28,  4.27it/s, loss=16.625767, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8441/8562 [34:19<00:28,  4.27it/s, loss=16.625719, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8442/8562 [34:19<00:28,  4.28it/s, loss=16.625719, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8442/8562 [34:20<00:28,  4.28it/s, loss=16.624714, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8443/8562 [34:20<00:27,  4.28it/s, loss=16.624714, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8443/8562 [34:20<00:27,  4.28it/s, loss=16.624520, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8444/8562 [34:20<00:27,  4.26it/s, loss=16.624520, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8444/8562 [34:20<00:27,  4.26it/s, loss=16.624368, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8445/8562 [34:20<00:27,  4.26it/s, loss=16.624368, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8445/8562 [34:20<00:27,  4.26it/s, loss=16.624420, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8446/8562 [34:20<00:27,  4.24it/s, loss=16.624420, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8446/8562 [34:21<00:27,  4.24it/s, loss=16.624755, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8447/8562 [34:21<00:27,  4.24it/s, loss=16.624755, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8447/8562 [34:21<00:27,  4.24it/s, loss=16.624354, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8448/8562 [34:21<00:26,  4.26it/s, loss=16.624354, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8448/8562 [34:21<00:26,  4.26it/s, loss=16.624121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8449/8562 [34:21<00:26,  4.26it/s, loss=16.624121, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8449/8562 [34:21<00:26,  4.26it/s, loss=16.624217, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8450/8562 [34:21<00:26,  4.20it/s, loss=16.624217, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8450/8562 [34:21<00:26,  4.20it/s, loss=16.624451, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8451/8562 [34:21<00:26,  4.22it/s, loss=16.624451, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8451/8562 [34:22<00:26,  4.22it/s, loss=16.624298, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8452/8562 [34:22<00:26,  4.13it/s, loss=16.624298, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8452/8562 [34:22<00:26,  4.13it/s, loss=16.624742, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8453/8562 [34:22<00:26,  4.15it/s, loss=16.624742, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8453/8562 [34:22<00:26,  4.15it/s, loss=16.624933, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8454/8562 [34:22<00:25,  4.18it/s, loss=16.624933, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▊| 8454/8562 [34:22<00:25,  4.18it/s, loss=16.625356, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8455/8562 [34:22<00:25,  4.12it/s, loss=16.625356, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8455/8562 [34:23<00:25,  4.12it/s, loss=16.625451, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8456/8562 [34:23<00:25,  4.15it/s, loss=16.625451, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8456/8562 [34:23<00:25,  4.15it/s, loss=16.625387, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8457/8562 [34:23<00:25,  4.16it/s, loss=16.625387, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8457/8562 [34:23<00:25,  4.16it/s, loss=16.625204, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8458/8562 [34:23<00:24,  4.22it/s, loss=16.625204, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8458/8562 [34:23<00:24,  4.22it/s, loss=16.624938, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8459/8562 [34:23<00:24,  4.20it/s, loss=16.624938, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8459/8562 [34:24<00:24,  4.20it/s, loss=16.624193, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8460/8562 [34:24<00:24,  4.21it/s, loss=16.624193, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8460/8562 [34:24<00:24,  4.21it/s, loss=16.624497, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8461/8562 [34:24<00:23,  4.23it/s, loss=16.624497, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8461/8562 [34:24<00:23,  4.23it/s, loss=16.624907, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8462/8562 [34:24<00:23,  4.20it/s, loss=16.624907, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8462/8562 [34:24<00:23,  4.20it/s, loss=16.625498, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8463/8562 [34:24<00:23,  4.13it/s, loss=16.625498, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8463/8562 [34:25<00:23,  4.13it/s, loss=16.625478, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8464/8562 [34:25<00:25,  3.79it/s, loss=16.625478, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8464/8562 [34:25<00:25,  3.79it/s, loss=16.625753, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8465/8562 [34:25<00:25,  3.82it/s, loss=16.625753, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8465/8562 [34:25<00:25,  3.82it/s, loss=16.626258, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8466/8562 [34:25<00:24,  3.92it/s, loss=16.626258, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8466/8562 [34:25<00:24,  3.92it/s, loss=16.625741, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8467/8562 [34:25<00:24,  3.94it/s, loss=16.625741, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8467/8562 [34:26<00:24,  3.94it/s, loss=16.625419, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8468/8562 [34:26<00:23,  3.99it/s, loss=16.625419, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8468/8562 [34:26<00:23,  3.99it/s, loss=16.624981, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8469/8562 [34:26<00:25,  3.70it/s, loss=16.624981, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8469/8562 [34:26<00:25,  3.70it/s, loss=16.624672, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8470/8562 [34:26<00:23,  3.85it/s, loss=16.624672, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8470/8562 [34:26<00:23,  3.85it/s, loss=16.624275, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8471/8562 [34:26<00:22,  3.98it/s, loss=16.624275, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8471/8562 [34:27<00:22,  3.98it/s, loss=16.624255, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8472/8562 [34:27<00:22,  4.06it/s, loss=16.624255, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8472/8562 [34:27<00:22,  4.06it/s, loss=16.624572, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8473/8562 [34:27<00:21,  4.10it/s, loss=16.624572, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8473/8562 [34:27<00:21,  4.10it/s, loss=16.624348, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8474/8562 [34:27<00:21,  4.16it/s, loss=16.624348, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8474/8562 [34:27<00:21,  4.16it/s, loss=16.624782, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8475/8562 [34:27<00:21,  4.09it/s, loss=16.624782, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8475/8562 [34:28<00:21,  4.09it/s, loss=16.624375, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8476/8562 [34:28<00:20,  4.14it/s, loss=16.624375, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8476/8562 [34:28<00:20,  4.14it/s, loss=16.623583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8477/8562 [34:28<00:20,  4.06it/s, loss=16.623583, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8477/8562 [34:28<00:20,  4.06it/s, loss=16.623666, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8478/8562 [34:28<00:20,  4.11it/s, loss=16.623666, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8478/8562 [34:28<00:20,  4.11it/s, loss=16.623965, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8479/8562 [34:28<00:19,  4.16it/s, loss=16.623965, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8479/8562 [34:29<00:19,  4.16it/s, loss=16.623112, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8480/8562 [34:29<00:19,  4.14it/s, loss=16.623112, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8480/8562 [34:29<00:19,  4.14it/s, loss=16.622617, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8481/8562 [34:29<00:19,  4.19it/s, loss=16.622617, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8481/8562 [34:29<00:19,  4.19it/s, loss=16.622375, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8482/8562 [34:29<00:19,  4.00it/s, loss=16.622375, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8482/8562 [34:29<00:19,  4.00it/s, loss=16.622904, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8483/8562 [34:29<00:21,  3.70it/s, loss=16.622904, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8483/8562 [34:30<00:21,  3.70it/s, loss=16.622504, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8484/8562 [34:30<00:22,  3.50it/s, loss=16.622504, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8484/8562 [34:30<00:22,  3.50it/s, loss=16.622715, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8485/8562 [34:30<00:23,  3.32it/s, loss=16.622715, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8485/8562 [34:30<00:23,  3.32it/s, loss=16.623299, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8486/8562 [34:30<00:24,  3.07it/s, loss=16.623299, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8486/8562 [34:31<00:24,  3.07it/s, loss=16.623585, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8487/8562 [34:31<00:24,  3.07it/s, loss=16.623585, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8487/8562 [34:31<00:24,  3.07it/s, loss=16.623710, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8488/8562 [34:31<00:24,  3.08it/s, loss=16.623710, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8488/8562 [34:31<00:24,  3.08it/s, loss=16.623878, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8489/8562 [34:31<00:23,  3.17it/s, loss=16.623878, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8489/8562 [34:32<00:23,  3.17it/s, loss=16.623805, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8490/8562 [34:32<00:22,  3.14it/s, loss=16.623805, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8490/8562 [34:32<00:22,  3.14it/s, loss=16.623976, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8491/8562 [34:32<00:23,  3.02it/s, loss=16.623976, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8491/8562 [34:32<00:23,  3.02it/s, loss=16.623527, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8492/8562 [34:32<00:24,  2.90it/s, loss=16.623527, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8492/8562 [34:33<00:24,  2.90it/s, loss=16.624015, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8493/8562 [34:33<00:23,  2.91it/s, loss=16.624015, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8493/8562 [34:33<00:23,  2.91it/s, loss=16.624277, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8494/8562 [34:33<00:23,  2.86it/s, loss=16.624277, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8494/8562 [34:34<00:23,  2.86it/s, loss=16.624550, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8495/8562 [34:34<00:23,  2.88it/s, loss=16.624550, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8495/8562 [34:34<00:23,  2.88it/s, loss=16.624476, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8496/8562 [34:34<00:22,  2.98it/s, loss=16.624476, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8496/8562 [34:34<00:22,  2.98it/s, loss=16.624400, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8497/8562 [34:34<00:22,  2.87it/s, loss=16.624400, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8497/8562 [34:34<00:22,  2.87it/s, loss=16.624669, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8498/8562 [34:35<00:21,  3.01it/s, loss=16.624669, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8498/8562 [34:35<00:21,  3.01it/s, loss=16.624329, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8499/8562 [34:35<00:20,  3.01it/s, loss=16.624329, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8499/8562 [34:35<00:20,  3.01it/s, loss=16.624841, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8500/8562 [34:35<00:20,  3.02it/s, loss=16.624841, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8500/8562 [34:36<00:20,  3.02it/s, loss=16.624704, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8501/8562 [34:36<00:20,  2.92it/s, loss=16.624704, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8501/8562 [34:36<00:20,  2.92it/s, loss=16.624209, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8502/8562 [34:36<00:20,  2.92it/s, loss=16.624209, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8502/8562 [34:36<00:20,  2.92it/s, loss=16.624125, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8503/8562 [34:36<00:20,  2.95it/s, loss=16.624125, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8503/8562 [34:37<00:20,  2.95it/s, loss=16.623913, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8504/8562 [34:37<00:20,  2.79it/s, loss=16.623913, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8504/8562 [34:37<00:20,  2.79it/s, loss=16.623670, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8505/8562 [34:37<00:19,  2.93it/s, loss=16.623670, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8505/8562 [34:37<00:19,  2.93it/s, loss=16.624102, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8506/8562 [34:37<00:17,  3.21it/s, loss=16.624102, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8506/8562 [34:37<00:17,  3.21it/s, loss=16.624314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8507/8562 [34:37<00:15,  3.45it/s, loss=16.624314, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8507/8562 [34:38<00:15,  3.45it/s, loss=16.624342, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8508/8562 [34:38<00:14,  3.62it/s, loss=16.624342, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8508/8562 [34:38<00:14,  3.62it/s, loss=16.624198, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8509/8562 [34:38<00:13,  3.80it/s, loss=16.624198, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8509/8562 [34:38<00:13,  3.80it/s, loss=16.623783, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8510/8562 [34:38<00:13,  3.88it/s, loss=16.623783, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8510/8562 [34:38<00:13,  3.88it/s, loss=16.623701, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8511/8562 [34:38<00:12,  3.97it/s, loss=16.623701, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8511/8562 [34:39<00:12,  3.97it/s, loss=16.623677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8512/8562 [34:39<00:12,  4.05it/s, loss=16.623677, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8512/8562 [34:39<00:12,  4.05it/s, loss=16.624046, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8513/8562 [34:39<00:12,  4.06it/s, loss=16.624046, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8513/8562 [34:39<00:12,  4.06it/s, loss=16.624294, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8514/8562 [34:39<00:11,  4.05it/s, loss=16.624294, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8514/8562 [34:39<00:11,  4.05it/s, loss=16.623995, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8515/8562 [34:39<00:11,  4.11it/s, loss=16.623995, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8515/8562 [34:40<00:11,  4.11it/s, loss=16.624072, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8516/8562 [34:40<00:11,  4.12it/s, loss=16.624072, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8516/8562 [34:40<00:11,  4.12it/s, loss=16.624427, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8517/8562 [34:40<00:10,  4.13it/s, loss=16.624427, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8517/8562 [34:40<00:10,  4.13it/s, loss=16.623863, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8518/8562 [34:40<00:10,  4.05it/s, loss=16.623863, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8518/8562 [34:40<00:10,  4.05it/s, loss=16.623769, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8519/8562 [34:40<00:10,  4.11it/s, loss=16.623769, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10:  99%|█████████▉| 8519/8562 [34:41<00:10,  4.11it/s, loss=16.623698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8520/8562 [34:41<00:10,  4.13it/s, loss=16.623698, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8520/8562 [34:41<00:10,  4.13it/s, loss=16.623141, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8521/8562 [34:41<00:09,  4.13it/s, loss=16.623141, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8521/8562 [34:41<00:09,  4.13it/s, loss=16.623274, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8522/8562 [34:41<00:09,  4.17it/s, loss=16.623274, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8522/8562 [34:41<00:09,  4.17it/s, loss=16.623218, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8523/8562 [34:41<00:09,  4.06it/s, loss=16.623218, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8523/8562 [34:42<00:09,  4.06it/s, loss=16.622788, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8524/8562 [34:42<00:09,  4.08it/s, loss=16.622788, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8524/8562 [34:42<00:09,  4.08it/s, loss=16.622572, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8525/8562 [34:42<00:09,  4.10it/s, loss=16.622572, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8525/8562 [34:42<00:09,  4.10it/s, loss=16.622913, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8526/8562 [34:42<00:08,  4.14it/s, loss=16.622913, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8526/8562 [34:42<00:08,  4.14it/s, loss=16.622700, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8527/8562 [34:42<00:08,  4.17it/s, loss=16.622700, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8527/8562 [34:42<00:08,  4.17it/s, loss=16.622777, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8528/8562 [34:42<00:08,  4.21it/s, loss=16.622777, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8528/8562 [34:43<00:08,  4.21it/s, loss=16.622691, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8529/8562 [34:43<00:07,  4.20it/s, loss=16.622691, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8529/8562 [34:43<00:07,  4.20it/s, loss=16.622277, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8530/8562 [34:43<00:07,  4.22it/s, loss=16.622277, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8530/8562 [34:43<00:07,  4.22it/s, loss=16.622280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8531/8562 [34:43<00:07,  4.18it/s, loss=16.622280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8531/8562 [34:43<00:07,  4.18it/s, loss=16.622345, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8532/8562 [34:43<00:07,  4.19it/s, loss=16.622345, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8532/8562 [34:44<00:07,  4.19it/s, loss=16.622695, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8533/8562 [34:44<00:06,  4.19it/s, loss=16.622695, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8533/8562 [34:44<00:06,  4.19it/s, loss=16.623102, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8534/8562 [34:44<00:06,  4.19it/s, loss=16.623102, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8534/8562 [34:44<00:06,  4.19it/s, loss=16.622280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8535/8562 [34:44<00:06,  4.21it/s, loss=16.622280, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8535/8562 [34:44<00:06,  4.21it/s, loss=16.622460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8536/8562 [34:44<00:06,  4.21it/s, loss=16.622460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8536/8562 [34:45<00:06,  4.21it/s, loss=16.622279, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8537/8562 [34:45<00:05,  4.24it/s, loss=16.622279, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8537/8562 [34:45<00:05,  4.24it/s, loss=16.621978, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8538/8562 [34:45<00:05,  4.25it/s, loss=16.621978, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8538/8562 [34:45<00:05,  4.25it/s, loss=16.622384, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8539/8562 [34:45<00:05,  4.18it/s, loss=16.622384, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8539/8562 [34:45<00:05,  4.18it/s, loss=16.622088, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8540/8562 [34:45<00:05,  4.20it/s, loss=16.622088, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8540/8562 [34:46<00:05,  4.20it/s, loss=16.622018, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8541/8562 [34:46<00:04,  4.21it/s, loss=16.622018, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8541/8562 [34:46<00:04,  4.21it/s, loss=16.622250, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8542/8562 [34:46<00:04,  4.22it/s, loss=16.622250, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8542/8562 [34:46<00:04,  4.22it/s, loss=16.621786, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8543/8562 [34:46<00:04,  4.22it/s, loss=16.621786, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8543/8562 [34:46<00:04,  4.22it/s, loss=16.622145, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8544/8562 [34:46<00:04,  4.20it/s, loss=16.622145, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8544/8562 [34:46<00:04,  4.20it/s, loss=16.622162, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8545/8562 [34:46<00:04,  4.20it/s, loss=16.622162, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8545/8562 [34:47<00:04,  4.20it/s, loss=16.622159, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8546/8562 [34:47<00:03,  4.22it/s, loss=16.622159, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8546/8562 [34:47<00:03,  4.22it/s, loss=16.622565, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8547/8562 [34:47<00:03,  4.22it/s, loss=16.622565, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8547/8562 [34:47<00:03,  4.22it/s, loss=16.622332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8548/8562 [34:47<00:03,  3.96it/s, loss=16.622332, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8548/8562 [34:47<00:03,  3.96it/s, loss=16.622592, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8549/8562 [34:47<00:03,  4.04it/s, loss=16.622592, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8549/8562 [34:48<00:03,  4.04it/s, loss=16.622957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8550/8562 [34:48<00:02,  4.05it/s, loss=16.622957, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8550/8562 [34:48<00:02,  4.05it/s, loss=16.623202, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8551/8562 [34:48<00:02,  4.11it/s, loss=16.623202, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8551/8562 [34:48<00:02,  4.11it/s, loss=16.623277, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8552/8562 [34:48<00:02,  4.05it/s, loss=16.623277, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8552/8562 [34:48<00:02,  4.05it/s, loss=16.623074, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8553/8562 [34:48<00:02,  4.04it/s, loss=16.623074, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8553/8562 [34:49<00:02,  4.04it/s, loss=16.623384, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8554/8562 [34:49<00:01,  4.11it/s, loss=16.623384, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8554/8562 [34:49<00:01,  4.11it/s, loss=16.622865, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8555/8562 [34:49<00:01,  4.16it/s, loss=16.622865, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8555/8562 [34:49<00:01,  4.16it/s, loss=16.622897, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8556/8562 [34:49<00:01,  4.19it/s, loss=16.622897, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8556/8562 [34:49<00:01,  4.19it/s, loss=16.623104, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8557/8562 [34:49<00:01,  4.21it/s, loss=16.623104, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8557/8562 [34:50<00:01,  4.21it/s, loss=16.623160, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8558/8562 [34:50<00:00,  4.24it/s, loss=16.623160, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8558/8562 [34:50<00:00,  4.24it/s, loss=16.623235, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8559/8562 [34:50<00:00,  4.26it/s, loss=16.623235, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8559/8562 [34:50<00:00,  4.26it/s, loss=16.622921, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8560/8562 [34:50<00:00,  4.27it/s, loss=16.622921, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8560/8562 [34:50<00:00,  4.27it/s, loss=16.623278, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8561/8562 [34:50<00:00,  4.27it/s, loss=16.623278, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|█████████▉| 8561/8562 [34:51<00:00,  4.27it/s, loss=16.623460, LR=3.14e-6]\u001b[A\n",
      "Training epoch: 10: 100%|██████████| 8562/8562 [34:51<00:00,  4.09it/s, loss=16.623460, LR=3.14e-6]\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "EfficientNet-B[3,5,6] Image + TRAINING.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
