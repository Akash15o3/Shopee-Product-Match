{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.programmersought.com/article/82038071353/\n",
    "- https://pypi.org/project/timm/\n",
    "- https://paperswithcode.com/method/arcface\n",
    "- https://albumentations.ai/docs/\n",
    "- https://www.kaggle.com/parthdhameliya77/pytorch-efficientnet-b3-image-tfidf-inference\n",
    "- https://www.kaggle.com/parthdhameliya77/pytorch-efficientnet-b5-image-tfidf-inference\n",
    "- https://github.com/rapidsai/cuml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "1) To use EfficientNetB[3,5,6] pretrained models as the backbone to train SHOPEE image dataset for product matching by [IMAGE].\n",
    "\n",
    "2) To use [TF-DIF Vectorizer] sklearn model as the backone to train SHOPEE metadata for product matching by [Title].\n",
    "\n",
    "3) To retrieve product matches(neighboutrs) for text & image embeddings from Step[1],[2] using KNN. \n",
    "\n",
    "4) Combine predictions for text, image product matches from Step[3]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import math\n",
    "import random \n",
    "import os \n",
    "import cv2\n",
    "import timm\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import albumentations as Albumentations \n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import Dataset \n",
    "from torch import nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "import gc\n",
    "\n",
    "#RAPIDS AI for NearestNeighbors, Vectorizing Text.\n",
    "import cudf \n",
    "import cuml\n",
    "import cupy\n",
    "from cuml.feature_extraction.text import TfidfVectorizer\n",
    "from cuml.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Configurations\n",
    "\n",
    "**Note:** model config structure adapted from https://www.kaggle.com/parthdhameliya77/pytorch-efficientnet-b3-image-tfidf-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained models config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pretrained_model_config:\n",
    "    \n",
    "    img_size = 512\n",
    "    batch_size = 40\n",
    "    seed = 2020\n",
    "    \n",
    "    device = 'cuda'\n",
    "    classes = 11014\n",
    "    \n",
    "    model_name = 'eca_nfnet_l0'\n",
    "    \n",
    "    #Change model name here to make predictions with the respective model\n",
    "    #eca_nfnet_l0\n",
    "    #tf_efficientnet_b3_ns\n",
    "    #tf_efficientnet_b5_ns\n",
    "    #tf_efficientnet_b6_ns\n",
    "    \n",
    "    model_path = '../input/shopee-pytorch-models/arcface_512x512_nfnet_l0.pt'\n",
    "   \n",
    "    #Change to respective pretrained model path\n",
    "    # ../input/shopee-pytorch-models/arcface_512x512_nfnet_l0.pt\n",
    "    #../input/shopee-pytorch-models/arcface_512x512_eff_b3_.pt\n",
    "    #../input/shopee-pytorch-models/arcface_512x512_eff_b5_.pt\n",
    "    #../input/arcface-512x512-eff-b6-pt/arcface_512x512_tf_efficientnet_b6.pt\n",
    "    \n",
    "    scale = 30 \n",
    "    margin = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure PyTorch seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_seed(seed=2000):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "configure_seed(pretrained_model_config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch test image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_transforms():\n",
    "\n",
    "    return Albumentations.Compose(\n",
    "        [\n",
    "            Albumentations.Resize(pretrained_model_config.img_size,pretrained_model_config.img_size,always_apply=True),\n",
    "            Albumentations.Normalize(),\n",
    "        ToTensorV2(p=1.0)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read test meta-data & images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testData():\n",
    "    df = pd.read_csv('../input/shopee-product-matching/test.csv')\n",
    "    df_cuda = cudf.DataFrame(df)\n",
    "    df_img_path = '../input/shopee-product-matching/test_images/' + df['image']\n",
    "    return df, df_cuda, df_img_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge predictions using Product (Title, Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergePredictions(product):\n",
    "    x = np.concatenate([product['image_predictions'], product['text_predictions']])\n",
    "    return ' '.join( np.unique(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similar products with image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_products_by_image(df, image_embeddings,neighbour_threshold = 4.5):\n",
    "    \n",
    "    if len(df) > 3:\n",
    "        KNN = 100\n",
    "    else : \n",
    "        KNN = 3\n",
    "    \n",
    "    model = NearestNeighbors(n_neighbors = KNN)\n",
    "    \n",
    "    model.fit(image_embeddings)\n",
    "    \n",
    "    distances, indices = model.kneighbors(image_embeddings)\n",
    "    \n",
    "    predictions = []\n",
    "    for k in tqdm(range(image_embeddings.shape[0])):\n",
    "        idn = np.where(distances[k,] < neighbour_threshold)[0]\n",
    "        idi = indices[k,idn]\n",
    "        id_matched = df['posting_id'].iloc[idi].values\n",
    "        predictions.append(id_matched)\n",
    "        \n",
    "    del model, distances, indices\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings(image_paths, model_name = pretrained_model_config.model_name):\n",
    "    embeds = []\n",
    "    \n",
    "    model = Product_Image_Prediction_Model(model_name = model_name)\n",
    "    model.eval()\n",
    "    model.load_state_dict(torch.load(pretrained_model_config.model_path))\n",
    "    model = model.to(pretrained_model_config.device)\n",
    "\n",
    "    image_dataset = Product_Images(image_paths=image_paths,transforms=get_test_transforms())\n",
    "    image_loader = torch.utils.data.DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=pretrained_model_config.batch_size,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "        num_workers=4\n",
    "    )\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img,label in tqdm(image_loader): \n",
    "            img = img.cuda()\n",
    "            label = label.cuda()\n",
    "            feat = model(img,label)\n",
    "            image_embeddings = feat.detach().cpu().numpy()\n",
    "            embeds.append(image_embeddings)\n",
    "    \n",
    "    \n",
    "    del model\n",
    "    image_embeddings = np.concatenate(embeds)\n",
    "    print(f'Our image embeddings shape is {image_embeddings.shape}')\n",
    "    del embeds\n",
    "    gc.collect()\n",
    "    return image_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get similar products with title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_products_by_title(df, max_features = 25_000):\n",
    "    \n",
    "    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n",
    "    text_embeddings = model.fit_transform(df_cu['title']).toarray()\n",
    "    preds = []\n",
    "    CHUNK_SIZE = 1024*4\n",
    "\n",
    "    print('Finding similar titles...')\n",
    "    TEXT_CHUNKS = len(df)//CHUNK_SIZE\n",
    "    if len(df)%CHUNK_SIZE!=0: TEXT_CHUNKS += 1\n",
    "    for j in range( TEXT_CHUNKS ):\n",
    "\n",
    "        a = j*CHUNK_SIZE\n",
    "        b = (j+1)*CHUNK_SIZE\n",
    "        b = min(b,len(df))\n",
    "        print('chunk',a,'to',b)\n",
    "        \n",
    "        cts = cupy.matmul( text_embeddings, text_embeddings[a:b].T).T\n",
    "\n",
    "        for k in range(b-a):\n",
    "            IDX = cupy.where(cts[k,]>0.75)[0]\n",
    "            o = df.iloc[cupy.asnumpy(IDX)].posting_id.values\n",
    "            preds.append(o)\n",
    "    \n",
    "    del model,text_embeddings\n",
    "    gc.collect()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "**Note:** model config structure adapted from https://www.kaggle.com/parthdhameliya77/pytorch-efficientnet-b3-image-tfidf-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product_Images(Dataset):\n",
    "    def __init__(self, image_paths, transforms=None):\n",
    "\n",
    "        self.image_paths = image_paths\n",
    "        self.augmentations = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.image_paths.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        \n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.augmentations:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented['image']       \n",
    "    \n",
    "        return image,torch.tensor(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArcFace Margin loss function for improved large-scale recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageRecog_MarginLoss_ArcFace(nn.Module):\n",
    "    def __init__(self, in_features, out_features, scale=30.0, margin=0.50, easy_margin=False, ls_eps=0.0):\n",
    "        super(ImageRecog_MarginLoss_ArcFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.scale = scale\n",
    "        self.margin = margin\n",
    "        self.ls_eps = ls_eps  # label smoothing\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        cosTheta = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sineTheta = torch.sqrt(1.0 - torch.pow(cosTheta, 2))\n",
    "        phi_val = cosTheta * self.cos_m - sineTheta * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi_val = torch.where(cosTheta > 0, phi_val, cosTheta)\n",
    "        else:\n",
    "            phi_val = torch.where(cosTheta > self.th, phi_val, cosTheta - self.mm)\n",
    "        one_hot_encoding = torch.zeros(cosTheta.size(), device='cuda')\n",
    "        one_hot_encoding.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        if self.ls_eps > 0:\n",
    "            one_hot_encoding = (1 - self.ls_eps) * one_hot_encoding + self.ls_eps / self.out_features\n",
    "        loss = (one_hot_encoding * phi_val) + ((1.0 - one_hot_encoding) * cosTheta)\n",
    "        loss *= self.scale\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Model for the Shopee products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Product_Image_Prediction_Model(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_classes = pretrained_model_config.classes,\n",
    "        model_name = pretrained_model_config.model_name,\n",
    "        fc_dim = 512,\n",
    "        margin = pretrained_model_config.margin,\n",
    "        scale = pretrained_model_config.scale,\n",
    "        use_fc = False,\n",
    "        pretrained = False):\n",
    "\n",
    "\n",
    "        super(Product_Image_Prediction_Model,self).__init__()\n",
    "        \n",
    "        print('Compiling & Building SHOPEE Model Backbone for {} model'.format(model_name))\n",
    "\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "\n",
    "        if model_name == 'tf_efficientnet_b3_ns':\n",
    "            final_in_features = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "\n",
    "        elif model_name == 'tf_efficientnet_b5_ns':\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "        \n",
    "        elif model_name == 'tf_efficientnet_b6_ns':\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "           \n",
    "        elif model_name == 'eca_nfnet_l0':\n",
    "            final_in_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "            \n",
    "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.use_fc = use_fc\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.0)\n",
    "        \n",
    "        self.fc = nn.Linear(final_in_features, fc_dim)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        \n",
    "        self._init_params()\n",
    "        \n",
    "        final_in_features = fc_dim\n",
    "\n",
    "        self.final = ImageRecog_MarginLoss_ArcFace(\n",
    "            final_in_features,\n",
    "            n_classes,\n",
    "            scale = scale,\n",
    "            margin = margin,\n",
    "            easy_margin = False,\n",
    "            ls_eps = 0.0\n",
    "        )\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, image, label):\n",
    "        feature = self.extract_feat(image)\n",
    "        return feature\n",
    "\n",
    "    def extract_feat(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "\n",
    "        if self.use_fc:\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc(x)\n",
    "            x = self.bn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df,df_cu,image_paths = get_testData()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = get_image_embeddings(image_paths.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predict similar products by title & image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions = get_similar_products_by_image(df, image_embeddings, neighbour_threshold = 4.5)\n",
    "text_predictions = get_similar_products_by_title(df, max_features = 25_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comine predictions & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_predictions'] = image_predictions\n",
    "df['text_predictions'] = text_predictions\n",
    "df['matches'] = df.apply(mergePredictions, axis = 1)\n",
    "df[['posting_id', 'matches']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['posting_id', 'matches']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences\n",
    "\n",
    "Combined predictions using EfficientNetB6 along with TF-DIF Vectorizer provided better F1 score over EfficientNetB[3,5] / RESNET152 + TF-DIF predictions.\n",
    "\n",
    "**LEARNING**\n",
    "\n",
    "1) How to train pretrained EfficientNetB[3,5] on the SHOPEE dataset for more epochs.\n",
    "2) How to train EfficientNetB[6] on the SHOPEE dataset from scratch and how to stay calm :) P.S B6 training took 34 hours.\n",
    "\n",
    "**Step-4**\n",
    "\n",
    "To replace [EfficientNet] model with the [eca-nfnet-10] model and make inferences on the SHOPEE Dataset, to see if the current [F1] Score can be improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
