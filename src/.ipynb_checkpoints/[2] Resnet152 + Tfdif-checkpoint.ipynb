{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- https://www.kaggle.com/finlay/unsupervised-image-text-baseline-in-20min\n",
    "- https://www.kaggle.com/pytorch/resnet152\n",
    "- https://www.kaggle.com/kwisatzhaderach/glove2word2vec\n",
    "- https://sigir-ecom.github.io/ecom20DCPapers/SIGIR_eCom20_DC_paper_7.pdf\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "1) To use [RESNET152] pretrained model as the backbone to train SHOPEE image dataset for product matching by [IMAGE].\n",
    "\n",
    "2) To use [TF-DIF Vectorizer] sklearn model as the backone to train SHOPEE metadata for product matching by [Title].\n",
    "\n",
    "3) To retrieve product matches(neighbours) by defining distance-based threshhold for text, image embeddings.\n",
    "\n",
    "4) Combine predictions of text, image from Step-[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:14.869532Z",
     "start_time": "2021-03-18T09:59:14.482759Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(attribute):\n",
    "    def measureF1(sample):\n",
    "        n = len( np.intersect1d(sample.target,row[attribute]) )\n",
    "        return ((2*n) / (len(sample.target)+len(row[attribute])))\n",
    "    return measureF1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input/shopee-product-matching/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store target by Label_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:15.92512Z",
     "start_time": "2021-03-18T09:59:15.308672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring CV Settings for training data..\n",
      "\n",
      "******* Target(Matching) images for training data *******\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>../input/shopee-product-matching/test_images/0...</td>\n",
       "      <td>ecc292392dc7687a</td>\n",
       "      <td>Edufuntoys - CHARACTER PHONE ada lampu dan mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>../input/shopee-product-matching/test_images/0...</td>\n",
       "      <td>e9968f60d2699e2c</td>\n",
       "      <td>(Beli 1 Free Spatula) Masker Komedo | Blackhea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>../input/shopee-product-matching/test_images/0...</td>\n",
       "      <td>ba81c17e3581cabe</td>\n",
       "      <td>READY Lemonilo Mie instant sehat kuah dan goreng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                              image  \\\n",
       "0  test_2255846744  ../input/shopee-product-matching/test_images/0...   \n",
       "1  test_3588702337  ../input/shopee-product-matching/test_images/0...   \n",
       "2  test_4015706929  ../input/shopee-product-matching/test_images/0...   \n",
       "\n",
       "        image_phash                                              title  \n",
       "0  ecc292392dc7687a  Edufuntoys - CHARACTER PHONE ada lampu dan mus...  \n",
       "1  e9968f60d2699e2c  (Beli 1 Free Spatula) Masker Komedo | Blackhea...  \n",
       "2  ba81c17e3581cabe   READY Lemonilo Mie instant sehat kuah dan goreng  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Training = False #Toggle this to train the model\n",
    "\n",
    "test = pd.read_csv(path + 'test.csv')\n",
    "\n",
    "if len(test)>3: Training = False\n",
    "    \n",
    "else: print('Configuring CV Settings for training data..')\n",
    "\n",
    "if Training: #Use training data.\n",
    "    train = pd.read_csv(path + 'train.csv')\n",
    "    train['image'] = path + 'train_images/' + train['image']\n",
    "    \n",
    "    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "    train['target'] = train.label_group.map(tmp)\n",
    "    \n",
    "else: #Use test-data for reference.\n",
    "    train = pd.read_csv(path + 'test.csv')\n",
    "    train['image'] = path + 'test_images/' + train['image'] \n",
    "\n",
    "print()\n",
    "print('******* Target(Matching) images for training data *******')\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store image hash by Label_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:19.569052Z",
     "start_time": "2021-03-18T09:59:18.284395Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Image phash for training data grouped by label_groups*******\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [test_2255846744]\n",
       "1    [test_3588702337]\n",
       "Name: groupedByID_image_phash, dtype: object"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = train.groupby('image_phash').posting_id.agg('unique').to_dict()\n",
    "train['groupedByID_image_phash'] = train.image_phash.map(tmp)\n",
    "print('******* Image phash for training data grouped by label_groups*******')\n",
    "train['groupedByID_image_phash'].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Cross-validation score with unique imagehash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:21.98207Z",
     "start_time": "2021-03-18T09:59:20.62671Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if Training:\n",
    "    train['f1'] = train.apply(evaluate_model_performance('groupedBy_image_phash'),axis=1)\n",
    "    print('CV score without training with hash data',train.f1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN RESNET152 - Find similar products using images\n",
    "\n",
    "**Note:** Adapted code structure from https://www.kaggle.com/finlay/unsupervised-image-text-baseline-in-20min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:24.147684Z",
     "start_time": "2021-03-18T09:59:23.6933Z"
    }
   },
   "outputs": [],
   "source": [
    "class ProductImages(Dataset):\n",
    "    def __init__(self, img_path, transform):\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.img_path[index]).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:25.6502Z",
     "start_time": "2021-03-18T09:59:25.64389Z"
    }
   },
   "outputs": [],
   "source": [
    "productImages = ProductImages(\n",
    "    train['image'].values,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize((512, 512)), #resize to be processed by 512x512 RESNET152\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.50, 0.50, 0.50], [0.25, 0.25, 0.25])\n",
    "]))\n",
    "    \n",
    "getProductImages = torch.utils.data.DataLoader(\n",
    "    productImages,\n",
    "    batch_size=100, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T09:59:27.08827Z",
     "start_time": "2021-03-18T09:59:27.083495Z"
    }
   },
   "outputs": [],
   "source": [
    "class ProductImage_Embedding_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProductImage_Embedding_Net, self).__init__()\n",
    "              \n",
    "        model = models.resnet152(True) #Can be changed with any resnet\n",
    "        \n",
    "        model.avgpool = nn.AdaptiveMaxPool2d(output_size=(1, 1))\n",
    "        \n",
    "        model = nn.Sequential(*list(model.children())[:-1])\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, img):        \n",
    "        out = self.model(img)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:01:20.420477Z",
     "start_time": "2021-03-18T09:59:28.809744Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389433c785ad41dfa15e1361a21dc96e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.43484437, 0.947295  , 3.708315  , ..., 1.5865095 , 1.3522434 ,\n",
      "        1.5033988 ],\n",
      "       [1.7952293 , 1.576125  , 1.4603299 , ..., 1.3755738 , 2.4345665 ,\n",
      "        3.3643942 ],\n",
      "       [1.2823381 , 1.2496889 , 3.1288214 , ..., 2.2141232 , 3.9003263 ,\n",
      "        0.47788632]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "DEVICE = 'cuda'\n",
    "\n",
    "product_Image_Embedding_Net = ProductImage_Embedding_Net()\n",
    "product_Image_Embedding_Net = product_Image_Embedding_Net.to(DEVICE) #load to GPU cuda device.\n",
    "\n",
    "extracted_image_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in tqdm_notebook(getProductImages):\n",
    "        data = data.to(DEVICE)\n",
    "        feature = product_Image_Embedding_Net(data)\n",
    "        feature = feature.reshape(feature.shape[0], feature.shape[1])\n",
    "        feature = feature.data.cpu().numpy()\n",
    "        extracted_image_features.append(feature)\n",
    "\n",
    "print(extracted_image_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:01:43.818543Z",
     "start_time": "2021-03-18T10:01:43.401624Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stacked_image_features = np.vstack(extracted_image_features)\n",
    "stacked_image_features = normalize(stacked_image_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeddings = torch.from_numpy(stacked_image_features)\n",
    "image_embeddings = image_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:01:54.771453Z",
     "start_time": "2021-03-18T10:01:44.50243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar images...\n",
      "Processing product image CHUNKS: 0 to 3\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "IMAGE_CHUNK_SIZE = 512*8\n",
    "\n",
    "print('Finding similar images...')\n",
    "\n",
    "IMAGE_CHUNKS = len(image_embeddings)//IMAGE_CHUNK_SIZE\n",
    "\n",
    "if len(image_embeddings)%IMAGE_CHUNK_SIZE!=0: IMAGE_CHUNKS += 1\n",
    "for j in range( IMAGE_CHUNKS ):\n",
    "    \n",
    "    a = j*IMAGE_CHUNK_SIZE\n",
    "    \n",
    "    b = (j+1)*IMAGE_CHUNK_SIZE\n",
    "    \n",
    "    b = min(b, len(image_embeddings))\n",
    "    \n",
    "    print('Processing product image CHUNKS:',a,'to',b)\n",
    "    \n",
    "    distances = torch.matmul(image_embeddings, image_embeddings[a:b].T).T\n",
    "    \n",
    "    distances = distances.data.cpu().numpy()\n",
    "    \n",
    "    for k in range(b-a):\n",
    "        IDX = np.where(distances[k,]>0.75)[0][:] #retrieve neighbors within a threshhold.\n",
    "        pred = train.iloc[IDX].posting_id.values\n",
    "        preds.append(pred)\n",
    "        \n",
    "del image_embeddings, product_Image_Embedding_Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:01:58.132852Z",
     "start_time": "2021-03-18T10:01:56.678412Z"
    }
   },
   "outputs": [],
   "source": [
    "train['pred_CNN'] = preds\n",
    "\n",
    "if Training:\n",
    "    train['f1'] = train.apply(evaluate_model_performance('pred_CNN'),axis=1)\n",
    "    print('CV score for RESNET152 Baseline =',train.f1.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF - Find similar products by title "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:02:00.631468Z",
     "start_time": "2021-03-18T10:01:59.851964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text embeddings shape (3, 28)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "model = TfidfVectorizer(stop_words=None, binary=True, max_features=40000)\n",
    "text_embeddings = model.fit_transform(train.title).toarray()\n",
    "print('text embeddings shape',text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = torch.from_numpy(text_embeddings)\n",
    "text_embeddings = text_embeddings.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:05:46.252393Z",
     "start_time": "2021-03-18T10:02:01.803979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding similar titles...\n",
      "chunk 0 to 3\n",
      "(3, 3)\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "TEXT_CHUNK_SIZE = 512*8\n",
    "\n",
    "print('Finding similar titles...')\n",
    "\n",
    "TEXT_CHUNKS = len(train)//TEXT_CHUNK_SIZE\n",
    "\n",
    "if len(train)%TEXT_CHUNK_SIZE!=0: TEXT_CHUNKS += 1\n",
    "    \n",
    "TEXT_CHUNK_index = 0\n",
    "\n",
    "for j in range( TEXT_CHUNKS ):\n",
    "    \n",
    "    a = j*TEXT_CHUNK_SIZE\n",
    "    b = (j+1)*TEXT_CHUNK_SIZE\n",
    "    b = min(b,len(train))\n",
    "    print('chunk',a,'to',b)\n",
    "    \n",
    "    cts = torch.matmul(text_embeddings, text_embeddings[a:b].T).T\n",
    "    cts = cts.data.cpu().numpy()\n",
    "    print(cts.shape)\n",
    "    for k in range(b-a):\n",
    "        IDX = np.where(cts[k,]>0.75)[0] #retrieve neighbors within a threshhold.\n",
    "        pred = train.iloc[IDX].posting_id.values\n",
    "        preds.append(pred)\n",
    "        TEXT_CHUNK_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:06:03.146166Z",
     "start_time": "2021-03-18T10:06:01.83687Z"
    }
   },
   "outputs": [],
   "source": [
    "train['pred_TFDIF'] = preds\n",
    "\n",
    "if Training:\n",
    "    train['f1'] = train.apply(evaluate_model_performance('pred_TFDIF'),axis=1)\n",
    "    print('CV score for TFDIF baseline =',train.f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:06:04.931476Z",
     "start_time": "2021-03-18T10:06:04.925838Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_Text_Image_Predictions(sample):\n",
    "    x = np.concatenate([sample.pred_TFDIF,sample.pred_CNN, sample.groupedByID_image_phash])\n",
    "    return ' '.join( np.unique(x) )\n",
    "\n",
    "def merge_Text_Image_CV(sample):\n",
    "    x = np.concatenate([sample.pred_TFDIF,sample.pred_CNN, sample.groupedByID_image_phash])\n",
    "    return np.unique(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:06:09.759812Z",
     "start_time": "2021-03-18T10:06:05.955972Z"
    }
   },
   "outputs": [],
   "source": [
    "if Training:\n",
    "    tmp = train.groupby('label_group').posting_id.agg('unique').to_dict()\n",
    "    train['target'] = train.label_group.map(tmp)\n",
    "    train['pred'] = train.apply(merge_Text_Image_CV,axis=1)\n",
    "    train['f1'] = train.apply(evaluate_model_performance('pred_CNN'),axis=1)\n",
    "    print('CV Score =', train.f1.mean() )\n",
    "\n",
    "train['matches'] = train.apply(merge_Text_Image_Predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-18T10:06:12.385916Z",
     "start_time": "2021-03-18T10:06:12.180234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>test_2255846744 test_3588702337 test_4015706929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>test_2255846744 test_3588702337 test_4015706929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>test_2255846744 test_3588702337 test_4015706929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                          matches\n",
       "0  test_2255846744  test_2255846744 test_3588702337 test_4015706929\n",
       "1  test_3588702337  test_2255846744 test_3588702337 test_4015706929\n",
       "2  test_4015706929  test_2255846744 test_3588702337 test_4015706929"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['posting_id','matches']].to_csv('submission.csv',index=False)\n",
    "sub = pd.read_csv('submission.csv')\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferences\n",
    "\n",
    "Combined predictions using [RESNET152](image processing)along with [TF-DIF Vectorizer](text processing) provided better F1 score over stand-alone predictions.\n",
    "\n",
    "#### LEARNING\n",
    "1) Using pretrained models to train the SHOPEE Dataset and make predictions(product match).\n",
    "\n",
    "2) How to retrieve matched images(neighbours) by defining distance threshhold).\n",
    "\n",
    "#### Step-3 \n",
    "To replace [RESNET152] model with the EfficientNet [3,5,6] models and train them on the SHOPEE Dataset, to see if the current [F1] Score can be improved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
